#+TITLE: Small Disjuncts
#+AUTHOR: Ignacio Cordón Castillo
#+OPTIONS: toc:t
#+LANGUAGE: es
#+STARTUP: indent
#+DATE:
#+LATEX_HEADER: \usepackage[spanish]{babel}
#+LATEX_HEADER: \usepackage{amsmath} 
#+LATEX_HEADER: \usepackage{amsthm}
#+LATEX_HEADER: \usepackage{mathrsfs}
#+LATEX_HEADER: \usepackage{dsfont}
#+LATEX_HEADER: \newtheorem{theorem}{Teorema}
#+LATEX_HEADER: \newtheorem{fact}{Proposición}
#+LATEX_HEADER: \newtheorem{lemma}{Lema}
#+LATEX_HEADER: \newtheorem{corollary}{Corolario}
#+LATEX_HEADER: \newtheorem{definition}{Definición}
#+LATEX_HEADER: \setlength{\parindent}{0pt}
#+LATEX_HEADER: \setlength{\parskip}{1em}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \newenvironment{wording}{\setlength{\parskip}{0pt}\rule{\textwidth}{0.5em}}{~\\\rule{\textwidth}{0.5em}}
#+LATEX_HEADER: \everymath{\displaystyle}


* TODO
** TODO Definir matemáticamente el problema de la clasificación 
** TODO ¿Pueden coexistir desbalanceo intrínseco/extrínseco con desbalanceo relativo/debido a instancias raras?
** TODO La notación del undersampling y oversampling quizás no es lo suficientemente buena
** TODO Informed undersampling - NearMiss3. No se entiende demasiado bien
** TODO Sampling basado en clústering: ¿el tamaño de clúster está prefijado?
** TODO Se repite el proceso del CBO hasta que sólo se actualia la media de un clúster (?)
** TODO Pasar a ordenador la primera parte de las notas de trabajo, actualmente en papel.
** TODO Buscar una traducción adecuada a Recall
** TODO Cuidado! La definición PAC de Valiant establece complejidad polinómica en $\frac{1}{\epsilon}$ y $\frac{1}{\delta}$, pero se demuestra a posteriori que la definición dada por el libro UML equivale. También habla de complejidad computacional, aunque esto el libro lo separa.
* Memoria
* Parte informática
** Jonatthan Ortigosa-Hernández, Iñakinza, Jose A.Lozano
** MWMOTE
** RWO
** RACOG, WRACOG
** JMLR2015, p.2695
** DBSMOTE 2012 Appl Intell
** PDFOS
* Notas de trabajo
** Small Disjuncts Review
Este material es un resumen de lo encontrado en cite:he09_learn_from_imbal_data
*** Introducción

El problema de la clasificación consiste en datos un conjunto de
datos...

Cualquier distribución desbalanceada de clases en el problema de la
clasificación puede considerarse un problema de clasificación con
desbalanceo, pero suele considerarse por convención que hay
desbalanceo cuando hay ratios de representación de clases de 1:100,
1:1000, 1:10000. Los ratios no tienen por qué referirse únicamente a
clasificación binaria, sino que pueden afectar a problemas multiclase.

El desbalanceo puede ser intrínseco, cuando lo que da lugar al mismo
es la naturaleza del espacio de muestreo, y extrínseco, en caso de que
los desbalanceos se produzcan por factores externos a la naturaleza
del espacio de muestreo como restricciones de almacenamiento y
capacidad de muestreo.

En muchas ocasiones, al realizar análisis de datos no se tiene en
cuenta el desbalanceo existente entre clases, lo cual hace que los
algoritmos no reflejen adecuadamente la distribución de clases. Esto
provoca que los algoritmos, en los ejemplos en que nos interesa una
ratio de predicción acertada alto en la clase minoritaria, no tengan
los resultados deseado.

Puede asimismo hacerse una distinción de desbalanceo como desbalanceo
relativo y desbalanceo debido a instancias raras (/absolute rarity/,
pequeñas instancias del conjunto de entrenamiento agrupadas en
regiones muy particulares del área de búsqueda). El desbalanceo
relativo es aquel en que la función de distribución de la clases se
conserva al tomar varias muestras aleatorias simples, el desbalanceo
debido a instancias raras no, y éste último está estrechamente
relacionado con el desbalanceo intra-clases (/within-class/) debido a
la distribución en distintos /clusters/ de instancias de una misma
clase en el espacio de exploración. La falta de representatividad de
la distribución de clases por parte del clasificador no sólo se debe
al desbalanceo de clases, sino que se puede deber a un amplio rango de
factores que afectan a la complejidad de los datos (/overlapping/,
falta de representatividad en los datos, /small disjuncts/, etc).

El desbalanceo intra-clases se encuentra estrechamente relacionado con
el problema de los /small-disjuncts/. En general, los clasificadores
intentan aprender a partir de una clase creando reglas disjuntas que
afecten a /clusters/ de instancias. Como consecuencia de la
infrarrepresentación de instancias en el caso de clases heterogéneas
(repartidas en varios /clusters/), podemos tener reglas que cubren una
pequeña porción de las instancias de una clase, esto es
/small-disjuncts/. Los /small-disjuncts/ no sólo afectan a la clase
minoritaria, sino que pueden darse también dentro de la mayoritaria,
aunque la mayor densidad de datos de esta clase hace que el efecto no
sea tan agravado o sea una situación menos frecuente. El gran desafío
en la identificación de los /small-disjunts/ es identificar todas las
agrupaciones minoritarias de una clase, sin generar también reglas de
clasificación para los datos que representan ruido. Por tanto, en
problemas con alta dimensionalidad y baja densidad de muestreo,
también encontramos /small-disjunts/.

*** Notación

\[S=\{(x_{i,1}, \ldots x_{i,m}, y_i)\, i=1,\ldots m \}\] \[(x_{i,1},
\ldots x_{i,m})\in X\] \[y_i \in \{1\ldots C\}\]

con $S$ muestra aleatoria de una variable, $X$ espacio de
características e $Y$ conjunto de clases finito con $C\ge 2$.

Notamos $S_{min}$ a los ejemplos de la clase minoritaria, $S_{maj}$ a
los de la mayoritaria. Se verifica $S_{min}\cap S_{maj} = \emptyset$

Llamamos $E$ al conjunto de instancias generadas mediante técnicas de
/sampling/ y $E_{maj}$, $E_{min}$ a las etiquetadas como de las clases
mayoritaria y minoritaria, respectivamente.

*** Oversampling y undersampling

En el /oversampling/ seleccionamos un conjunto $E\subseteq S_{min}$ y
lo adherimos a $S_{min}$. El /undersampling/ consiste en eliminar un
subconjunto de $S_{maj}$

**** /Informed undersampling/

Ejemplos de esta técnica son los algoritmos:

***** EasyEnsemble

Método de aprendizaje no supervisado, que genera un conjunto de
clasificadores tomando muestras aleatorias con remplazamiento de la
clase mayoritaria, juntándolas con las instancias de la clase
minoritaria originales y generando un clasificador.

***** BalanceCascade

Toma $E$ verificando $|E_{maj}| = |S_{min}|$, e inferimos un
clasificador $H_1$ desde $N_1={E_{maj}\cup S_{min}}$. Por inducción,
el clasificador $n$-ésimo lo entrenamos a partir del $(n-1)$-ésimo
llamando $N_{maj}^{*}$ los ejemplos de $S_{maj}$ correctamente
clasificados por $H_1$, eliminándolos de $S_{maj}$ y tomando de nuevo
un $N_n={E_{maj}\cup S_{min}}$

***** NearMiss-1

Elimina aquellos ejemplos de la clase mayoritaria cuya distancia media
a 3 vecinos más cercanos de la clase minoritaria es mínima.

***** NearMiss-2

Elimina aquellos ejemplos de la clase mayoritaria cuya distancia media
a 3 vecinos más lejanos de la clase minoritaria es mínima.

***** NearMiss-3

Elimina un número dado de ejemplos de la clase mayoritaria más
cercanos a cada ejemplo de la clase minoritaria.

***** One-sided selection (OSS)

**** /Sampling/ artificial

***** Synthetic minority oversampling technique(SMOTE)

Genera instancias etiquetadas como minoritarias haciendo

\[y = x_i + (\widehat{x_i}-x_i)\cdot \delta\]

Con $x_i\in S_{min}$, $\delta \in[0,1]$, $\widehat{x_i}$ una instancia
de entre sus $K$ vecinos máx cercanos en $S_{min}$ con $K$ prefijado.
**** Adaptative Synthetic Sampling

SMOTE no tiene en cuenta el vecindario de las instancias minoritarias
para generar una nueva a partir de ellas. Así, una instancia aislada
que bien podría representar ruido, generaría otra instancia a partir
de ella, y en conjunto, si esta circunstancia es asidua, se generaría
/overlapping/.

***** Borderline-SMOTE

Fijado $K$ etiqueta como ejemplos de la frontera aquellos $x_i\in
S_{min}$ verificando que tienen más vecinos de la clase mayoritaria
que de la clase minoritaria. Salvo si su número de vecinos más cercano
de la clase mayoritaria es $K$ (en cuyo caso la instancia se etiqueta
como ruido y se elimina a efectos de buscar el clasificador), en otro
caso se generan instancias artificiales a partir de los elementos
"fronterizos" mediante SMOTE.

***** ADASYN

Toma \(G = (|S_{maj}| - |S_{min}|)\cdot \beta \) done \(\beta\)
representa el nivel de balanceo buscado después del algoritmo. Para
cada $x_i$ buscar sus $K$ vecinos más cercanos, con $K$ prefijado y
tomar:

\[\Gamma_i = \frac{\Delta_i}{\sum_i^{|S_{min}|} \Delta_i}, \qquad
i=1,\ldots, |S_{min}|\]

Para dicho $x_i$ necesitamos generar $g_i = \Gamma_i \cdot G$
instancias.

**** Samplig con técnicas de limpieza

Se usan los links Tomek. Dos instancias $x_i, x_j$ forman un link de
Tomek si $x_i \in S_{min}, x_j \in S_{maj}$ y no existe $x_k$
verificando $min(d(x_i, x_k), d(x_j, x_k)) < d(x_i, x_j)$. Los links
de Tomek representan /overlapping/ y por tanto basta eliminarlos
después de haber hecho /oversampling/.

***** SMOTE + ENN (Edited Nearest Neighbour)
***** SMOTE + enlaces Tomek

**** Sampling basado en /clustering/
***** CBO
El algoritmo CBO (Cluster Based Oversampling) usa el algoritmo
$K$-means con $K$ prefijado para calcular los clústers del conjunto de
entrenamiento. Posteriormente hace /oversampling/ de los demás
/clusters/ de forma que las dos clases queden balanceadas, y todos los
/clusters/ de la clase mayoritaria tengan el mismo número de
elementos, y los de la clase minoritaria también. Esto elimina el
desbalanceo /within-class/ y el /between-class/.

**** Sampling + Boosting
***** SMOTEBosst 
Combina AdaBoost.M2 + SMOTE
***** DataBoost-IM
Genera instancias artificiales de acuerdo al ratio dificultad de
aprendizaje entre clases
***** JOUS-Boost
Aplica boosting donde a cada paso, en el /oversampling/ que hace de
instancias minoritarias introduce /jittering/ (ruido en las
componentes que selecciona de forma uniforme con media 0) ****

*** Cost-Sensitive
Llamamos $C(i,j)$ al coste de clasificar una instancia de la clase $j$
como de la clase $i$, donde $C(i,i)= 0. En el claso de clasificación
binaria, tendríamos $i,j \in \{Min, Maj\}$ clases minoritarias y
mayoritarias.

El riesgo condicional viene dado por la fórmula $R(i|x) = \sum_j
P(j|x)C(i,j)$ 

Los métodos de esta categoría se clasifican principalmente en:

**** TODO /Cost-Sensitive Dataspace Weighting/
Se construyen sobre el teorema de translación.

***** AdaC1, AdaC2, AdaC3
Modifican AdaBoost.M1, cambiando la función de distribución de los
datos con las iteraciones, pero introduciendo un factor de
coste. Estos algoritmos incrementan la probabilidad de elegir al hacer
/sampling/ los ejemplos que más clasifica incorrectamente el
clasificador.

**** TODO Metacost-Sensitive

**** TODO Diseño de componentes
Adapta el coste a los paradigmas de ciertos clasificadores.

***** Árboles de decisión cost-sensitive
En presencia de desbalanceo de clases, la poda de árboles tiende a
eliminar las hojas describiendo el concepto minoritario. Por ello la
poda resulta muy negativa al aplicarla sobre los árboles de decisión,
pero el uso de árboles sin podar tampoco resuelve el problema de los
/small disjuncts/ por ejemplo, porque se produce /overfiting/. Los
esfuerzos se centran en mejorar el estimador probabilístico de cada
clase en los nodos del árbol.

***** Redes neuronales cost-sensitive 

***** Redes bayesianas cost-sensitive

***** Máquinas de soporte vectorial cost-sensitive

*** Métodos basados en kernel y métodos de aprendizaje activo

**** Framework de aprendizaje basado en núcleo

***** SVMs
Problema de las máquinas de soporte de vectores es que tienden a
clasificar los ejemplos como pertenecientes a la clase mayoritaria,
para maximizar la tasa de acierto.

**** Sampling hibridado con métodos basados en kernel

***** SDCs: SMOTE with different costs

***** Over/undersampled SVMs

***** SVMs con clasificación errónea asimétrica(SVMs with asymmetric misclassification)

***** Granular Support Vector Machines (GSVMs)

Se basan en los principios de la teoría del aprendizaje estadístico y
de la teoría de computación granular.

Tienen como ventajas frente a los SVMs mejor eficiencia computacional,
debido al uso de paralelismo.

Destacan en este grupo los **GSVM-RU**

**** Métodos de modificación de kernels para aprendizaje desbalanceado

Se centran en modificar SVM. Hay un kernel basado a su vez en OFS y
ROWLS.

***** OFS: Orthogonal Forward Selection

Integra ideas de LOO (*Leaving-One-Out*) y AUC (Área bajo la curva)

***** ROWLS: Orthogonal Weigthed Least Squares

Usado para asignar mayor peso a los ejemplos erróneos de la clase
minoritaria.

***** Métodos para ajustar la frontera de los SVM: BM, BPs, CBA, KBA

Destaca especialmente KBA, que realiza una aproximación al problema
modificando la matriz del kernel en el espacio de caracterísicas.

***** Método SVM basado en Kernel difuso (TAF-SVM)

Tiene como ventajas que maneja bien el *overfitting* debido a la
*fuzzificación* de los datos de entrenamiento, su adaptabilidad a
diferentes distribuciones

***** PSVM: SVM proximal $k$ -categórica ($k$ -category proximal support vector machine)

Tiene como gran ventaja su rapidez, puesto que su funcionamiento se
basa en la resolución de un sistema de $k$ ecuaciones lineales.

***** Modificación de Raskutti y Kowalcyzk 

**** Métodos de aprendizaje activo para aprendizaje desbalanceado

***** Aproximación SALH

La idea fundamental de este método es proporcionar un modelo genérico
para la evolución de los clasificadores basados en programación
genética, integrando el *subsamplimg* estocástico y una función de
coste *Wilcoxon-Mann-Whitney(WMW)* modificada.

*** Otros métodos para aprendizaje desbalanceado

**** Aprendizaje de una clase (one-class learning)

Estudios han ilustrado que este tipo de métodos son muy efectivos para
tratar con datasets tremendamente desbalanceados y con alta
dimensionalidad.

**** Mahalanobi-Taguchi System (MTS)

*** Medida de la bondad de los métodos

|   | p     | n     |
| Y | TP    | FP    |
| N | FN    | TN    |
|   | $p_c$ | $n_c$ |


Donde $p$ y $n$ representan la verdadera clase: positiva y negativa, y
$Y$, $N$ la clase de la hipótesis.

\[ Exactitud = \frac{TP+TN}{P_C+N_C} \hspace{3em} Ratio_{error} = 1 -
Exactitud \]

En general estas dos medidas resultan suficientes para expresar la
bondad de los métodos. Pero en algunos casos pueden resultar
engañosas, y ser muy sensibles a cambios en los datos.

Por ejemplo, si un *dataset* tiene 95% de datos pertenecientes a la
clase mayoritaria, y 5% a la minoritaria, si clasificáramos todos los
ejemplos como de la clase mayoritaria, obtendríamos un 95% de
precisión, pero no clasificaríamos bien ni un solo ejemplo de la clase
minoritaria.

Por convenio llamaremos a la clase mayoritaria, clase positiva; y a la
clase minoritaria, clase negativa.

Observamos que la exactitud tiene en cuenta tanto el total de la clase
mayoritaria como minoritaria. Por tanto depende de la distribución de
datos de nuestro *dataset*, y no va a ser una medida adecuada para
medir la bondad de métodos de aprendizaje desbalanceado.

\begin{eqnarray}
&& Precision = \frac{TP}{TP+FP}\\ && Recall = \frac{TP}{TP+FN}\\ &&
F-Measure = \frac{(1+\beta)^2\cdot Recall \cdot
Precision}{\beta^2\cdot Recall + Precision} \end{eqnarray}


Donde $\beta$ indica un coeficiente para ajustar la importancia de la
precisión frente a *Recall*:

\[ G-mean = \sqrt{\frac{TP}{TP+FN} \cdot \frac{TN}{TN+FP}} \]

- Precisión refleja la exactitud de los datos
- *Recall* refleja la completitud de los datos
  
** Clasificación con CCCD
Resumido desde el artículo cite:JMLR:v17:15-604
*** Concepto de CCCD (Class Cover Catch Digraphs)

Sea $(\Omega, M), \Omega = \mathbb{R}^d$ un espacio de medida y sea $\mathcal{X}_n = \{x_1, x_2, \ldots x_n\} \subseteq \Omega$, $\mathcal{Y}_m = \{y_1, \ldots y_m\} \subseteq \Omega$ observaciones para dos clases $X, Y$ con sus respectivas funciones de distribución $F_X, F_Y$ y función de distribución conjunta $F_{XY}$. Asumimos que la clase positiva es la clase $\mathcal{X}$. Para cada $x_i \in \mathcal{X}_n$ consideramos un radio $r_i$, y la bola abierta $B(x_i, r_i)$. $x_i$ se dice que cubre a $x_j$ si $x_j$ está en $B(x_i, r_i)$, considerando el espacio métrico con la distancia euclídea. Los CCCD son grafos dirigidos $(V,A)$ donde $V = \mathcal{X}_n$ y $(u,v) \in A \Leftrightarrow v\in B(u, r_u)$

Dado un subconjunto $Q_{\mathcal{X}} \subseteq \mathcal{X}_n$, queremos encontrar la cobertura de menor número de bolas de entre $\{B_1, \ldots B_n\}$ asociadas a los puntos de $\mathcal{X}_n$
Si $Q_{\mathcal{X}} = \mathcal{X}_n$ la cobertura se llama /propia/. Si $Q_{\mathcal{X}} \subset \mathcal{X}_n$ estrictamente, la cobertura se llama /impropia/.

Se define el vecindario de un punto $s\in V$ como:

\[N(s) = \{t \in V : (s,t) \in A\}\]

Un conjunto dominante de un grafo dirigido $(V,A)$ es un conjunto $S\subseteq V$ tal que $\cup_{s \in S} N(s) = V$

*** Pure-CCCDs (PCCDs)
Las coberturas no contienen puntos que no pertenecen a $\mathcal{X}_n$

Definen:

\[r(x):= (1-\tau) d(x,l(x)) + \tau d(x,u(x))\]
\[u(x):= argmin_{y\in \mathcal{Y}_m} d(x,y)\]
\[l(x):= argmax\{d(x,z): d(x,z) < d(x,u(x))\}\]


#+begin_theorem
Encontrar la cobertura mínima aproximada por greedy de $\mathcal{X}_n$, con clase negativa $\mathcal{Y}_m$ es $\mathcal{O}(n(n+m)d)$. Además esta cobertura es de tamaño a lo sumo $\mathcal{O}(n)$ más grande que la óptima.
#+end_theorem

Un clasificador P-CCCD encuentra las coberturas $C_\mathcal{X}$ y $C_\mathcal{Y}$, por tanto es de complejidad $\mathcal{O}((n+m)^2 d)$

Dado un punto del $z$ espacio muestral, caben 3 posibilidades:

1. Está o en $C_\mathcal{X}$ o en $C_\mathcal{Y}$, pero no en ambas.
2. Está en ambas coberturas.
3. No está en ninguna.

La clase se estima por:

\[argmin_{C \in C_{\mathcal{X}}, C_{\mathcal{Y}}} \Bbig[ min_{x \in B(x,r) \in C} \rho(z,x) \Bbig]\]

donde $\rho(z,x) = \frac{d(z,x)}{r(x)}$.

Los clasificadores PCCDs aproximan al clasificador de Bayes cuando $F_{\mathcal{X}}$ y ${\mathcal{Y}}$ y las clases son separables, es decir, $min_{x\in \mathcal{X}_n, y\in \mathcal{Y}_m} d(x,y) > 0$.

** Ejercicios
*** 2.1

#+begin_wording
/Overfitting con polinomios/. Prueba que dado un conjunto $S=\{(x_i, f(x_i))\}_{i=1}^m \subseteq (\mathbb{R}^d \times \{0,1\})^m$ existe un polinomio $p_S$ verificando $h_S(x)=1$ sii $p_S(x)\ge 0$ con:

\[h(x) = \left\{\begin{array}{lcl}
y_i && \exists i : x_i=x\\
0   && si\quad no
\end{array}\right.\]

De aquí se deduce que la clase de funciones de umbrales polinómicos usando el paradigma ERM puede llevar a /overfitting/
#+end_wording

Tomamos:

\[p_S(x) = -\prod_{i : f(x_i)=1} (x-x_i)^2\]

Es un polinomio que se anula en los puntos donde $h_S(x)=1$ y tiene un valor menor que 0 en otro caso.

*** 2.2

#+begin_wording
Sea $\mathcal{H}$ clase de clasificadores binarios sobre un dominio $\mathcal{X}$. Sea $\mathcal{D}$ una distribución desconocida sobre $\mathcal{X}$. Sea $f$ una hipótesis objetivo en $\mathcal{H}$. Se fija $h\in \mathcal{H}$. Probar que: 

\[\mathbb{E}_{S\sim \mathcal{D}} [L_S(h)] = L_{\mathbb{D},f}(h)\]
#+end_wording

Llamamos $P=\mathbb{P}_{x\sim \mathcal{D}}(f(x)\neq h(x))$

\begin{align*}
\mathbb{E}_{S\sim \mathcal{D}} [L_S(h)] &= \sum_{k=0}^m \frac{k}{m} \binom{m}{k} P^k(1-P)^{m-k} = \sum_{k=1}^m \frac{k}{m} \binom{m}{k} P^k(1-P)^{m-k} =\\
&= \sum_{k=1}^m \binom{m-1}{k-1} P^k(1-P)^{m-k} = \sum_{k=0}^{m-1} \binom{m-1}{k} P^{k+1}(1-P)^{m-1-k} = \\
&= P\cdot \sum_{k=0}^{m-1} \binom{m-1}{k} P^{k}(1-P)^{m-1-k} = P(1+(1-P))^{m-1} = P
\end{align*}
*** 2.3

#+begin_wording
*Clasificadores de rectángulo*

Un clasificador de rectángulo es un clasificador que asigna 1 a los puntos que se quedan dentro de un cierto rectángulo en el plano. 

\[h_{a,b,c,d}(x,y) = \left\{\begin{array}{lcl}
1 && a\le x\le b, c\le y\le d\\
0 && si \quad no
\end{array}\right.\]

La clase de clasificadores de rectángulo en el plano se define por:

\[\mathcal{H}^2_{rec} = \{ h_{a,b,c,d}: a\le b, c\le d\}\]

Asumiremos propiedad de factibilidad.

1. Sea $A$ el algoritmo que devuelve el rectángulo más pequeño que engloba a todos los ejemplos positivos del conjunto de entrenamiento. Prueba que $A$ es un ERM.
2. Probar que si $A$ recibe un conjunto de entrenamiento de tamaño mayor o igual que $\frac{4}{\epsilon}log\left(\frac{4}{\delta}\right)$ entonces con probabiliad al menos $1-\delta$ devuelve una hipótesis con error no superior a $\epsilon$.
3. Generaliza a rectángulos en $\mathbb{R}^d$
4. Probar que el tiempo de aplicación del algoritmo $A$ anterior es polinomial en $d$, $1/\epsilon$, y en $log(1/\delta)$.
#+end_wording

1. 

Partiendo de la propiedad de factibilidad, debe existir un clasificador de rectángulo $\bar{h} = h_{a,b,c,d}$ que haga el ERM nulo y que verifique $L_{\mathcal{D},f}(\bar{h})$. Por tanto debe verificarse que para un conjunto de entrenamiento $S$, $h_S$ debe contener a todos los ejemplos positivos del conjunto de entrenamiento, ya que si valiese 0 en algún ejemplo positivo del conjunto de entrenamiento, el ERM sería mayor que 0.

El algoritmo que devuelve el mínimo rectángulo que engloba a todos los ejemplos positivos es por tanto un ERM.

2.

Sea $R^{\ast} = R(a,b,c,d)$ el rectángulo del apartado 1. Entonces $P_{S\sim \mathcal{D}^2}[f(R^{\ast})=\{1\}] = 1$

Tomamos $R_1 = R(a,a^{\ast},c,d)$ un rectángulo que concentra una masa de probabilidad menor o igual a $\epsilon/4$, con $a\le a^{\ast}$.

$R_2=(b^{\ast},b,c,d), R_3=(a,b,c,c^{\ast}), R_4=(a,b,d^{\ast},d)$ se definen de forma análoga.


Llamando $h_R=A(S)$, $R$ el rectángulo obtenido como resultado de aplicar el algoritmo del ejercicio. Es claro que con probabilidad 1, $R\subset R^{\ast}$. 

Si se tiene $\forall i : R\cap R_i \neq \emptyset$:

\begin{align*}
L_{\mathcal{D},f}(h_R) &= P_{x\sim \mathcal{D}}[h_R(x)\neq f(x)] \le P_{x\sim \mathcal{D}}\left(\cup_i [h_R(x)\neq f(x)]\cap R_i\right) \le\\
&\le P_{x\sim \mathcal{D}}\left(\cup_i R_i\right) \le 4\frac{\epsilon}{4} = \epsilon
\end{align*}


La demostración acaba probando que:

\[P(\exists i : S\cap R_i = \emptyset) \le \sum_{i=1}^4 P(S\cap R_i = \emptyset) = 4(1-\frac{\epsilon}{4})^m \le 4e^{-m}\]

3. 
En $\mathbb{R}^d$ podríamos obtener el mismo resultado tomando $m \ge \frac{2d}{\epsilon}log\left(\frac{2d}{\delta}\right)$, repitiendo una demostración análoga.

4.

Sea 

\begin{align*}
m &= \frac{2d}{\epsilon}log\left(\frac{2d}{\delta}\right) = \frac{2d}{\epsilon}\left[log(2d) + log\left(\frac{1}{\delta}\right)\right] \le \\
&\le \frac{2d}{\epsilon}\left[2d + log\left(\frac{1}{\delta}\right)\right] = p(d, 1/\epsilon, log(1/\delta))
\end{align*}


Fijada una componente de entre las $d$ posibles, los algoritmos de fuerza bruta para buscar el máximo y el mínimo son $m^2$. Por tanto el algoritmo global tendrá eficiencia:


\[\mathcal{O}(d m^2) \subset \mathcal{O}(d \cdot p(d, 1/\epsilon, log(1/\delta))^2)\]

*** 3.1
#+begin_wording
*Monotonía de la complejidad muestral*

Sea $\mathcal{H}$ una clase de hipótesis para clasificación binaria. Suponer que $\mathcal{H}$ es PAC learnable y su complejidad muestral está dada por $m_{\mathcal{H}}$. Probar que $m_{\mathcal{H}}$ es monótona decreciente en cada variable.
#+end_wording

Llamo $\gamma = inf_{h'\in \mathcal{H}} L_{\mathcal{D}} (h')$

Fijado $1 > \delta > 0$, y dados $\epsilon_1 < \epsilon_2$. Sea $m \ge m_{\mathcal{H}}(\epsilon_1, \delta)$ verificando que dado $S\sim \mathcal{D}^m$ entonces:

\[\mathbb{P}_{S \sim \mathcal{D}^m} (L_{\mathcal{D}}(h) - \gamma \le \epsilon_1}) \ge 1-\delta\]

Entonces por monotonía de la función de probabilidad:

\[\mathbb{P}_{S \sim \mathcal{D}^m} (L_{\mathcal{D}}(h) - \gamma \le \epsilon_2}) \ge \mathbb{P}_{S \sim \mathcal{D}^m} (L_{\mathcal{D}}(h) - \gamma \le \epsilon_1}) \ge 1-\delta \]

Análoga la demostración en la primera variable.

*** 3.2
#+begin_wording
Sea $\mathcal{X}$ un conjunto discreto, y $\mathcal{H}_{singleton} = \{h_z : z\in \mathcal{X}\} \cup \{h^{-}\}$ donde para cada $z\in \mathcal{X}$, $h_z(x) = 1, x=z$ y $h_z(x) = 0, x\neq z$. $h^{-}$ es la hipótesis nula. Se verifica que la verdadera función de etiquetado etiqueta todas las instancias del dominio negativamente, excepto quizás una.

1. Describe un algoritmo que implemente ERM para aprender $\mathcal{H}_{singleton}$
2. Demostrar que $\mathcal{H}_{singleton}$ es PAC learnable. Dar una cota superior a la complejidad muestral.
#+end_wording

1. 

Tomamos el algoritmo que devuelve $h^{-}$ en caso de que $\not\exists (x,y) \in S, y=1$, o $h_x$ si $\exists (x,y) \in S : y=1$.

2.

Por hipótesis de factibilidad $\exists h \in \mathcal{H}_{singleton}$ verificando que $P_{x\sim \mathcal{D}} [h = f] = 1$

Si $L_{\mathcal{D}} (ERM(S)) > \epsilon$ entonces como a lo sumo falla en una instancia, $z \in \mathcal{X}$, con $P_{x\sim \mathcal{D}} (x = z) > \epsilon$, lo que significa que esa instancia no está presente entre ninguna de las $m$ muestras del conjunto de entrenamiento.

\[P_{S\sim \mathcal{D}^m}(L_{\mathcal{D}} (ERM(S)) > \epsilon) \le (1-\epsilon)^m \le e^{-m}\]

Por tanto una cota superior para la complejidad muestral es $\left \lceil \frac{log(1/\delta)}{\epsilon} \right\rceil$
*** 3.3
#+begin_wording
Sea $\mathcal{X} = \mathbb{R}^2$, $\mathcal{Y} = \{0,1\}$ y sea $\mathcal{H}$ la clase de círculos concéntricos en el plano, esto es, $\mathcal{H} = \{h_r : r\in \mathbb{R}^{+}\}$, con $h_r(x) = 1_{|x| \le r}$. Probar que $\mathcal{H}$ es PAC learnable asumiendo propiedad de factibilidad si su complejidad muestral está acotada por:

\[m_{\mathcal{H}}( \epsilon, \delta ) \le \left\lceil \frac{log(1/\delta)}{\epsilon} \right\rceil\]

#+end_wording


Fijamos $\mathcal{D}$ una distribución sobre $\mathcal{X}$. Tomamos el algoritmo que devuelve para un conjunto de entrenamiento $S$ la hipótesis $h_r$ donde $r = argmax_{x \in S} |x|$

Escogemos $0 < \delta, \epsilon < 1$, y $m \ge m_{\mathcal{H}}(\delta, \epsilon)$

Sea $R > 0$ verificando $P_{x\sim \mathcal{D}}(h_R = f) = 1$. Sea $0 < r < R$ verificando $P_{x\sim \mathcal{D}}(h_r = h_R) = 1 - \epsilon$.

\[P_{S \sim \mathcal{D}^m} (L_{\mathcal{D}, f} (A(S)) > \epsilon) = \bbig( P_{x\sim \mathcal{D}}( |x| < r ) \bbig)^m \le (1-\epsilon)^m \le e^{-\epsilon m} < \delta\]
** PAC learning
Adaptación del contenido del libro cite:shwartz_understanding_ml
*** Introducción
Damos unas notaciones/definiciones básicas que utilizaremos de aquí en adelante.

- *Dominio*: $\mathcal{X}$, sobre el que tenemos definida una $\sigma$ álgebra de conjuntos $\mathscr{B}$. Llamamos una instancia a $x\in \mathcal{X}$
- *Conjunto de etiquetas*: $\mathcal{Y} \subseteq \mathbb{R}$ finito , que asumiremos como $\{0,1\}$ en lo que sigue hasta que se indique lo contrario. Esto nos restringe al paradigma de clasificación binario.
- *Verdadero etiquetado*: Asumimos la existencia de una función $f: \mathcal{X} \rightarrow \mathcal{Y}$ que devuelve el verdadero etiquetado de todas las instancias.
- *Generación de instancias*: Asumimos la existencia de una distribución de probabilidad $\mathcal{D}$ sobre $\mathcal{X}$, para la $\sigma$ álgebra de conjuntos mencionada anteriormente, que nos da información sobre la probabilidad de extraer cada posible instancia desde $\mathcal{X}$.
- *Conjunto de entrenamiento*: Tenemos una muestra aleatoria simple $S = (\mathcal{X}_1, \ldots ,\mathcal{X}_m)$, idéntica e independientemente distribuida, donde $S \sim \mathcal{D}^m$, esto es cada $X_i$ sigue la misma distribución que $\mathcal{X}$, $X_i \sim \mathcal{D}$, y las distribuciones marginales son independientes entre sí. Notaremos $S_x$ a una realización muestral $(x_1, \ldots x_m)$. Cada elemento $x_i$ de una realización muestral $S_x = (x_1, \ldots x_n)$ se etiqueta por $f$, y llamando $f(x_i) = y_i$ definimos como conjunto de entrenamiento a la tupla $((x_1, y_1), \ldots ,(x_m, y_m))$. La relación entre la realización muestral y el conjunto de entrenamiento asociado es biunívoca, por lo que por abuso de notación llamaremos indiferentemente conjunto de entrenamiento a ambas tuplas.
- *Resultado del aprendizaje*: disponemos de un algoritmo de aprendizaje $A: (\mathcal{X} \times \mathcal{Y})^m \rightarrow \mathcal{Y}^{\mathcal{X}}$ que recibe un conjunto de entrenamiento y devuelve una función $h: \mathcal{X} \rightarrow \mathcal{Y}$ que llamaremos hipótesis/clasificador. El algoritmo "desconoce" el valor de la verdadera función de etiquetado $f$ en los puntos no pertenecientes al conjunto de entrenamiento.
- *Error del clasificador*: Definimos el error de un clasificador $h: \mathcal{X} \rightarrow \mathcal{Y}$ como:

\[L_{\mathcal{D},f}(h) :=  P (\{x\in \mathcal{X} : h(x)\neq f(x)\}) = P[f\neq h]\]

Por simplificar la escritura, omitiremos a partir de ahora el hecho de que sobre $\mathcal{X}$ tenemos definida una $\sigma$ álgebra de conjuntos, $\mathscr{B}$, y que todas las distribuciones asignan probabilidad a los conjuntos de alguna $\sigma$ álgebra que contenga a $\mathscr{B}$. Además, consideraremos que la función de verdadero etiquetado y los clasificadores son funciones medibles para que la definición de los errores sea correcta.

**** Minimización del riesgo empírico (ERM)

#+begin_definition
*Riesgo empírico (ER)*

Fijado un clasificador $h : \mathcal{X} \rightarrow \mathcal{Y}$, definimos el riesgo empírico o error empírico, como una variable aleatoria:

\[\begin{array}{llll}
L_S(h): & \mathcal{X}_1 \times \ldots \times \mathcal{X}_m & \rightarrow &\mathbb{R}\\
& (x_1, \ldots x_m) & \mapsto & L_{S_x} (h) = \frac{\#\{i\in {1\ldots m}: h(x_i) \neq f(x_i)\}}{m}
\end{array}\]
#+end_definition

Para un conjunto de entrenamiento el riesgo empírico proporciona el error del clasificador sobre el conjunto de entrenamiento.

Un algoritmo que obtiene una hipótesis que minimiza el error empírico sobre un conjunto de entrenamiento recibe el nombre de ERM y notamos $ERM(S_x)$ al clasificador obtenido con dicho algoritmo.

Este error no es siempre óptimo. Pensemos en el siguiente ejemplo:

Sea $\mathcal{X} = \mathbb{R}$, $\mathcal{D}$ la distribución uniforme sobre $[0,2]\subset \mathbb{R}$, y la siguiente función:

\[f(x) = \left\{\begin{array}{lcl}
1 && x\in [0,1]\\
0 && x\in \mathbb{R}\setminus [0,1]
\end{array}\right.\]


Sea $((x_1,y_1), \ldots (x_m, y_m))$ un conjunto de entrenamiento de tamaño $m$ y el clasificador:

\[h(x) = \left\{\begin{array}{lcl}
y_i && \exists i\in \{1\ldots m\} : x=x_i\\
0 && \nexists i\in \{1\ldots m\} : x=x_i
\end{array}\right.\]

Nótese que el conjunto de entrenamiento no puede tener elementos no repetidos puesto que se etiquetan mediante $f$, que es una función y no puede arrojar dos imágenes distintas para un mismo $x \in \mathcal{X}$ de entrada.

Este clasificador es perfecto respecto a la minimización de riesgo empírico, pero $L_{\mathcal{D}, f}(h) = 1/2$. Es decir, tiene el mismo nivel de acierto que el clasificador idénticamente 1. A este fenómeno, minimizar el riesgo empírico siendo un clasificador con un error muy alto, lo denominamos /overfitting/.

El hecho de tomar el error sobre el conjunto de entrenamiento como aproximación al verdadero error del clasificador se respalda por la siguiente proposición:

#+begin_fact
*Relación entre riesgo empírico y error del clasificador*

Sea $\mathcal{H}$ clase de clasificadores binarios sobre un dominio $\mathcal{X}$. Sea $\mathcal{D}$ una distribución desconocida sobre $\mathcal{X}$. Sea $f$ la función de verdadero etiquetado. Para $h\in \mathcal{H}$ fijo se verifica:

\[\mathbb{E} [L_S(h)] = L_{\mathcal{D},f}(h)\]
#+end_fact

Llamamos $p=P [f \neq h ] = L_{\mathcal{D},f}(h)$

\begin{align*}
\mathbb{E} [L_S(h)] &= \sum_{k=0}^m \frac{k}{m} \binom{m}{k} p^k(1-p)^{m-k} = \sum_{k=1}^m \frac{k}{m} \binom{m}{k} p^k(1-p)^{m-k} =\\
&= \sum_{k=1}^m \binom{m-1}{k-1} p^k(1-p)^{m-k} = \sum_{k=0}^{m-1} \binom{m-1}{k} p^{k+1}(1-p)^{m-1-k} = \\
&= p\cdot \sum_{k=0}^{m-1} \binom{m-1}{k} p^{k}(1-p)^{m-1-k} = p(1+(1-p))^{m-1} = p
\end{align*}


**** ERM con /sesgo inductivo/
 
Con objeto de corregir el ERM, para evitar /overfitting/, usamos el conocimiento previo sobre el problema (la información que dispongamos sobre el dominio, la distribución, etc) restringiendo el espacio de búsqueda, esto es, la clase de hipótesis $\mathcal{H}$ desde la que el algoritmo puede escoger un $h: \mathcal{X}\rightarrow \mathcal{Y}$. Llamamos a esto *sesgo inductivo* puesto que se asumirá una determinada clase de funciones $\mathcal{H}$ en función de las características del problema.

Notaremos a un clasificador obtenido con este paradigma $h_{S_x} := ERM_{\mathcal{H}}(S_x)$, y lo definimos de manera que:

\[h_{S_x} \in \underset{h\in \mathcal{H}}{argmin} \{L_{S_x}(h)\}\]

La existencia de $\underset{h\in \mathcal{H}}{min} \{L_{S_x}(h)\}$ está garantizada, ya que $m \cdot L_{S_x}(h) \in \mathbb{N}$ para todo $h\in \mathcal{H}$.

Enunciamos la propiedad de factibilidad, que usaremos más adelante.

#+begin_definition
*Propiedad de factibilidad*

Existe $\bar{h} \in \mathcal{H}$ verificando $L_{\mathcal{D},f}(\bar{h}) = 0$.
#+end_definition

La hipótesis de factibilidad implica que $P [L_S(\bar{h})=0] = 1$, ya que:

\begin{align*}
P (\{(x_1, \ldots x_m): \bar{h}(x_i) = f(x_i), i=1, \ldots m\}) =\\
= \prod_{i=1}^m P [h=f] = \prod_{i=1}^m (1 - P[h\neq f]) = 1
\end{align*}

Por tanto $P [L_S(h_S)=0]=1$.

Para finalizar estos preliminares remarcamos que el valor de $L_{\mathcal{D},f}(h_{S_x})$ dependerá del conjunto de entrenamiento, extraído y etiquetado a partir del vector aleatorio $S$, y la elección del mismo está sometida al azar. Asimismo, necesitamos una medida de la bondad de la predicción.

*** Aprendizaje PAC.

#+begin_definition
*PAC (Probablemente Aproximadamente Correcto) cognoscible*

Una clase de funciones $\mathcal{H} \subseteq \mathcal{Y}^{\mathcal{X}}$ es PAC cognoscible sii existe una función $m_{\mathcal{H}} : ]0,1[^2\rightarrow \mathbb{N}$, llamada complejidad muestral, y un algoritmo $A$ verificando que si $0 < \epsilon, \delta < 1$, entonces para toda distribución $\mathcal{D}$ sobre $\mathcal{X}$ y para toda función de verdadero etiquetado $f:\mathcal{X} \rightarrow \{0,1\}$ cumpliendo la propiedad de factibilidad, ejecutando el algoritmo para un conjunto de entrenamiento generado por $S\sim \mathcal{D}^m$ etiquetado mediante $f$, con $m\ge m_{\mathcal{H}}(\epsilon, \delta)$ se tiene que:

\[P [L_{\mathcal{D},f}(A(S)) \le \epsilon] \ge 1-\delta\]
#+end_definition

Llamamos a $(1-\delta)$ /confianza de la predicción/ y a $(1-\epsilon)$ la /exactitud/. Estos dos parámetros explican el nombre aproximadamente ($\leftrightarrow$ confianza) correcto ($\leftrightarrow$ exactitud).

Podemos considerar $m_{\mathcal{H}}$ única en el sentido de que para cada $(\delta, \epsilon)$ nos devuelva el menor natural verificando las hipótesis del enunciado.

Nótese que las condiciones exigidas, cumplir la propiedad de factibilidad y que la hipótesis devuelta deba estar en $\mathcal{H}$, son muy fuertes. Relajaremos esta definición más adelante con el concepto de PAC agnóstico.

**** Aprendizaje con clases finitas

#+begin_theorem
*Las clases finitas de funciones son PAC cognoscibles*

Sea $\mathcal{H} \subseteq \mathcal{Y}^{\mathcal{X}}$ finito. Sean $0 < \epsilon, \delta < 1$, y un natural $m\in \mathbb{N}$ verificando:

\[m \ge \frac{1}{\epsilon}log\left(\frac{|\mathcal{H}|}{\delta}\right)\]

Entonces para toda función de verdadero etiquetado $f: \mathcal{X}\rightarrow \{0,1\}$, y para toda distribución $\mathcal{X}\sim \mathcal{D}$ para la que se verifique la propiedad de factibilidad, las hipótesis que obtenemos a través del algoritmo ERM son con una confianza superior a $1-\delta$, $1-\epsilon$ exactas.

Como consecuencia, deducimos que la complejidad muestral es menor o igual a $\left\lceil \frac{1}{\epsilon}log \left(\frac{|\mathcal{H}|}{\delta} \right) \right\rceil$
#+end_theorem

#+begin_proof
Fijada una distribución $\mathcal{D}$, $m\in \mathbb{N}$ y una función de etiquetado $f$, notamos:

\[\mathcal{H}_B = \{h\in \mathcal{H}: L_{\mathcal{D},f}(h) > \epsilon\}\]

Se tiene:

\[P [L_{\mathcal{D},f}(h_S) > \epsilon] \le P [\exists h\in \mathcal{H}_B : L_S(h) = 0] \le \sum_{h\in \mathcal{H}_B} P [L_S(h) = 0] \]

La primera desigualdad viene dada porque dada $h_{S_x}$ se verifica, por la propiedad de factibilidad, que $L_S(h_{S_x})=0$. La segunda por subaditividad.

Además, fijada $h\in \mathcal{H}_B$, como $L_{\mathcal{D},f}(h) > \epsilon$:

\begin{align*}
P [L_S(h) = 0] = P (\{(x_1, \ldots x_m) : h(x_i) = f(x_i), i =1,\ldots m\}) =\\
= \prod_{i=1}^m P [h = f] = \prod_{i=1}^m (1 - L_{\mathcal{D},f}(h)) \le (1-\epsilon)^m \le e^{-\epsilon m}
\end{align*}


Las dos desigualdades probadas, junto a la hipótesis del enunciado, y usando $\mathcal{H}_B \subseteq \mathcal{H}$ dan lugar a:

\[P_{S\sim \mathcal{D}^m}[L_{\mathcal{D},f}(h_S) > \epsilon] \le |\mathcal{H}|e^{-\epsilon m} \le \delta\]
#+end_proof

**** Aprendizaje con clases no finitas
 
¿Hay ejemplos de clases infinitas PAC cognoscibles? Veamos un ejemplo.

#+begin_definition
*Clasificadores de rectángulo*

Un clasificador de rectángulo es un clasificador que asigna 1 a los puntos que se quedan dentro de un cierto rectángulo en el plano real.

\[h_{a,b,c,d} = \mathds{1}_{[a,b]\times [c,d]\]

La clase de clasificadores de rectángulo en el plano se define por:

\[\mathcal{H}^2_{rec} = \{ h_{a,b,c,d}: a\le b, c\le d\}\]
#+end_definition


#+begin_fact
*Los rectángulos son PAC cognoscibles*

Asumiendo propiedad de factibilidad, los rectángulos son PAC cognoscibles
#+end_fact

Sea $A$ el algoritmo que devuelve el rectángulo más pequeño que engloba a todos los ejemplos positivos del conjunto de entrenamiento $S_x$.

Partiendo de la propiedad de factibilidad, debe existir un clasificador de rectángulo $\bar{h} = h_{a,b,c,d}$ que haga el ERM nulo y que cumpla $L_{\mathcal{D},f}(\bar{h}) = 0$. Por tanto debe verificarse que $h_{S_x}$ debe acertar en todas las instancias positivas (cuya etiqueta sea 1) del conjunto de entrenamiento, con probabilidad 1, ya que si valiese 0 en algún ejemplo positivo del conjunto de entrenamiento, el ERM sería mayor que 0.

El algoritmo que devuelve el mínimo rectángulo que engloba a todos los ejemplos positivos es por tanto un ERM.

Veamos que con este algoritmo minimizador del ERM la clase de rectángulos es PAC cognoscible.

Sea $R^{\ast} = [a,b]\times [c,d]$ el rectángulo que materializa la propiedad de factibilidad. Fijamos $1 > \epsilon, \delta > 0$.

Tomamos $R_1 = [a,b^{\ast}] \times [c,d]$ un rectángulo verificando $L_{\mathcal{D},f}(\mathds{1}_{R_1}) \le \epsilon/4$, con $a\le b^{\ast} \le b$.

$R_2= [a^{\ast},b] \times [c,d], R_3=[a,b] \times [c,d^{\ast}], R_4=[a,b] \times [c^{\ast},d]$ se definen de forma análoga.


Llamando $h_{R}=A(S)$, $R(S) = R$ el rectángulo obtenido como resultado de aplicar el algoritmo del ejercicio para cada conjunto de entrenamiento, es claro que $P_{S \sim \mathcal{D}^m}[R \subset R^{\ast}] = 1$. 

Supongamos $\forall i : R \cap R_i \neq \emptyset$. Entonces:

\[L_{\mathcal{D},f}(h_R) = P_{x\sim \mathcal{D}} [h_R \neq f] \le P \left(\cup_i [h_R \neq f] \cap R_i\right) \le P \left(\cup_i R_i\right) \le 4\frac{\epsilon}{4} = \epsilon\]

La demostración acaba probando que:

\[P_{S\sim \mathcal{D}^m} [\exists i : R(S)\cap R_i = \emptyset] \le \sum_{i=1}^4 P [R(S)\cap R_i = \emptyset] = 4(1-\frac{\epsilon}{4})^m \le 4e^{-\epsilon m/4}\]

y tomando $m > \frac{4}{\epsilon} log \left( \frac{4}{\delta} \right)$.

*** Generalización aprendizaje PAC: PAC agnóstico
Hasta ahora tenemos dos problemas en la definición de PAC. Intentamos buscar una hipótesis sobre una función de verdadero etiquetado, $f$ determinista, que por tanto no podrá asignar dos imágenes distintas al mismo punto, y además, estamos suponiendo que se cumple la propiedad de factibilidad.

Para paliar esto, podríamos considerar $\mathcal{D}$ como la distribución conjunta sobre $\mathcal{X} \times \mathcal{Y}$, y la noción de error para $h: \mathcal{X} \rightarrow \mathcal{Y}$ quedaría:

\[L_{\mathcal{D}}(h):= P_{(x,y) \sim \mathcal{D}} [h(x) \neq y]\]

Con estos conceptos revisitados, podríamos asegurar que la hipótesis que menor error comete para $\mathcal{Y} = \{0,1\}$ es el llamado *clasificador de Bayes*:

\[f_{\mathcal{D}}(x) = \left\{\begin{array}{ll}
1 & P [y = 1 |x] >= 0.5\\
0 & \quad si \quad no
\end{array}\right.\]

Pero deseamos ir aún más allá, y generalizar la definición para una función de pérdida arbitraria.

#+begin_definition
*Función de pérdida*

Dados un conjunto $\mathcal{H}$, $Z$ y una $\sigma$ álgebra de conjuntos sobre $Z$, se denomina función de pérdida de $\mathcal{H}$ sobre $Z$ a cualquier función de la forma:

\[l : \mathcal{H} \times Z \rightarrow \mathbb{R}^{+}\]

que verifique que fijada $h\in \mathcal{H}$ arbitrario la función $l(h, \cdot)$ sea medible.
#+end_definition

Aumiendo ya como $\mathcal{D}$ la distribución conjunta, con funciones de pérdida arbitrarias, redefiniríamos los conceptos de /error/ y /error empírico/ de la forma:

\begin{align*}
L_{\mathcal{D}} (h) := \mathbb{E}_{z\sim \mathcal{D}}[l(h,z)]\\
L_{S_z} (h) := \frac{1}{m} \sum_{i=1}^m l(h,z_i)
\end{align*}

Donde los conjuntos de entrenamiento se generan a partir de una muestra aleatoria simple $S = (Z_1 \times \ldots \times Z_m)$ con $Z_i = (\mathcal{X}\times \mathcal{Y})_i \sim \mathcal{D}$

#+begin_definition
*Aprendizaje PAC agnóstico*

Una clase de funciones $\mathcal{H} \subseteq \mathcal{Y}^{\mathcal{X}}$ es agnósticamente PAC cognoscible respecto a $Z = \mathcal{X} \times \mathcal{Y}$ (sobre el que tenemos definida una $\sigma$ álgebra de conjuntos) y a una función de pérdida $l: \mathcal{H} \times Z \rightarrow \mathbb{R}^{+}$ si existe una función $m_{\mathcal{H}} : ]0,1[^2\rightarrow \mathbb{N}$ y un algoritmo $A$ verificando que si $0 < \epsilon, \delta < 1$, entonces para toda distribución $\mathcal{D}$ sobre $Z$ ejecutando el algoritmo para un conjunto de entrenamiento $S\sim \mathcal{D}^m$, con $m\ge m_{\mathcal{H}}(\epsilon, \delta)$ se tiene:

\[P_{S\sim \mathcal{D}^m}[L_{\mathcal{D}}(A(S)) \le \underset{h\in \mathcal{H}}{inf} L_{\mathcal{D}}(h) + \epsilon] \ge 1-\delta\]

El algoritmo $A$ devuelve un elemento de $\mathcal{H}$.
#+end_definition


Notamos desde esta definición tomando una *función de pérdida 0-1*:

\[l_{0-1} (h,(x,y)) := \left\{\begin{array}{ll}
0 & h(x) = y\\
1 & si \quad no
\end{array}\right.\]

equivale a la primera definición que dimos de aprendizaje PAC si asumimos propiedad de factibilidad. Por ello no distinguiremos en el uso de uno u otro concepto, sino que se deducirá de si estamos asumiendo propiedad de factibilidad o no.

Cuando permitimos que el algoritmo $A$ devuelva una función $h \notin \mathcal{H}$, de manera que $h \in \mathcal{H}'$ y $\mathcal{H} \subset \mathcal{H}'$ una clase de funciones donde la función de pérdida es extensible de manera natural, el aprendizaje recibe el nombre de *aprendizaje impropio*. La definición aquí dada se ha hecho para *aprendizaje propio*.

*** Condiciones suficientes para ser PAC cognoscible

#+begin_definition
*Convergencia uniforme / clase de Glivenko-Cantelli*

Decimos que una clase de hipótesis $\mathcal{H}$ tiene la propiedad de *convergencia uniforme o es de Glivenko-Cantelli*, respecto a un dominio $Z$, y a una función de pérdida $l$ si existe una función $m_{\mathcal{H}}^{CU}: ]0,1[^2 \rightarrow \mathbb{N}$ verificando que para todo $0 < \delta, \epsilon < 1$ y para toda distribución $\mathcal{D}$ sobre $Z$, si $S$ es un conjunto de entrenamiento de tamaño mayor o igual a $m \ge m_{\mathcal{H}}^{CU}(\epsilon, \delta)$, entonces:

\[P_{S\sim \mathcal{D}^m} [\forall h\in \mathcal{H}, |L_S(h) - L_{\mathcal{D}}(h)| \le \epsilon] \ge 1-\delta\]
#+end_definition

#+begin_theorem
*La convergencia uniforme es condición suficiente para ser PAC cognoscible*

Sea $\mathcal{H}$ una clase de hipótesis con la propiedad de convergencia uniforme. Entonces es agnósticamente PAC cognoscible con cualquier algoritmo ERM y complejidad muestral menor o igual al $m_{\mathcal{H}}^{UC} \left(\frac{\epsilon}{2}, \delta \right)$ dado en la definición anterior.
#+end_theorem

#+begin_proof
Fijamos $m = m_{\mathcal{H}}^{UC} \left(\frac{\epsilon}{2}, \delta \right)$.

Fijado un conjunto de entrenamiento $S_z$ extraído de la variable aleatoria $S = (Z_1, \ldots Z_m) \sim \mathcal{D}^m$ verificando que: 

\[\forall h\in \mathcal{H}, |L_{S_z}(h)-L_{\mathcal{D}}(h)| \le \frac{\epsilon}{2}\]

Entonces, notando $\bar{h} = ERM_{\mathcal{H}}(S_z)}$, para $h \in \mathcal{H}$ arbitrario:

\[L_{\mathcal{D}}(\bar{h}) \le L_{S_z}(\bar{h}) + \frac{\epsilon}{2} \le L_{S_z}(h) + \frac{\epsilon}{2} \le L_{\mathcal{D}}(h) + \frac{\epsilon}{2} + \frac{\epsilon}{2} =  L_{\mathcal{D}}(h) + \epsilon\] 

Donde la segunda desigualdad viene desde la definición de ERM.
#+end_proof

#+begin_fact
*Las clases finitas tienen la propiedad de convergencia uniforme*

Sea $\mathcal{H}$ una clase de hipótesis finita, $Z$ un dominio y sea $l : \mathcal{H} \times Z \rightarrow [a,b]$ una función de pérdida. Entonces $\mathcal{H}$ verifica la propiedad de convegencia uniforme con: 

\[m_{\mathcal{H}}^{CU}(\epsilon, \delta) \le \left\lfloor \frac{log(2|\mathcal{H}|/\delta)(b-a)^2}{2\epsilon^2} \right\rfloor + 1\]
#+end_fact

#+begin_lemma
*Desigualdad de Hoeffding*

Sean $(X_1, \ldots X_m)$ una muestra aleatoria simple de una variable $X$, $\bar{X} = \frac{1}{m} \sum_{i=1}^m X_i$ con $E[\bar{X}] = \mu$ y $P[a \le X_i \le b] = 1, i=1, \ldots m$. Entonces para todo $\epsilon > 0$

\[P\left[\left| \bar{X} - \mu \right| > \epsilon \right] \le 2e^{-2m \left(\frac{\epsilon}{b-a}\right)^2} \]
#+end_lemma

#+begin_proof
Sea $\mathcal{H}$ una clase de hipótesis finita.

Fijamos $0 < \delta, \epsilon < 1$. Necesitamos encontrar $m\in \mathbb{N}$ verificando:

\[P_{S\sim \mathcal{D}^m} [\exists h\in \mathcal{H} |L_S(h) - L_{\mathcal{D}}(h)| > \epsilon] < \delta\]

Partimos de la siguiente desigualdad, que usaremos más adelante, obtenida por subaditividad:

\[P [\exists h\in \mathcal{H} |L_S(h) - L_{\mathcal{D}}(h)| > \epsilon] \le \sum_{h \in \mathcal{H}} P [|L_S(h) - L_{\mathcal{D}}(h)| > \epsilon]\]
Fijamos $h \in \mathcal{H}$.

Dado un conjunto de entrenamiento $S_z = (z_1, \ldots z_m)$, recordamos que $L_{\mathcal{D}} (h) = \mathbb{E}_{z\sim \mathcal{D}} [l(h,z)]$ y que $L_{S_z}(h) = \frac{1}{m} \sum_{i=1}^m l(h,z_i)$

Donde $z_i \sim \mathcal{D}$ y por tanto $\mathbb{E}_{S \sim \mathcal{D}^m} [L_S(h)] = \mathbb{E}_{z \sim \mathcal{D}} [l(h,z)] = L_{\mathcal{D}} (h)$. Además, llamando $X_i = l(h,Z_i)$, por ser $S=(Z_1, \ldots Z_m)$ m.a.s que genera los conjuntos de entrenamiento, se tiene que las $X_i$ son independientes e idénticamente distribuidas, con $P[a \le X_i \le b] = 1$. Estamos en condiciones de aplicar la desigualdad de Hoeffding.

Por tanto:

\[P \left[\left| \frac{1}{m} \sum_{i=1}^m X_i - L_{\mathcal{D}} (h) \right| > \epsilon\right] = P [|L_S(h) - L_{\mathcal{D}}(h)| > \epsilon] \le 2e^{-2m \left( \frac{\epsilon}{b-a} \right)^2}\]

Y por tanto:

\[P [\exists h\in \mathcal{H} |L_S(h) - L_{\mathcal{D}}(h)| > \epsilon] \le |\mathcal{H}| 2e^{-2m \left( \frac{\epsilon}{b-a} \right)^2}\]

Despejando $m$ para que $|\mathcal{H}| 2e^{-2m \left( \frac{\epsilon}{b-a} \right)^2} < \delta$ llegamos al resultado buscado.
#+end_proof

Recordemos hasta ahora el resultado que habíamos obtenido era su carácter PAC cognoscible, donde agnósticamente PAC cognoscible y cognoscible con funciones de pérdida 0-1 era un término equivalente. El teorema que enunciamos a continuación, deducible a partir del teorema sobre el caracter agnóstico - PAC cognoscible de clases de funciones con propiedad de convergencia uniforme, en particular las finitas, generaliza el resultado para cualquier funciones de pérdida acotada.

#+begin_theorem
*Las clases finitas son agnósticamente PAC cognoscible*

Sea $\mathcal{H}$ una clase de hipótesis finita, $Z$ un dominio y sea $l : \mathcal{H} \times Z \rightarrow [a,b]$ una función de pérdida. Entonces $\mathcal{H}$ es PAC cognoscible con complejidad muestral:

\[m_{\mathcal{H}}( \epsilon, \delta ) \le \left\lceil \frac{2 log(2|\mathcal{H}|/\delta)(b-a)^2}{\epsilon^2} \right\rceil\]
#+end_theorem

#+begin_proof
Es trivial desde el anterior teorema y el hecho de que convergencia uniforme implica ser agnósticamente PAC cognoscible
#+end_proof

*** Equilibrio error-varianza /bias-complexity tradeoff/
Veamos que dado un algoritmo de aprendizaje no puede ser el óptimo para aprender todas las distribuciones.

Damos un lema previo, la desigualdad de Markov:

#+begin_lemma
*Desigualdad de Markov*

Dada una variable aleatoria $Z$ no negativa. Entonces para todo $a\ge 0$

\[P[Z \ge a] \le \frac{\mathbb{E}[Z]}{a}\]
#+end_lemma

#+begin_theorem
*Teorema de No Free Lunch*

Sea $A$ cualquier algoritmo de aprendizaje para clasificación binaria con respecto a la función de pérdida 0-1 sobre el dominio $\mathcal{X}$. Sea un conjunto de entrenamiento de tamaño $m < |\mathcal{X}|/2$. Entonces existe una distribución $\mathcal{D}$ sobre $\mathcal{X} \times \{0,1\}$ verificando:

1. Existe una función $f: \mathcal{X} \rightarrow \{0,1\}$ con $L_{\mathcal{D}}(f)=0$
2. $P_{S\sim \mathcal{D}^m} [L_{\mathcal{D}} (A(S)) \ge 1/8] \ge 1/7$
#+end_theorem

#+begin_proof
Sea un conjunto de entrenamiento (consideramos un conjunto y no una secuencia) de tamaño $2m$, $C$. Hay $T = 2^{2m}$ posibilidades de etiquetado del conjunto, esto es, $2^{2m}$ posibles hipótesis, $f_i: C\rightarrow \{0,1\}$, que vamos a extender a $\mathcal{X}$ llamándolas $\bar{f}_i$ de forma que $\bar{f}_{i|C} = f_i$ y $\bar{f}_i(x) = 0 \quad \forall x\in \mathcal{X}\setminus C$. Vamos a tomar para cada una de ellas una distribución $\mathcal{D}_i$ definida sobre $\mathcal{X} \times \{0,1\}$ definida por:


\[\forall (x,y)\in \mathcal{X} \times \{0,1\} \qquad P_{z\sim \mathcal{D}_i} [z = (x,y)] = \left\{\begin{array}{ll}
1/|C| & \exists x_i \in C : y=f(x_i)\\
0     & si \quad no
\end{array}\right.\]

Claramente $L_{\mathcal{D}_i}(f_i) = 0$

Vamos a probar que:

\[\exists i\in \{1, \ldots 2m\} : \mathbb{E}_{S\sim \mathcal{D}_i^m} [L_{\mathcal{D}_i} (A(S))] \ge \frac{1}{4}\]

Hay $k = (2m)^m$ posibles secuencias de entrenamiento de tamaño $m$, $S_j, j=1, \ldots k$ tomadas desde $C$. Siendo $S_j = (x_1, \ldots x_m)$ notamos $S_j^i = ((x_1, f_i(x_1)), \ldots, (x_m, f_i(x_m)))$. Cada $S_j$ tiene la misma probabilidad de ser nuestro conjunto de entrenamiento (extracción de $m$ valores con reemplazamiento desde el conjunto $C$), verificándose:

\[\mathbb{E}_{S\sim \mathcal{D}_i^m} [L_{\mathcal{D}_i} (A(S))] = \frac{1}{k} \sum_{j=1}^k L_{\mathcal{D}_i} (A(S_j^i))\]

Recordando que hemos llamado $k=(2m)^m$, $T=2^{2m}$, se tiene:

\begin{align*}
max_{i \in \{1,\ldots T\}} \frac{1}{k} \sum_{j=1}^{k} L_{\mathcal{D}_i} (A(S_j^i)) &\ge 
       \frac{1}{T} \sum_{i=1}^{T} \frac{1}{k} \sum_{j=1}^{k}  L_{\mathcal{D}_i} (A(S_j^i))   =\\
&=     \frac{1}{k} \sum_{j=1}^{k} \frac{1}{T} \sum_{i=1}^{T}  L_{\mathcal{D}_i} (A(S_j^i)) \ge\\
&\ge min_{j \in \{1, \ldots k\}} \frac{1}{T} \sum_{i=1}^{T}  L_{\mathcal{D}_i} (A(S_j^i))
\end{align*}


Además fijado $j \in \{1,\ldots k\}$, se tiene que que para todo $i \in \{1,\ldots T\}$:

\[L_{\mathcal{D}_i} (h) = \frac{1}{|C|} \sum_{x\in C} \mathds{1}_{[A(S^i_j)(x) \neq f_i(x)]} = \frac{1}{2m} \sum_{x \in C} \mathds{1}_{[A(S^i_j)(x) \neq f_i(x)]}\]


Por tanto:

\begin{align*}
\frac{1}{T} \sum_{i=1}^{T}  L_{\mathcal{D}_i} (A(S_j^i)) &\ge
\frac{1}{T} \sum_{i=1}^{T}  \frac{1}{2m} \sum_{x \in C} \mathds{1}_{[A(S^i_j)(x) \neq f_i(x)]} = \\
&= \frac{1}{2m} \sum_{x \in C} \frac{1}{T} \sum_{i=1}^{T}  \mathds{1}_{[A(S^i_j)(x) \neq f_i(x)]} \ge \\
&\ge \frac{1}{2} min_{x\in C} \frac{1}{T} \sum_{i=1}^{T}  \mathds{1}_{[A(S^i_j)(x) \neq f_i(x)]}
\end{align*}


Como dado un $x\in C$ cualquiera, la mitad de clasificadores $f_i$ clasificarán $x$ bien y la otra mitad mal, se tiene:

\[\frac{1}{2} min_{x\in C} \frac{1}{T} \sum_{i=1}^{T}  \mathds{1}_{[A(S^i_j)(x) \neq f_i(x)]} = \frac{1}{2} \frac{1}{T} \frac{T}{2} = \frac{1}{4}\]

Y uniendo toda esta información:

\[max_{i \in \{1,\ldots T\}} \frac{1}{k} \sum_{j=1}^{k} L_{\mathcal{D}_i} (A(S_j^i)) \ge \frac{1}{4}\]

Sea $k = argmax_{i \in \{1,\ldots T\}} \frac{1}{k} \sum_{j=1}^{k} L_{\mathcal{D}_i} (A(S_j^i))$

Si $\mathcal{D} = \mathcal{D}_k$ cumple la parte 2 del enunciado del teorema, es nuestra distribución buscada, y como función buscada en el apartado 1. podemos tomar $f=f_k$

Como $L_{\mathcal{D}} (A(\cdot))$ puede ser vista como una variable aleatoria donde $S \sim \mathcal{D}^m$ y que toma valores en $[0,1]$, tenemos que tomando $Z = 1-L_{\mathcal{D}}(A(\cdot))$, $a=\frac{7}{8}$ en el lema previo llegamos a:

\[P_{S\sim \mathcal{D}^m} \left(\frac{1}{8} \ge L_{\mathcal{D}}(A(S)) \right) \le \frac{3}{4} \cdot \frac{8}{7} = 24/28\]

donde $\mathbb{E}(Z) = \mathbb{E} (1 - L_{\mathcal{D}}(A(\cdot))) = 1 - \mathbb{E} (L_{\mathcal{D}}(A(\cdot))) \le \frac{3}{4}$

Es decir:

\[P_{S\sim \mathcal{D}^m} \left( L_{\mathcal{D}}(A(S)) \ge \frac{1}{8} \right) \ge \frac{4}{28} = \frac{1}{7}\]
#+end_proof


Como consecuencia del teorema, podemos decir que no hay un algoritmo de aprendizaje óptimo para todas las distribuciones, puesto que para una dada por el resultado del teorema, el algoritmo ERM con $\mathcal{H} = \{f\}$ aprendería mejor.

*** Dimensión Vapnik-Chervonenkis
**** Introducción
La abreviaremos dimensión VC

#+begin_definition
*Restricción de $\mathcal{H}$ a $C$*

Sea $\mathcal{H}$ clase de hipótesis de $\mathcal{X}$ a $\{0,1\}$, y $C=\{c_1, \ldots c_m\} \subseteq \mathcał{X}$. Llamamos restricción de $\mathcal{H}$ a $C$ al conjunto de funciones:

\[\mathcal{H}_{C} = \{h_{|C} : h\in \mathcal{H}\} \cong \{(h(c_1), \ldots h(c_m)): h\in \mathcal{H}\}\]
#+end_definition


#+begin_definition
*Restricción de $\mathcal{X}$ a $\mathcal{H}$*

Sea $\mathcal{H}$ clase de hipótesis de $\mathcal{X}$ a $\{0,1\}$, y $C=\{c_1, \ldots c_m\} \subseteq \mathcał{X}$. Llamamos restricción de $\mathcal{X}$ por $\mathcal{H}$ a:

\[\mathcal{X}_{\mathcal{H}} = \{S \subseteq X: \exists h\in \mathcal{H}, h(S)=\{1\} \}\]
#+end_definition


#+begin_definition
*Conjunto fragmentado por otro*
 
Un conjunto $\mathcal{F}$ diremos que fragmenta a otro conjunto finito $C$ si se verifica que para todo subconjunto de $D \subseteq C$ existe $S\in \mathcal{F}$ con $S \cap C = D$
#+end_definition


#+begin_definition
*Conjunto fragmentado por una clase de hipótesis*

Una clase de hipótesis $\mathcal{H}$ fragmenta un conjunto finito $C \subseteq \mathcal{X}$ sii la restricción de $\mathcal{H}$ a $C$ nos da todas las posibles funciones de $C$ a $\{0,1\}$. Esto es, si $|\mathcal{H}_{C}| = 2^{|C|}$

#+begin_lemma
*Caracterización del concepto de fragmentación de $C$ por $\mathcal{H}$*

Una clase de hipótesis $\mathcal{H}$ fragmenta un conjunto finito $C \subseteq \mathcal{X}$ sii $X_{\mathcal{H}}$ fragmenta $C$
#+end_lemma

#+begin_proof
La demostración de la caracterización es trivial desde la biyección:

\[\{h_{|C} : h\in \mathcal{H}\} \cong \{(h(c_1), \ldots h(c_m)): h\in \mathcal{H}\}\]

con $C = \{c_1, \ldots c_m\}$
#+end_proof

Este lema nos permite trabajar indistintamente con la fragmentación de un conjunto por una clase de funciones o por la restricción del espacio por dicha clase de hipótesis.

Cuando demostrábamos el teorema de No Free Lunch, no teníamos ninguna restricción sobre la distribución que construíamos ni sobre la hipótesis que daba lugar a esa distribución, la $f$ que cumplía que tenía error nulo. Siempre que el conjunto $C$ que tomamos sea fragmentado por $\mathcal{H}$, podremos asegurar que la $f$ que genera la distribución pertenece a la clase de funciones $\mathcal{H}$. Formalmente:

#+begin_theorem
*Teorema de No Free Lunch revisitado*

Sea $\mathcal{H}$ una clase de hipótesis de $\mathcal{H}$ a $\{0,1\}$, $m < |\mathcal{X}|/2$ el tamaño del conjunto de entrenamiento. Supongamos que existe $C\subseteq \mathcal{X}$ de tamaño $2m$ fragmentado $\mathcal{H}$. Sea $A$ cualquier algoritmo de aprendizaje, entonces existe una distribución $\mathcal{D}$ sobre $\mathcal{X} \times \{0,1\}$ verificando:

1. Existe una función $f: \mathcal{X} \rightarrow \{0,1\}$, $f\in \mathcal{H}$ con $L_{\mathcal{D}}(f)=0$
2. $P_{S\sim \mathcal{D}^m} [L_{\mathcal{D}} (A(S)) \ge 1/8] \ge 1/7$

label:nofreelunch-v2
#+end_theorem

Intuitivamente, si existe un conjunto $C$ fragmentado por $\mathcal{H}$ y nuestro conjunto de entrenamiento contiene la mitad de instancias de $C$ (recordemos que la distribución que construíamos en la demostración de No Free Lunch asignaba toda la masa de probabilidad al conjunto $C$), entonces no tenemos información suficiente para etiquetar correctamente el resto de instancias (hay demasiadas posibles hipótesis que etiquetan el conjunto de entrenamiento de igual forma pero difieren en el resto de instancias).

#+begin_definition
*Dimensión VC*

Definimos la dimensión VC de una clase de hipótesis $\mathcal{H}$ como el tamaño máximo de los conjuntos $C \subseteq \mathcal{X}$ verificando que son fragmentados por $\mathcal{H}$. Si no existe máximo, decimos que $\mathcal{H}$ tiene dimensión VC infinita. La notamos $VC(\mathcal{H})$
#+end_definition


Del teorema No Free Lunch revisitado deducimos:

#+begin_theorem
*Ser PAC cognoscible implica tener dimensión VC finita*

Sea $\mathcal{H}$ clase de hipótesis con $VC(\mathcal{H}) = \infty$. Entonces $\mathcal{H}$ no es PAC cognoscible
#+end_theorem

**** Ejemplos

***** Intervalos $]-\infty, a[$

Sea $\mathcal{H} = \{h_a = \mathds{1}_[x<a]: a\in \mathbb{R}\}$ clase de hipótesis sobre $\mathbb{R}$. 

Dado un conjunto $C=\{\alpha\}$, podemos tomar $h_{\alpha}$ y $h_{\alpha+1}$, que nos dan todos los posibles etiquetados de $C$.
Sin embargo, dado un conjunto de tamaño 2, $C=\{\alpha, \beta\}$, donde podemos suponer spg. $\alpha < \beta$. Entonces no podemos encontrar $h_b \in \mathcal{H}$ verificando $h_b(\alpha)=0$ y $h_b(\beta) = 1$, ya que esto implicaría que $b > \beta$ y por tanto entraría en contradicción con que $h_b(\alpha) = 0$

Hemos probado $VCdim(\mathcal{H}) = 1$.

***** Intervalos cerrados y acotados

Sea $\mathcal{H} = \{h_{a,b} = \mathds{1}_[a<x<b]: a,b\in \mathbb{R}\}$ clase de hipótesis sobre $\mathbb{R}$. 

Dado un conjunto $C=\{\alpha\, \beta\}$, con $\alpha < \beta$, las hipótesis $h_{\alpha+\delta_1, \beta + \delta_2}$ con $\delta_i \in \{0,1\}$ nos dan todos los posibles etiquetados de $C$.
Sin embargo, dado un conjunto de tamaño 3, $C=\{\alpha, \beta\, \theta}$, donde podemos suponer $\alpha < \beta < \theta$. Entonces no podemos encontrar $h_b \in \mathcal{H}$ verificando $h_b(\alpha)=1$ y $h_b(\theta) = 1$ y $h_b(\beta) = 0$

Hemos probado $VCdim(\mathcal{H}) = 2$.

***** Clases finitas

Sea $\mathcal{H}$ una clase finita. Entonces para un conjunto $C \subeteq \mathcal{H}$ se tiene $|mathcal{H}_C| \le |\mathcal{H}|$ y por tanto el conjunto no puede ser fragmentado por $\mathcal{H}$ si $|\mathcal{H}| < 2^{|C|}$, lo que implica $VCdim(\mathcal{H}) \le log_2(|\mathcal{|H||)$

***** Dimensión VC y número de parámetros

Puede demostrarse que la dimensión VC de los clasificadores de rectángulo $\mathcal{H} = \{h_{a,b,c,d} := \mathds{1}_{[a\le x\le b, c\le y\le d]}\}$ en $\mathbb{R}^2$, que ya mencionamos en un ejemplo en los temas anteriores es 4. Esto, unido a los ejemplos anteriores con intervalos podría hacernos conjeturar que la dimensión VC depende del número de parámetros con el que definimos los clasificadores. El siguiente ejemplo demuestra que esto es falso.

Dada la clase de clasificadores $\mathcal{H} = \{h_{\theta}: \theta \in \mathbb{R}\}$ donde $h_{\theta}: \mathcal{X} \rightarrow {0,1}$ está definida por $h_\theta (x) = \lceil 0.5 sen(\theta x) \rceil$, y $d\in \mathbb{N}$ arbitrario, podemos tomar $d$ puntos codificados por $x_i = 0.x_{1,j} \ldots x_^{2^d,j} x^{2^d + 1,j}$ donde cada $x_{i}$ es una fila de la matriz $(x_{i,j})$ donde la columna $2^{d+1}$ ésima es 1, y la columna $i$ -ésima codifica el número $i-1$ en binario, leído de arriba a abajo. Así dado una asignación de $d$ etiquetas, debe codificar un número en binario entre $1$ y $2^d-1$, a saber, la columna $k$ ésima de la matriz. Tomamos el clasificador $h = \lceil 0.5 sen(10^k \pi x) \rceil$ que verificará que su asignación de etiquetas es justamente la columna $k$ ésima. El sentido de la última columna constantemente $1$ puede explicarse en que daremos $x_{i,1} \ldots x_{i,k}$ medias vueltas a la circunferencia unidad y recorreremos y una fracción $0.x_{i,k+1} \ldots x_{2^d,j} 1$ no nula de otra media vuelta a la circunferencia. Si $x_{i,k} = 1$ entonces $h(x_i) = 1$, y si $x_{i,k}=0$ entonces $h(x_i) = 0$. Luego $VCdim(\mathcal{H}) = \infty$.

bibliography:references
bibliographystyle:IEEEtran
**** Teorema fundamental de aprendizaje PAC

#+begin_theorem
*Teorema fundamental de aprendizaje PAC*

Sea $\mathcal{H}$ clase de hipótesis de la forma $h: \mathcal{X} \rightarrow \{0,1\}$ y la función de pérdida 0-1. Entonces equivalen:

1. $\mathcal{H}$ tiene la propiedad de convergencia uniforme.
2. $\mathcal{H}$ es agnósticamente PAC cognoscible por cuaquier algoritmo ERM.
3. $\mathcal{H}$ es agnósticamente PAC cognoscible.
4. $\mathcal{H}$ es PAC cognoscible.
5. $\mathcal{H}$ es PAC cognoscible por cualquier algoritmo ERM.
6. $VC (\mathcal{H}) < \infty$.
#+end_theorem


La implicación que nos falta es $6 \implies 1$. El resto de implicaciones se consiguen a partir de teoremas ya probados en temas anteriores.

Daremos una serie de lemas y definiciones previas antes de probarla.

#+begin_definition
*Función de crecimiento*

Sea $\mathcal{H}$ una clase de hipótesis. Definimos como función de crecimiento de $\mathcal{H}$:

\[\begin{array}{ll}
\tau_{\mathcal{H}}: & \mathbb{N} \rightarrow \mathbb{N}\\
                    & m          \mapsto     \underset{C \subseteq \mathcal{X}: |C|=m}{max}{|\mathcal{H}_C|}
\end{array}\]

Esta función está bien definida puesto que fijado $m \in \mathbb{N}$ se tiene siempre que $|\mathcal{H}_C| \le 2^C$
#+end_definition

#+begin_lemma
*Lema de Sauer-Shelah*

Sea $\mathcal{H}$ clase de hipótesis con $VC(\mathcal{H}) \le d < \infty$. Entonces para todo $m\in \mathbb{N}$ se tiene $\tau_{\mathcal{H}} (m) \le \sum_{i=0}^d \binom{m}{i}$. Se deduce que si $m > d+1$ entonces $\tau_{\mathcal{H}}(m) \le (em/d)^d$.
#+end_lemma


#+begin_proof
Empezamos probando que una clase de hipótesis $\mathcal{F}$ finita fragmenta al menos $|\mathcal{F}|$ conjuntos.

Lo hacemos por inducción sobre el tamaño de $\mathcal{X}_\mathcal{F}$ que es un conjunto finito por ser $\mathcal{F}$ clase finita.

Si su tamaño es 1, parte al conjunto vacío.

Supuesto que se verifica la hipótesis para tamaños menores que $k-1$ y sea $|\mathcal{X}_\mathcal{F}| = k$. Escogemos entonces $x\in \mathcal{X}$ verificando que $x$ pertenece a algunos conjuntos de $\mathcal{X}_\mathcal{F}$ pero no a todos (debe existir, sino tendríamos que $\mathcal{X}_\mathcal{F}$ contiene un único conjunto).

Sean $A = \{S \subseteq \mathcal{X}_\mathcal{F} : x\in S\}$, $A'=\{S\setminus\{x\} : S \in A\}$, $B= \{S \subseteq \mathcal{X}_\mathcal{F} : x\not\in S\}$.

Claramente $|A| = |A'|$ y por hipótesis de inducción $A'$ fragmenta $k \ge |A|$ conjuntos, y $B$ fragmenta $|B|$ conjuntos. También es trivial ver que los conjuntos $S$ y $S\cup \{x\}$ son fragmentados por $A$ donde $S$ es un conjunto fragmentado por $A'$. Hemos probado que $A$ fragmenta $|A| + k$ conjuntos, y si un conjunto es fragmentado por $A$ y $B$ a la vez, entoces no contiene a $x$, luego es fragmentado por $A'$ y por $B$ a la vez, y podremos tener a lo sumo $k$ conjuntos de este tipo.

En definitiva hemos probado que fragmentamos $|A| + k + |B| - k = |A| + |B| = |\mathcal{F}|$ conjuntos.
Probado esto, si $\tau_{\mathca{H}}(m) > \sum_{i=0}^d \binom{m}{i}$ entonces $\mathcal{H}$ debe fragmentar un conjunto de tamaño $d+1$ al menos, puesto que el número de subconjuntos de un conjunto finito $C$ menor que  $d+1$ es exactamente $\sum_{i=0}^d \binom{|C|}{i}$. Luego tendríamos $VC(\mathcal{H}) > d$

#+end_proof
*** Aprendizaje no uniforme
Establecemos el concepto de aprendizaje no uniforme, relajando la definición de agnósticamente PAC cognoscible. Recordamos que en la definición de PAC cognoscible, el tamaño de la muestra sólo dependía de los parámetros de confianza y error que buscásemos satisfacer. En el aprendizaje uniforme hacemos depender el tamaño de la muestra de dichos parámetros de confianza y error, pero además también de una hipótesis de etiquetado a la que queremos acercarnos, sea cual sea la verdadera distribución de las instancias y sus etiquetas.

#+begin_definition
*Aprendizaje no uniforme*

Una clase de funciones $\mathcal{H}$ sobre $Z=\mathcal{X} \times \mathcal{Y}$ es no-uniformemente PAC cognoscible si existe un algoritmo $A$ y una función $m_{\mathcal{H}}^{NU} : ]0,1[^2 x \mathcal{H} \rightarrow \mathbb{N}$ verificando que si $0 < \epsilon, \delta < 1$, y $\bar{h} \in \mathcal{H}$ entonces para toda distribución $\mathcal{D}$ sobre $Z$ ejecutando el algoritmo para un conjunto de entrenamiento $S\sim \mathcal{D}^m$, con $m\ge m_{\mathcal{H}}^{NU} (\epsilon, \delta, \bar{h})$ el algoritmo devuelve una hipótesis $A(S) = h\in \mathcal{H}$ verificando que:

\[P_{S\sim \mathcal{D}^m}[L_{\mathcal{D}}(h) \le L_{\mathcal{D}}(\bar{h}) + \epsilon] \ge 1-\delta\]
#+end_definition

**** Minimización de riesgo estructural
Hasta ahora hemos traducido el conocimiento previo sobre el problema como una restricción global en la clase de hipótesis para la minimización del riesgo empírico. Ahora generalizaremos esto aún más y estableceremos la suposición de que $\mathcal{H}= \cup_{n\in \mathbb{N}} \mathcal{H}_n$, donde a cada clase $\mathcal{H}_n$ se le asignará un peso $w(n)$. 

#+begin_theorem
Sea $w : \mathcal{N} \rightarrow [0,1]$ verificando $\sum_{n=1}^\infty w(n) \le 1$. Sea $\mathcal{H}$ una clase de hipótesis binarias que puede ser escrita como $\mathcal{H}= \cup_{n\in \mathbb{N}} \mathcal{H}_n$ unión numerable de clases con la propiedad de convergencia uniforme. Sea $m_{\mathcal{H}_n}^{UC}$ .Sea $\epsilon_n : \mathcal{N} \times [0,1] \rightarrow [0,1]$, $\epsilon_n (m,\delta) = inf \{\epsilon \in [0,1] : m_{\mathcal{H}_n}^{UC} (\epsilon, \delta) \le m\}$. Entonces para todo $\delta \in [0,1]$ y para toda distribución $\mathcal{D}$ se verifica:


\[ \quad P_{S \sim \mathcal \mathcal{D}^m} [\forall n\in \mathbb{N}, \forall h\in \mathcal{H}_n, \quad |L_{\mathcal{D}}(h) - L_S(h)| \le \epsilon_n (m, w(n)\cdot \delta)] \ge 1-\delta\] 


En particular dado $h\in \mathcal{H}_n$ tomando $q = min\{n: h\in \mathcal{H}_n\}$:

\[P_{S \sim \mathcal \mathcal{D}^m} [\forall n\in \mathbb{N}, \quad |L_{\mathcal{D}}(h) - L_S(h)| \le \epsilon_n (m, w(q)\cdot \delta)] \ge 1-\delta \]
#+end_theorem

#+begin_proof
Fijado $n$, por hipótesis de convergencia uniforme:

\[P [\forall h\in \mathcal{H}_n, \quad |L_{\mathcal{D}}(h) - L_S(h)| \le \epsilon_n (m, w(n) \delta) ] \ge 1- w(n) \delta\]

Por tanto:

\begin{align*}
& P [\forall n\in \mathbb{N}, h\in\mathcal{H} \quad |L_{\mathcal{D}}(h) - L_S(h)| \le \epsilon_n (m, w(n)\cdot \delta)] = \\
& = 1 - P[\exists n\in \mathbb{N}, h\in \mathcal{H}_n, \quad |L_{\mathcal{D}}(h) - L_S(h)| > \epsilon_n (m, w(n) \delta)] > 1- \sum_{n=1}^{\infty} w(n) \delta \ge 1-\delta
\end{align*}

#+end_proof

Esto da lugar a un nuevo paradigma para obtener buenas aproximaciones de la función de verdadero etiquetado, la *minimización del error estructural (SRM)*. Sea $\mathcal{H} = \cup_{n\in \mathbb{N}} \mathcal{H}_n$ donde $\mathcal{H}_n$ tiene la propiedad de convergencia uniforme con $m_{\mathcal{H}_n}^{CU}$, y $w : \mathbb{N} \rightarrow [0,1]$, $\sum_n w(n) \le 1$. Definiendo (con $h\in \mathcal{H}$):

\[\epsilon_n (m,\delta) = inf \{\epsilon \in [0,1] : m_{\mathcal{H}_n}^{UC} (\epsilon, \delta) \le m\}, \quad n(h) = min\{n: h\in \mathcal{H}_n\}\]

SRM consiste en devolver $h \in \underset{h\in \mathcal{H}}{argmin} [L_S(h) + \epsilon_{n(h)}(m,w(n(h)) \cdot \delta)]$.

En comparación con el paradigma ERM, no intentamos sólo miminizar el error empírico, sino que además cuantificamos cómo de bien aproxima una determinada clase de hipótesis. Además, cabe remarcar que estamos suponiendo que siempre existe $\underset{h\in \mathcal{H}}{argmin} [L_S(h) + \epsilon_{n(h)}(m,w(n(h)) \cdot \delta)]$

#+begin_theorem
Sea $\mathcal{H}$ una clase de hipótesis verificando $\mathcal{H} = \cup_{n\in \mathbb{N}} \mathcal{H}_n$ donde cada $\mathcal{H}_n$ tiene la propiedad de convergencia uniforme con complejidad muestral $m_{\mathcal{H}_n}^{CU}$. Sea $w : \mathbb{N} \rightarrow [0,1]$ verificando $w(n) = \frac{6}{(\pi n)^2}$. Entonces $\mathcal{H}$ es no-uniformemente cognoscible usando paradigma SRM. Además se verifica:

\[m_{\mathcal{H}}^{NU} (\epsilon, \delta, h) \le m_{\mathcal{H}_{n(h)}}^{CU} \left(\epsilon/2, \frac{3 \delta}{(\pi n(h))^2} \right)\]
#+end_theorem


#+begin_proof
Fijamos $h\in \mathcal{H}, \epsilon \in [0,1], \delta \in [0,1]$. Tomamos $m \ge m_{\mathcal{H}_{n(h)}}^{CU} \left(\epsilon/2, \frac{3 \delta}{(\pi n(h))^2} \right)$. Claramente $\epsilon_{n(h)}(m, w(n(h)) \delta) \le \epsilon/2$

Usando el hecho de que $\frac{w(n(h))}{2} = \frac{3\delta}{(\pi n(h))^2}$ y $\sum_{n\ge 1} w(n) = 1$ aplicando el teorema anterior obtenemos que con probabilidad menor que $\delta / 2$ respecto a la elección de $S \sim \mathcal{D}^m$ se cumple:

\[\exists \bar{h} \in \mathcal{H}, L_{\mathcal{D}} (\bar{h}) \le L_S(\bar{h}) + \epsilon_{n(\bar{h})}(m, w(n(\bar{h})) \delta)\]

Además con probabilidad menor que $\delta/2$, usando convergencia uniforme:

\[L_S(h) \le L_{\mathcal{D}}(h) + \epsilon/2\]

Uniendo estas dos desigualdades con el hecho de que $L_{\mathcal{D}} (A(S)) \le \underset{\bar{h}}{min} [L_S(\bar{h}) + \epsilon_{n(\bar{h})}(m, w(n(\bar{h})) \delta)]$, y $A$ un algoritmo SRM, obtenemos el resultado buscado.
#+end_proof

**** Caracterización de aprendizaje no-uniforme

#+begin_theorem
*Teorema de caracterización de aprendizaje no-uniforme*

Una clase de hipótesis binarias $\mathcal{H}$ es no uniformemente cognoscible sii es unión contable de clases agnósticamente PAC cognoscibles
#+end_theorem

#+begin_proof
Supongamos $\mathcal{H} = \cup_{n\in \mathbb{N}} \mathcal{H}_n$ donde cada $\mathcal{H}_n$ es agnósticamente PAC aprendible. Por teorema fundamental de aprendizaje estadístico, cada clase $\mathcal{H}_n$ tiene la propiedad de convergencia uniforme, y podemos aplicar el teorema anterior.

En el sentido opuesto, si $\mathcal{H}$ es no-uniformemente cognoscible, usando un cierto algoritmo $A$. Definimos $\mathcal{H}_n = \{h \in \mathcal{H} : m_{\mathcal{H}}^{NU}(1/8, 1/7, h) \le n\}$ para todo $n$ natural. Claramente $\mathcal{H} = \cup_{n\in \mathbb{N}} \mathcal{H}_n$. Sea una distribución $\mathcal{D}$ que satisface la propiedad de factibilidad sobre $\mathcal{H}_n$. Entonces con probabilidad de al menos $6/7$ sobre $S \sim \mathcal{D}^n$ se tiene $L_{\mathcal{D}}(A(S)) \le 1/8$. Esto niega que $VC(\mathcal{H}_n)$ sea infinita, por teorema ref:nofreelunch-v2, y por tanto $\mathcal{H}_n$ es agnósticamente PAC cognoscible.
#+end_proof

