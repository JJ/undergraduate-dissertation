\section{Motivación}

El objetivo del aprendizaje automático es convertir datos en conocimiento a través de un razonamiento inductivo, de manera que
proporcionándole datos a una máquina seamos capaces de extraer un conocimiento (una generalización de los datos que nos permita
inferir información a partir de nuevos datos). Surge la pregunta de por qué es necesario el aprendizaje automático o 
\textit{machine learning}, si la estadística también se encarga de obtener conocimiento a partir de unos datos.

\subsection{¿Por qué necesitamos \textit{machine learning}?}
\begin{enumerate}[i]
 \item Para resolver \textbf{tareas que requieren automatización}: entran dentro de esta categoría tanto aquellas tareas para
 las que no existe una axiomatización o un conocimiento exacto, como pueden ser el reconocimiento de dígitos o de voz, como 
 aquellas tareas que requieren del análisis de un gran número de datos, y quedan fuera de la capacidad humana para realizar
 un análisis estadístico manual. En el primer caso necesitamos apoyarnos en conocimiento auxiliar (por ejemplo, 
 un conjunto de dígitos o de muestras de voz preetiquetadas con los que poder comparar muestras sin etiquetar/clasificar); 
 en el segundo, se hace necsario el uso de una máquina para poder extraer conocimiento de todos los datos.
 
 \item \textbf{Tareas que requieren adaptatividad}: si cambian los datos de entrada, necesitamos que los algoritmos se readapten
 a ellos, y no tengamos un conocimiento rígido, sino que pueda cambiar/mejorar en función de la entrada.
\end{enumerate}

\subsection{Áreas relacionadas con el aprendizaje}
Entre las áreas relacionadas con el aprendizaje automático, cabe mencionar:

\begin{enumerate}[i]
 \item \textbf{Inteligencia Artificial}
 \item \textbf{Algorítmica}: debemos analizar el tiempo asintótico de los algoritmos mediante los que aprende la máquina.
 \item \textbf{Inferencia}: entre las diferencias que podemos mencionar con la estadística convencional, destaca la necesidad de 
 programar las tareas, dado el volumen de datos con el que normalmente se trabaja, mientras que en muchos análisis estadísticos basta 
 lápiz y papel. También destaca la \textbf{independencia respecto a distribución} con la que se trabaja (no se asume una distribución
 determinada sobre los datos). La principal diferencia del aprendizaje automático respecto a la inferencia es que la inferencia
 se encarga de comprobar la validez de las hipótesis que propone el estadista, mientras que el algoritmo de 
 \textit{machine learning} genera hipótesis para unos datos determinados, con unas ciertas condiciones de aproximación y error.
 \item \textbf{Álgebra lineal}
 \item \textbf{Optimización de algoritmos}
\end{enumerate}

\subsection{Ejemplo práctico}\label{sec:first-ex}
Pensemos en un ejemplo: tenemos clientes de un banco que quieren solicitar un préstamo, y a todos se les categoriza
el tamaño del préstamo que quieren solicitar(cómo de grande es su importe) y su nivel de ingresos. Estas variables se miden 
en una escala de $0$ a $1$ donde $1$ es el nivel máximo. Queremos etiquetar a cada cliente como $1$:conceder préstamo o 
$0$:no conceder préstamo.

Dado un histórico de $m\in \mathbb{N}$ clientes a los que se les concedieron y devolvieron o no préstamos, tenemos una tupla 
$((x_1, y_1), \ldots (x_m, y_m))$ donde $x_i = ((x_i)_1, (x_i)_2) \in [0,1]^2, y_i \in \{0,1\}$. Asimismo $(x_i)_1$ representa el 
tamaño del préstamo solicitado y $(x_i)_2$ representa el nivel ingresos mensuales. Tenemos a los clientes clasificados en 
función de si devolvieron los préstamos o no.

Queremos encontrar una función que ofrezca una predicción sobre cualquier cliente, para minimizar posibles pérdidas del banco, es
decir, buscamos una $f:[0,1]^2 \rightarrow \{0,1\}$ que llamaremos predicción.

Asumimos que los datos de los clientes de que disponemos no van a tener una distribución determinada y
van a ser idéntica e independientemente distribuidos (i.i.d.). El histórico siempre va a crecer, y no sabemos cómo va a hacerlo, queriendo aprovechar
al máximo la información del mismo.

También asumiremos que tenemos una clase de predicciones $H \subseteq \{0,1\}^{[0,1]^2}$, de entre las que existe
una óptima. Podríamos limitar la clase de predicciones sobre la que buscamos como subrectángulos de $[0,1]^2$ por ejemplo, 
esto es, funciones:

\[h_{a,b,c,d} = \mathds{1}_{[a,b]\times[c,d]}, \qquad [a,b]\times [c,d] \subseteq [0,1]^2\]

\img{./imgs/rect-ex.png}{0.85}

También necesitaremos definir una medida de acierto: ¿cómo de buena es una predicción?.

\section{Definiciones básicas}
\label{sec:defs}

Damos unas notaciones/definiciones básicas que utilizaremos de aquí en adelante, en base a lo descrito en \ref{sec:first-ex}:

\begin{itemize}
\item \textbf{Dominio}: $X$. Llamamos \textbf{instancia} a $x\in X$

\item \textbf{Conjunto de etiquetas}: $Y$ consideramos $\{0,1\}$, lo que nos restringe de momento al paradigma de clasificación binaria. 
En ocasiones también usaremos $Y = \{-1,1\}$ para las etiquetas. Al $1$ se le llama clase positiva, y al $0$ o $-1$ clase negativa.

\item \textbf{Verdadero etiquetado}: \sloppy Asumimos la existencia de una función ${f: X \rightarrow Y}$ 
que proporciona la verdadera etiqueta de todas las instancias.

\item \textbf{Generación de instancias}: \fussy Se tiene $x\sim \mathcal{D} = (\Sigma, \prob)$. La distribución de probabilidad nos da 
información sobre la probabilidad de extraer cada posible instancia desde  $x \in X$. 

\item \textbf{Conjunto de entrenamiento}: $S = ((x_1,y_1) \ldots (x_m,y_m)) \in (X \times Y)^m$ 
Nótese que llamarlo conjunto puede dar lugar a confusión, puesto que se trata de una tupla. Notaremos 
$S_x = (x_1, \ldots x_m)$

De momento asumiremos que las etiquetas del conjunto de entrenamiento se corresponden con el verdadero etiquetado: 
$y_i = f(x_i)$, por lo que no podemos tener una instancia con etiquetas diferentes.

La elección de $S_x$ es idéntica e independientemente distribuida, esto es $x_i \sim \mathcal{D}$ para todo $i=1, \ldots, m$.
Lo notamos $S_x \sim \mathcal{D}^m$, o $S \sim \mathcal{D}^m$, por abuso de notación.

\item \textbf{Hipótesis/clasificador/predicción}: cada posible aplicación perteneciente a 
$\{h, h:X \rightarrow Y\} := 2^{X}$. 

\item \textbf{Error del clasificador}: Definimos el error del clasificador, suponiendo 
$\{x\in X : h(x) \neq f(x)\} := [h\neq f] \in \Sigma$ como:

\[L_{\mathcal{D},f}(h) :=  \prob [h \neq f]\]

\item \textbf{Algoritmo de aprendizaje}: Llamamos algoritmo de aprendizaje a cualquier aplicación que tome conjuntos de 
entrenamiento y devuelva hipótesis sobre el problema:

\[A: \underset{m\in \mathbb{N}}{\bigcup} (X\times Y)^m \rightarrow 2^{X}\]

Asumimos que el algoritmo no tiene acceso a la función de verdadero etiquetado $f: X \rightarrow Y$ ni a
la distribución $\mathcal{D}$.
\end{itemize}

Es decir, nos centraremos en el problema de la clasificación, que se trata de un aprendizaje supervisado (conocemos las etiquetas para
el conjunto de entrenamiento), \emph{batch} (recibimos todo el conjunto de entrenamiento, y no sucesivas porciones del mismo en
contraposición al aprendizaje por lotes) y pasivo (el algoritmo no tiene interacción con el usuario).

\subsection{Relación entre hipótesis y conjuntos}
En clasificación binaria, existe una biyección canónica entre la clase de hipótesis y el conjunto potencia de $X$, donde
a cada hipótesis se le asigna su clase positiva:

\begin{equation}
 \begin{array}{rcl} 
  \{h, h:X \rightarrow Y\} & \longrightarrow & \mathcal{P}(X) \\
  h & \longmapsto & X_h := \{x\in X: h(x) = 1\}
 \end{array}
 \label{biyeccion-canonica} 
\end{equation}

Es biyección claramente, lo que justifica que identifiquemos $\{h, h:X \rightarrow Y\}$ con $2^X$, donde $2^X$ suele ser una notación
empleada para referirse a $\mathcal{P}(X)$. Esta biyección nos permitirá trabajar refiriéndonos a $h:X \rightarrow Y$ 
como hipótesis o conjunto.

\subsection{Minimización del riesgo empírico}

\begin{definition} \textbf{Riesgo empírico}

Definimos el riesgo empírico o error empírico como:

\[L_S(h) = \frac{|i\in {1,\ldots, m}: h(x_i) \neq y_i|}{m}\]

\end{definition}

Es decir, es el error del clasificador $h$ sobre el conjunto de entrenamiento $S$. 

\begin{fact}
Sea $H\subseteq 2^X$. Sea $\mathcal{D}$ distribución sobre $\mathcal{X}$. Sea $f \in mathcal{H}$ fija. Entonces para
cualquier $h\in H$:
\[\mathbb{E}_{S\sim \mathcal{D}} [L_S(h)] = L_{\dist,f}(h)\]

\end{fact}

\begin{proof}
  Llamamos $p= \prob [f\neq h]$

  \begin{align*}
  \expect_{S\sim \dist} [L_S(h)] = \sum_{k=0}^m \frac{k}{m} \binom{m}{k} P^k(1-p)^{m-k} & = \sum_{k=1}^m \frac{k}{m} \binom{m}{k} p^k(1-p)^{m-k} =\\
  = \sum_{k=1}^m \binom{m-1}{k-1} p^k(1-p)^{m-k} = \sum_{k=0}^{m-1} \binom{m-1}{k} p^{k+1}(1-p)^{m-1-k} = \\
  = p\cdot \sum_{k=0}^{m-1} \binom{m-1}{k} p^{k}(1-p)^{m-1-k} = p(1+(1-p))^{m-1} = p
  \end{align*}
\end{proof}


\begin{definition} \textbf{Minimizador del riesgo empírico, ERM}

Decimos que un algoritmo $A: \underset{m\in \mathbb{N}}{\bigcup} (X\times Y)^m \rightarrow 2^{X}$ es un $ERM$ 
(\emph{Empirical Risk Minimizer}) si busca una hipótesis cuyo error empírico sea mínimo:

\[A(S) = \argmin_{h\in 2^X} L_S(h)\]
\end{definition}

Cuando notemos $ERM(S)$ nos refereriremos a una propiedad que se verifica para toda la clase de algoritmos que son ERM.

Trivialmente $L_S(f) = 0$ para cualquier $S \in (X \times Y)^m$ conjunto de entrenamiento, por la hipótesis de que existe una
verdadera función de etiquetado. Esto implica que $A(S) = 0$ para $A$ un algoritmo ERM.

Aunque $A(S)$ minimice el error sobre el conjunto de entrenamiento, esto no significa que el error $L_{\mathcal{D},f} (A(S))$ 
sea mínimo. Pensemos en el siguiente ejemplo:

\begin{example}
Sea $X = \mathbb{R}$, $\mathcal{D}$ la distribución uniforme sobre $[0,2]\subset \mathbb{R}$, y la siguiente función:

\[f(x) = \left\{\begin{array}{lcl}
1 && x\in [0,1]\\
0 && x\in \mathbb{R}\setminus [0,1]
\end{array}\right.\]


Sea $S = ((x_1,y_1), \ldots (x_m, y_m))$ un conjunto de entrenamiento de tamaño $m$ sin elementos repetidos y el clasificador:

\[h_S(x) = \left\{\begin{array}{lcl}
y_i && \exists i\in \{1\ldots m\} : x=x_i\\
0 && \nexists i\in \{1\ldots m\} : x=x_i
\end{array}\right.\]

Este clasificador es un ERM, pero $\prob[h_S(x)] = 1/2$, es decir, tiene el mismo nivel de acierto que el 
clasificador idénticamente 1. A este fenómeno lo denominamos \textbf{overfitting}.
\end{example}

\begin{definition} \textbf{Overfitting}

 Decimos que un clasificador $h: X\rightarrow Y$ produce overfitting sobre el conjunto de entrenamiento 
 $S$ si $L_S(h) << L_{\mathcal{D},f}(h)$.
\end{definition}

Necesitamos incorporar por tanto \textbf{conocimiento previo} al problema, de manera que revisitamos la definición de riesgo
empírico.

\subsection{ERM con sesgo inductivo}
Se intenta corregir el riesgo de producir \emph{overfitting} restringiendo el espacio de búsqueda, esto es, la clase de 
hipótesis $H \subseteq 2^X$ desde la que el algoritmo puede escoger un $h: X\rightarrow Y$. Llamamos a esto 
\emph{sesgo inductivo}, puesto que se asume una determinada clase de funciones $H$ en función de las 
características del problema y del conocimiento previo que tenemos del mismo.

\begin{definition} \textbf{ERM con sesgo inductivo}

Decimos que un algoritmo $A: \underset{m\in \mathbb{N}}{\bigcup} (X\times Y)^m \rightarrow 2^{X}$ es un $ERM$ con sesgo 
inductivo hacia $H$ y lo notamos $ERM_H$, si busca una hipótesis en $H$ cuyo error empírico 
sea mínimo, es decir:

\[A(S) = \argmin_{h\in H} L_S(h)\]
\end{definition}

Cuando notemos $ERM_H(S)$ nos refereriremos a una propiedad que se verifica para toda la clase de algoritmos que 
son $ERM_H$.

\begin{definition} \textbf{Hipótesis de factibilidad}

Diremos que $H$ verifica hipótesis de factibilidad respecto a $f\in 2^X$ y $\mathcal{D}$ distribución sobre $X$, si 
$\exists {\bar{h}} \in H$ verificando $L_{D,f}(\bar{h}) = 0$.
\end{definition}

\begin{fact}
Dada $H$ verificando factibilidad respecto a $f\in 2^X$ y $\mathcal{D}$ distribución sobre $X$, entonces siendo 
${\bar{h}} \in H$ tal que $L_{D,f}(\bar{h}) = 0$, se verifica:

\[\mprob \bigg[L_S(\bar{h}) = 0 \bigg] = 1\]

En particular: $\mprob \bigg[L_S(ERM_H(S)) = 0 \bigg] = 1$
\label{fact:ermh}
\end{fact}

  \begin{proof}
  Sea $A$ un $ERM_H$, y además sabemos que por hipótesis de factibilidad existe $\bar{h} \in H$ tal que
  $L_{\mathcal{D},f}(\bar{h}) = \prob[\bar{h}\neq f] = 0$

  Entonces: \[\mprob[L_S(\bar{h}) > 0] = \mprob
  [\exists i: \bar{h}(x_i) \neq f(x_i)] \le \sum_{i=1}^m \prob[h\neq f] = 0\]

  Y como $[L_S(\bar{h}) = 0] \subseteq [L_S(A(S)) = 0]$ por ser $A(S) = \argmin_{h\in \mathcal{H}} L_S(h)$, deducimos:
  \[1 = \mprob[L_S(\bar{h}) = 0] \le \mprob[L_S(A(S)) = 0] \le 1\]
  \end{proof}

El valor $L_{\mathcal{D},f}(ERM_H(S))$ dependerá por tanto del conjunto de entrenamiento $S$, y la elección del
mismo está sometida al azar. 

%Además, necesitamos definir cómo de buena será la predicción.