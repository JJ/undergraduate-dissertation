En este capítulo proporcionamos una introducción y una motivación al \textit{framework} teórico que usaremos
posteriormente. Daremos definiciones básicas como dominio, conjunto de etiquetas, error empírico, etc. que
usaremos asiduamente en los capítulos posteriores.

\section{Motivación}
El objetivo del aprendizaje automático es convertir datos en conocimiento a través de un razonamiento inductivo, de manera que
proporcionándole datos a una máquina seamos capaces de extraer un conocimiento (una generalización de los datos que nos permita
inferir información). Surge la pregunta de por qué es necesario el aprendizaje automático o \textit{machine learning}, si la
estadística también se encarga de obtener conocimiento a partir de unos datos.

\subsection{¿Por qué necesitamos \textit{machine learning}?}
\begin{enumerate}[i]
 \item Para resolver \textbf{tareas que requieren automatización}: entran dentro de esta categoría tanto aquellas tareas para
 las que no existe una axiomatización o un conocimiento exacto, como pueden ser el reconocimiento de dígitos o de voz, como 
 aquellas tareas que requieren del análisis de un gran número de datos, y quedan fuera de la capacidad humana para realizar
 un análisis estadístico manual. En el primer caso necesitamos apoyarnos en conocimiento auxiliar (por ejemplo, 
 un conjunto de dígitos o de muestras de voz preetiquetadas con los que poder comparar muestras sin etiquetar/clasificar); 
 en el segundo, se hace necsario el uso de una máquina para poder extraer conocimiento de todos los datos.
 
 \item \textbf{Tareas que requieren adaptatividad}: si cambian los datos de entrada, necesitamos que los algoritmos se readapten
 a ellos, y no tengamos un conocimiento rígido, sino que pueda variar/mejorar en función de la entrada.
\end{enumerate}

\subsection{Áreas relacionadas con el aprendizaje}
Entre las áreas relacionadas con el aprendizaje automático, cabe mencionar:

\begin{enumerate}[i]
 \item \textbf{Inteligencia Artificial}
 \item \textbf{Algorítmica}: debemos analizar el tiempo asintótico de los algoritmos mediante los que aprende la máquina.
 \item \textbf{Inferencia}: entre las diferencias que podemos mencionar con la estadística convencional, destaca la necesidad de 
 programar las tareas, dado el volumen de datos con el que normalmente se trabaja, mientras que en muchos análisis estadísticos basta 
 lápiz y papel. También destaca la \textbf{independencia respecto a distribución} con la que se trabaja (no se asume una distribución
 determinada sobre los datos). La principal diferencia del aprendizaje automático respecto a la inferencia es que la inferencia
 se encarga de comprobar la validez de las hipótesis que propone el estadista, mientras que el algoritmo de 
 \textit{machine learning} genera hipótesis para unos datos determinados, con unas ciertas condiciones de aproximación y error.
 \item \textbf{Álgebra lineal}: algoritmos como los SVM (máquinas de soporte vectorial) basados en álgebra lineal siguen
 siendo hoy día muy importantes, a pesar de haber sido desarrollados en los años 60.
 \item \textbf{Optimización de algoritmos}
\end{enumerate}

\subsection{Ejemplo práctico}\label{sec:first-ex}
Pensemos en un ejemplo: tenemos clientes de un banco que quieren solicitar un préstamo, y a todos se les categoriza
el tamaño del préstamo que quieren solicitar(cómo de grande es su importe) y su nivel de ingresos. Estas variables se miden 
en una escala de $0$ a $1$ donde $1$ es el nivel máximo. Queremos etiquetar a cada cliente como $1\equiv$ conceder préstamo o 
$0\equiv$ no concederlo.

Dado un histórico de $m\in \mathbb{N}$ clientes a los que se les concedieron y devolvieron o no préstamos, tenemos una tupla 
$((x_1, y_1), \ldots (x_m, y_m))$ donde $x_i = (x_i^{(1)}, x_i^{(2)}) \in [0,1]^2, y_i \in \{0,1\}$. Asimismo $x_i^{(1)}$ representa el 
tamaño del préstamo solicitado y $x_i^{(2)}$ representa el nivel ingresos mensuales. Tenemos a los clientes clasificados en 
función de si devolvieron los préstamos o no.

Queremos encontrar una función que ofrezca una predicción sobre cualquier cliente, para minimizar posibles pérdidas del banco, es
decir, buscamos una $f:[0,1]^2 \rightarrow \{0,1\}$ que llamaremos predicción.

Asumimos que los datos de los clientes de que disponemos no van a tener una distribución determinada y
van a ser idéntica e independientemente distribuidos (i.i.d.). El histórico siempre va a crecer, y no sabemos cómo va a hacerlo, 
queriendo aprovechar al máximo la información del mismo.

También asumiremos que tenemos una clase de predicciones $H \subseteq \{0,1\}^{[0,1]^2}$, de entre las que existe
una óptima. Por ejemplo, podríamos limitar la clase de predicciones sobre la que buscamos como subrectángulos de $[0,1]^2$, 
esto es, funciones:

\[h_{a,b,c,d} = \mathds{1}_{[a,b]\times[c,d]}, \qquad [a,b]\times [c,d] \subseteq [0,1]^2\]

\img{./imgs/rect-ex.png}{0.85}

También necesitaremos definir una medida de acierto: ¿cómo de buena es una predicción?.

\section{Definiciones básicas}
\label{sec:defs}

Damos unas notaciones/definiciones básicas que utilizaremos de aquí en adelante, en base a lo descrito en \ref{sec:first-ex}:

\begin{itemize}
\item \textbf{Dominio}: $X$. Llamamos \textbf{instancia} a $x\in X$

\item \textbf{Conjunto de etiquetas}: $Y$. Consideramos $Y = \{0,1\}$, lo que nos restringe de momento al paradigma de clasificación binaria. 
En ocasiones también usaremos $Y = \{-1,1\}$ para las etiquetas. Al $1$ se le llama clase positiva, y al $0$ o $-1$ clase negativa.

\item \textbf{Verdadero etiquetado}: \sloppy Asumimos la existencia de una función ${f: X \rightarrow Y}$ 
que proporciona la verdadera etiqueta de todas las instancias.

\item \textbf{Generación de instancias}: \fussy Se tiene $x\sim \mathcal{D} = (\Sigma, \prob)$. La distribución de probabilidad nos da 
información sobre la probabilidad de extraer cada posible instancia desde  $x \in X$. 

\item \textbf{Conjunto de entrenamiento}: $S = ((x_1,y_1), \ldots, (x_m,y_m)) \in (X \times Y)^m$ 
Nótese que llamarlo conjunto puede dar lugar a confusión, puesto que se trata de una tupla. Notaremos 
$S_x = (x_1, \ldots x_m)$.

De momento asumiremos que las etiquetas del conjunto de entrenamiento se corresponden con el verdadero etiquetado: 
$y_i = f(x_i)$, por lo que no podemos tener una instancia con etiquetas diferentes.

La elección de $S_x$ es idéntica e independientemente distribuida, esto es $x_i \sim \mathcal{D}$ para todo $i=1, \ldots, m$.
Lo notamos $S_x \sim \mathcal{D}^m$, o $S \sim \mathcal{D}^m$, por abuso de notación. Por lo visto en el capítulo
anterior, en \ref{sec:prob-space}, dicha probabilidad existe.

\item \textbf{Hipótesis/clasificador/predicción}: cada posible aplicación perteneciente a 
$\{h, h:X \rightarrow Y\} := 2^{X}$. 

\item \textbf{Error del clasificador}: Definimos el error del clasificador, suponiendo 
$\{x\in X : h(x) \neq f(x)\} := [h\neq f] \in \Sigma$ como:

\[L_{\mathcal{D},f}(h) :=  \prob [h \neq f]\]

\item \textbf{Algoritmo de aprendizaje}: Llamamos algoritmo de aprendizaje a cualquier aplicación que tome conjuntos de 
entrenamiento y devuelva hipótesis sobre el problema:

\[A: \underset{m\in \mathbb{N}}{\bigcup} (X\times Y)^m \rightarrow 2^{X}\]

Asumimos que el algoritmo no tiene acceso a la función de verdadero etiquetado $f: X \rightarrow Y$ ni a
la distribución $\mathcal{D}$.
\end{itemize}

Es decir, nos centraremos en el problema de la clasificación, que se trata de un aprendizaje supervisado (conocemos las etiquetas para
el conjunto de entrenamiento), \emph{batch} (recibimos todo el conjunto de entrenamiento, y no sucesivas porciones del mismo en
contraposición al aprendizaje por lotes) y pasivo (el algoritmo no tiene interacción con el usuario).

\subsection{Relación entre hipótesis y conjuntos}
\label{biyeccion-canonica} 
En clasificación binaria, existe una biyección canónica entre la clase de hipótesis y el conjunto potencia de $X$, donde
a cada hipótesis se le asigna su clase positiva:

\begin{equation*}
 \begin{array}{rcl} 
  \{h, h:X \rightarrow Y\} & \longrightarrow & \mathcal{P}(X) \\
  h & \longmapsto & X_h := \{x\in X: h(x) = 1\}
 \end{array}
\end{equation*}

Esta relación es biyección claramente, lo que justifica que identifiquemos $\{h, h:X \rightarrow Y\}$ con $2^X$, donde $2^X$ 
suele ser una notación empleada para referirse a $\mathcal{P}(X)$. Esto nos permite trabajar con una aplicación
$h:X \rightarrow Y$ o con su conjunto asociado.

\subsection{Minimización del riesgo empírico}

\begin{definition}[Riesgo empírico]
Definimos el riesgo empírico o error empírico de un clasificador $h:X \rightarrow Y$ como:

\[L_S(h) = \frac{|\{i\in {1,\ldots, m}: h(x_i) \neq y_i\}|}{m}\]

\end{definition}

Es decir, es el error del clasificador $h$ sobre el conjunto de entrenamiento $S$. Nótese que si $[h\neq f]$ como hemos
supuesto al principio, fijado una tamaño de muestra $m$ y $h\in 2^X$, $S \mapsto L_S(h)$ es una variable aleatoria, y por
tanto $L_S(h)^{-1}(]-\infty, a])$ sería un conjunto $\Sigma^m$ medible

\begin{fact}
Sea $H\subseteq 2^X$. Sea $\mathcal{D}$ distribución sobre $\mathcal{X}$. Sea $f \in H$ fija. Entonces para
cualquier $h\in H$:
\[\mexpect(L_S(h)) = L_{\dist,f}(h)\]

\end{fact}

\begin{proof}
  Llamamos $p= \prob [f\neq h]$

  Nótese que $h$ puede cometer como mucho $m$ errores.
  
  \begin{align*}
  & \mexpect(L_S(h)) = \sum_{k=0}^m \frac{k}{m} \binom{m}{k} p^k(1-p)^{m-k} = \sum_{k=1}^m \frac{k}{m} \binom{m}{k} p^k(1-p)^{m-k} =\\
  & = \sum_{k=1}^m \binom{m-1}{k-1} p^k(1-p)^{m-k} = \sum_{k=0}^{m-1} \binom{m-1}{k} p^{k+1}(1-p)^{m-1-k} = \\
  & = p\cdot \sum_{k=0}^{m-1} \binom{m-1}{k} p^{k}(1-p)^{m-1-k} = p(1+(1-p))^{m-1} = p
  \end{align*}
\end{proof}


\begin{definition}[Minimizador del riesgo empírico, ERM]
Decimos que un algoritmo $A: \underset{m\in \mathbb{N}}{\bigcup} (X\times Y)^m \rightarrow 2^{X}$ es un $ERM$ 
(\textit{Empirical Risk Minimizer}) si busca una hipótesis cuyo error empírico sea mínimo:

\[A(S) = \argmin_{h\in 2^X} L_S(h)\]
\end{definition}

Definimos $ERM(S)$ como todas las hipótesis producidas por algoritmos que son ERM
sobre el conjunto $S$. Cuando notemos $\mathcal{Q}(ERM(S))$ con $\mathcal{Q}$ un predicado sobre elementos 
de $2^X$ nos refereriremos a que la propiedad se verifica para todos los elementos de $ERM(S)$.

Trivialmente $L_{\mathcal{D},f}(f) = 0$ y $L_S(f) = 0$ para cualquier $S \in (X \times Y)^m$ conjunto de entrenamiento.
Esto último implica que $A(S) = 0$ para $A$ un algoritmo ERM.

Aunque $A(S)$ minimice el error sobre el conjunto de entrenamiento, esto no significa que el error $L_{\mathcal{D},f} (A(S))$ 
sea mínimo. Lo ilustramos en el siguiente ejemplo:

\begin{example}
Sea $X = \mathbb{R}$, $\mathcal{D}$ la distribución uniforme sobre $[0,2]\subset \mathbb{R}$, y la función
$f= \mathds{1}_{[0,1]}$.


Sea $S = ((x_1,y_1), \ldots (x_m, y_m))$ un conjunto de entrenamiento de tamaño $m$ sin elementos repetidos 
y el clasificador:

\[h_S(x) = \left\{\begin{array}{ll}
y_i & \textrm{si } \exists i\in \{1\ldots m\} : x=x_i\\
0   & \textrm{si } \nexists i\in \{1\ldots m\} : x=x_i
\end{array}\right.\]

No es difícil ver tomando $A(S) = h_S$, $A$ es un ERM. Pero se verifica $\prob[h_S \neq f] = 1/2$. $A(S)$ tiene el 
mismo nivel de acierto que el clasificador idénticamente $1$. A este fenómeno lo denominamos \textit{overfitting}.
\end{example}

\begin{definition}[Overfitting]

 Dado $h: X\rightarrow Y$ clasificador, diremos que produce overfitting sobre el conjunto de entrenamiento 
 $S$ si se cumple $L_S(h) << L_{\mathcal{D},f}(h)$.
\end{definition}

Necesitamos incorporar, por tanto, \textbf{conocimiento previo} al problema, de manera que revisitamos la definición de riesgo
empírico.

\subsection{ERM con sesgo inductivo}
Se intenta corregir el riesgo de producir \emph{overfitting} restringiendo el espacio de búsqueda, esto es, la clase de 
hipótesis $H \subseteq 2^X$ desde la que el algoritmo puede escoger un $h: X\rightarrow Y$. Llamamos a esto 
\emph{sesgo inductivo}, puesto que se asume una determinada clase de funciones en función de las 
características del problema y del conocimiento previo que tenemos del mismo.

\begin{definition}[ERM con sesgo inductivo]
Decimos que un algoritmo $A: \underset{m\in \mathbb{N}}{\bigcup} (X\times Y)^m \rightarrow 2^{X}$ es un $ERM$ con sesgo 
inductivo hacia $H$ y lo abreviamos $ERM_H$, si busca una hipótesis en $H$ cuyo error empírico 
sea mínimo, es decir:

\[A(S) = \argmin_{h\in H} L_S(h)\]
\end{definition}

Se define $ERM_H$ y $ERM_H(S)$ de manera análoga a $ERM$ y $ERM(S)$.

Dada $H\in 2^X$, podemos tomar la clase de funciones iguales probabilístamente a $f$:
$\Hclass = \{f\in 2^X: \exists h\in H \textrm{ verificando }\prob[f\neq h] = 0\}$.

\begin{definition}[Hipótesis de factibilidad]

Diremos que $H$ verifica hipótesis de factibilidad respecto a $f\in 2^X$ y $\mathcal{D}$ distribución sobre 
$X$ si $f\in \Hclass$. 
\end{definition}

Por definición: $\prob[f\neq h]=0$ sii $L_{D,f}(h) = 0$ sii $L_{D,h}(f) = 0$. También es inmediato afirmar que
$H \subseteq \Hclass$ para cualquier $\dist$.

\begin{fact}
Dada $H$ verificando factibilidad respecto a $f\in 2^X$ y $\mathcal{D}$ distribución sobre $X$, entonces siendo 
${\bar{h}} \in H$ tal que $L_{D,f}(\bar{h}) = 0$, se verifica, con $A$ algoritmo arbitrario:
\begin{enumerate}[i.]
\item $\mprob \bigg[L_S(\bar{h}) = 0 \bigg] = 1$.
\item $\mprob \bigg[L_S(ERM_H(S)) = 0 \bigg] = 1$.
\item $L_{\dist,f}(A(S)) = L_{\dist,\bar{h}}(A(S))$ para un algoritmo $A$ arbitrario.
\end{enumerate}
\label{fact:factibilidad}
\end{fact}

\begin{proof}
Fijamos $A$ un $ERM_H$. Sabemos que por hipótesis de factibilidad existe $\bar{h} \in H$ tal que
$L_{\mathcal{D},f}(\bar{h}) = \prob[\bar{h}\neq f] = 0$

Entonces: \[\mprob[L_S(\bar{h}) > 0] = \mprob
[\exists i: \bar{h}(x_i) \neq f(x_i)] \le \sum_{i=1}^m \prob[h\neq f] = 0\]

Y como $[L_S(\bar{h}) = 0] \subseteq [L_S(A(S)) = 0]$ por ser $A(S) = \argmin_{h\in \mathcal{H}} L_S(h)$, deducimos:
\[1 = \mprob[L_S(\bar{h}) = 0] \le \mprob[L_S(A(S)) = 0] \le 1\]

Fijamos ahora $A$ un algoritmo. Veamos $L_{\dist,f}(A(S)) = L_{\dist,\bar{h}}(A(S))$:
  \begin{align*}
  \prob[A(S) \neq f] &= \prob\bigg([A(S) \neq f \wedge f=h] \cup[A(S) \neq f \wedge f\neq h]\bigg) = \\
  &= \prob[A(S) \neq f \wedge f=h] + \prob[A(S) \neq f \wedge f\neq h] = \\
  &= \prob[A(S) \neq h \wedge f=h] + 0 = \prob[A(S)\neq h]
  \end{align*}
\end{proof}

Nótese que para definir $ERM$ y $ERM_H$ necesitamos poder tomar $\argmin$ siempre. Parece una condición fuerte,
pero en nuestro caso, como $L_{S}(f)$ siendo $S$ de tamaño $m$ puede tomar finitos valores $0, \frac{1}{m}, \ldots, \frac{m-1}{m},1$,
el mínimo se alcanza. El valor $L_{\mathcal{D},f}(ERM_H(S))$ depende por tanto del conjunto de entrenamiento $S$, y la elección del
mismo está sometida al azar. Ahí entrará nuestra definición de aprendizaje PAC, midiendo conjuntos probabilísticamente.