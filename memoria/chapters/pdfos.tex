\section{Algoritmo PDFOS}
\subsection{Motivación}
Dada una función de distribución de $X$ variable aleatoria, $F(x)$, si esta función es derivable casi seguramente entonces
podemos tomar la función de densidad como la derivada de la función de distribución, casi seguramente, es decir:
\[f(x) = \lim_{h\rightarrow 0} \frac{F(x+h) - F(x-h)}{2h} = \lim_{h\rightarrow 0} \frac{P(x-h < X \le x+h)}{2h}\]

Si tenemos muestras aleatorias de $X$, a saber, $X_1, \ldots X_n$ y $x_1, \ldots x_n$ una realización muestral, entonces un 
estimador para $f$ sería:
\[\widehat{f}(x) = \frac{1}{2hn} \bigg[\textrm{Número de } x_1, \ldots, x_n \textrm{ que se quedan en ]x-h, x+h[}\bigg]\]

Es decir, definiendo $\omega(x) = \left\{\begin{array}{ll} 
                                \frac{1}{2} &, |x| < 1\\
                                0 & \textrm{en otro caso}
                                \end{array}\right.$
                                
                                
y $w_h(x) = w\left(\left|\frac{x}{h}\right|\right)$, podríamos reformular $\widehat{f}$ como:
\[\widehat{f}(x) = \frac{1}{nh} \sum_{i=1}^n \omega_h(x-x_i)\]

Es decir, supuestas que las observaciones $x_1, \ldots, x_n$ se distancian múltiplos de $2h$ (caen en el centro de intervalos
de longitud $2h$, habríamos construido $\widehat{f}$ a base de un histograma donde cada barra tiene ancho $2h$ y altura 
$\frac{1}{2nh} \cdot \bigg[[\textrm{Número de muestras } x_1, \ldots, x_n \textrm{ en el intervalo}]\bigg]$. Al parámetro $h$
se le llama parámetro de \textit{bandwidth}, por referencia al significado que tiene en el caso de los histogramas.

\subsection{Generalización a funciones kernel}
Tomar $w = \frac{1}{2} \mathds{1}_{]-1,1[}$ presenta el problema de que $\widehat{f}$ será una función a saltos, y no
será continua. Surge una generalización al tomar $\omega$ como funciones verificando $\int_{\Omega} \omega(x) dx = 1$, 
siendo $\Omega$ el dominio de $X$. Se suelen considerar además funciones simétricas $w(x) = w(-x)$ para cualquier 
$x\in \Omega$.

El estimador que se suele usar para evaluar la bondad de $\widehat{f}$ es el error cuadrático medio integrado (MISE):

\[MISE(h) = \underset{x_1, \ldots, x_n}{\expect} \int (\widehat{f}(x) - f(x))^2 dx\]

\imgcaption{./imgs/kernel-estimation.png}{Ejemplo de estimación de densidad con funciones kernel \footnotemark}{0.9}

\footnotetext{Imagen de dominio público tomada de Wikimedia Commons}

\subsection{Funciones kernel Gaussianas multivariantes}
El algoritmo PDFOS (\textit{Probability Distribution Oversampling}) se basa en tomar funciones kernel Gaussianas multivariantes.
Recordamos la definición de la función de densidad de la distribución Gaussiana multivariante de media $0$ y covarianza $\Psi$:

\[\phi^{\Psi}(x) = \frac{1}{\sqrt{2\pi \cdot det(\Psi)}} exp\left(\frac{1}{2} x \Psi^{-1} x^T \right)\]

Dada $\spos = \{x_i = (w_1^{(i)}, \ldots, w_d^{(i)})\}_{i=1}^m$, la clase minoritaria, calcularemos el estimador no sesgado 
para la covarianza:

\[U = \frac{1}{m-1} \sum_{i=1}^m (x_i - \overline{x})(x_i - \overline{x})^T, 
  \qquad \textrm{siendo } \overline{x} = \frac{1}{M}\sum_{i=1}^M x_i\]
  
Las funciones kernel que tomaremos serán de la forma: $\phi = \phi^{U}$, y tendremos que ajustar el \textit{bandwidth} $h$ de
$\phi_h(x) = \phi^U\left(\frac{x}{h}\right)$ para minimizar el MISE. A tales efectos, hay que minimizar la función de validación
cruzada:
\[M(h) = \frac{1}{m^2} \sum_{i=1}^m \sum_{j=1}^m \phi_h^{\ast} (x_i - x_j) + \frac{2}{m} \phi_h(0)\]

donde se toma la aproximación:
\[\phi_h^{\ast} \approx \phi_{h\sqrt{2}} - 2\phi_h\]

Nótese que $x\mapsto \phi_h(x-x_i)$ es la función de densidad de una normal con matriz de covarianza $h^2 U$ y centrada en $x_i$.

Una vez minimizada la función de validación cruzada $M$, el esquema de generación de instancias se basará en dada una instancia
$x_i \in \spos$, tomar $x_i + h R r$, donde $r\sim N^m(0,1)$ y$R$ es la matriz triangular superior resultante de la descomposición
de Choleski. Análogamente al caso unidimensional, donde se pueden generar instancias desde $N(\mu, \sigma)$ tomando $\mu + \sigma r$
con $r\sim N(0,1)$, en el caso multivariante dada $U =  R \cdot R^T$, se pueden generar instancias siguiendo una distribución $N^m(\mu, U)$,
tomando $\mu + R \cdot r$, donde $r\sim N^m(0,1)$.

En \cite{gao2014} se usa para descomponer $U = R\cdot R^T$ la descomposición de Choleski 
\footnote{\url{https://en.wikipedia.org/wiki/Cholesky_decomposition}}, que sólo sirve para matrices definidas positivas.

\begin{lemma}
 $\sum_{i=1}^m (x_i - \overline{x})(x_i - \overline{x})^T$ es una matriz semi-definida positiva.
\end{lemma}
\begin{proof}
 Dado $y \in \mathbb{R}^d$, se tiene:
 
 \begin{align*}
 y^T \left(\sum_{i=1}^m (x_i - \overline{x})(x_i - \overline{x})^T\right) y &=
 \sum_{i=1}^m ( \underbrace{(x_i - \overline{x})^T y}_z)^T ((x_i - \overline{x})^T y) = \\
 &\underset{z \textrm{ es un vector}}{=} \sum_{i=1}^m ((x_i - \overline{x})^T y)^2 \ge 0
 \end{align*}
\end{proof}

Luego al estimador $S$ que calculamos para la varianza le podemos calcular la descomposición de Choleski cuando sea 
definido positivo ($y^T \cdot S y > 0$ de manera estricta para todo $0 \neq y\in \mathbb{R}^d$. Pero no está garantizado 
que siempre sea definido positivo.

\begin{algorithm}[H]
\begin{algorithmic}[1]
  \REQUIRE $\spos = \{z_1=(x_1, y_1), \ldots z_m=(x_m, y_m)\}$, ejemplos positivos
  \REQUIRE $T$, número de instancias sintéticas deseado
  \STATE{$S = \spos_x = \{x_i=(w_1^{(i)}, \ldots w_d^{(i)})\}_{i=1}^m$}
  \STATE{$S'= \emptyset$}
  \STATE{$\sigma =$ búsqueda con GridSearch que minimice $M$}
  \STATE{Calcular $U$ la matriz de covarianza de $S$}
  \STATE{Calcular descomposición de Choleski de $U$, donde $U=R^{T} R$, y $R$ triangular superior}
  \NEWLINE
  \FOR{$i=1, \ldots, T$}
    \STATE{Escoger $x\in S$}
    \STATE{Escoger $r$ siguiendo una normal multivariante, $r \sim N^d(0,1)$}
    \STATE{$S' = S' \cup \{x + \sigma r R\}$}
  \ENDFOR
  \NEWLINE
  \RETURN{$S'$, ejemplos positivos sintéticos}
\end{algorithmic}
\caption{Algoritmo de \textit{oversampling} PDFOS}
\label{alg:pdfos}
\end{algorithm}


Se comenzó usando el siguiente algoritmo de búsqueda \textit{grid}:

\begin{algorithm}[H]
\begin{algorithmic}[1]
  \STATE{Hacer $M_{best} = \infty$}
  \FOR{$\tau \in \{0.2, 0.22, 0.24, \ldots 2\}$}
    \IF{$M(\tau) < M_{best}$}
      \STATE{$\sigma = \tau, M_{best} = M$}
    \ENDIF
  \ENDFOR
  \RETURN{$\sigma$}
\end{algorithmic}
\caption{Algoritmo de búsqueda GridSearch}
\end{algorithm}

Posteriormente se agregaron también al algoritmo de búsqueda la comprobación con los valores de parámetro propuestos por Scott
y Silverman (siendo $d$ el tamaño del espacio de atributos, y $m$ el número de instancias pertenecientes a la clase minoritaria):

\[\sigma_{Scoot} = \left(\frac{1}{m}\right)^{\frac{1}{d+4}}, \quad \sigma_{Silverman} = \left(\frac{4}{m(d+2)}\right)^{\frac{1}{d+4}}\]

\subsection{Críticas al algoritmo}
Podemos efectuar dos críticas principales al algoritmo, no estando reflejadas las siguientes consideraciones en el \textit{paper}
de sus autores:

\begin{itemize} 
 \item No toda matriz $S$ es invertible, siendo el caso del dataset \texttt{ecoli1} incluido en el paquete de software 
 desarrollado, por ejemplo, una matriz de covarianza no invertible. En el caso de nuestro software se ha decidido interrumpir
 el proceso en caso de que se encuentre una matriz no invertible, avisándolo a tales efectos. Podría estudiarse cómo encajaría
 dentro del algoritmo sustituir la inversa convencional por la inversa generalizada 
 \footnote{\url{https://en.wikipedia.org/wiki/Multivariate_normal_distribution\#Degenerate_case}}.

 \item El método de Choleski sólo se puede aplicar en los casos en los que la matriz es definida positiva, por lo que 
 al implementarse el algoritmo nos hemos preocupado de usar el método de Choleski con pivotado (lo usa la función \texttt{rmvnorm}
 de \texttt{mvtnorm} para generar las instancias, para lo cual hubo que revisar el código de dicha función). El método
 de Choleski con pivotado sí que puede emplearse en el caso de matrices semidefinidas positivas no definidas positivas.
\end{itemize}
