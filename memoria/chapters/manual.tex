\inputencoding{utf8}
\HeaderA{racog}{Rapidy Converging Gibbs algorithm.}{racog}
%
\begin{Description}\relax
Allows you to treat imabalanced discrete numeric datasets by generating
synthetic minority examples by approximating their probability distribution.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
racog(dataset, iterations, burnin = 100, lag = 20, 
      classAttr = "Class")
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{dataset}] data.frame to treat. All columns, except \code{classAttr} one,
have to be numeric.

\item[\code{iterations}] Integer. Number of iterations to run for each minority
example.

\item[\code{burnin}] Integer. It determines how many examples generated for a given
one are going to be discarded firstly. By default, 100.

\item[\code{lag}] Integer. Number of iterations between new generated example for a
minority one. By default, 20.

\item[\code{classAttr}] String. Indicates the class attribute from \code{dataset}.
Must exist in it.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Aproximates minority distribution using Gibbs Sampler. Dataset must be
discretized and numeric. In each iteration, it builds a new sample using a
Markov chain. It discards first \code{burnin} iterations, and from then on,
it validates each \code{lag} example as a new minority example. It generates
\eqn{d (iterations-burnin)/lag}{} where \eqn{d}{} is minority examples number.
\end{Details}
%
\begin{Value}
A \code{data.frame} with the same structure as \code{dataset},
containing the synthetic examples generated.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
data(iris0)
set.seed(12345)

# Generates new minority examples
newSamples <- racog(iris0, burnin = 10, iterations = 100, classAttr = "Class")

\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{wracog}{Wrapper for Rapidy Converging Gibbs algorithm.}{wracog}
%
\begin{Description}\relax
Generates synthetic minority examples by approximating their probability
distribution until sensitivity of \code{wrapper} over \code{validation}
cannot be further improved. Works only on discrete numeric datasets
\end{Description}
%
\begin{Usage}
\begin{verbatim}
wracog(train, validation, wrapper, slideWin = 10, threshold = 0.02,
  classAttr = "Class")
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{train}] \code{data.frame}. A initial dataset to generate first model.
All columns, except \code{classAttr} one, have to be numeric

\item[\code{validation}] \code{data.frame}. A dataset to compare results of
consecutive classifiers. Must have the same structure of \code{train}.

\item[\code{wrapper}] An \code{S3} object. There must be a method
\code{\LinkA{trainWrapper}{trainWrapper}} implemented for the class of the object, and a
\code{\LinkA{predict}{predict}} method implemented for the class of the model
returned by \code{trainWrapper}.

\item[\code{slideWin}] Number of last sensitivities to take into account to meet the
stopping criteria. By default, 10.

\item[\code{threshold}] Threshold that the last \code{slideWin} sensitivities mean
should reach. By default, 0.02.

\item[\code{classAttr}] String. Indicates the class attribute from \code{dataset}.
Must exist in it.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Until the last \code{slideWin} executions of \code{wrapper} over
\code{validation} dataset reach a mean sensitivity lower than
\code{threshold}, the algorithm keeps generating samples using Gibbs Sampler,
and adding misclassified samples with respect to a model generated by a
former train, to the train dataset. Iniial model is built on initial
\code{train}.
\end{Details}
%
\begin{Value}
A \code{data.frame} with the same structure as \code{dataset},
containing the synthetic examples generated.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
data(haberman)
set.seed(12345)
myWrapper <- structure(list(), class="C50Wrapper")
trainWrapper.C50Wrapper <- function(wrapper, train, trainClass){
  C50::C5.0(train, trainClass)
}

trainFold <- sample(1:nrow(haberman), nrow(haberman)/2, FALSE)
newSamples <- wracog(haberman[trainFold, ], haberman[-trainFold, ],
                     myWrapper, classAttr = "Class")

\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{rwo}{Random Walk Oversampling}{rwo}
%
\begin{Description}\relax
Generate synthetic minority examples for a dataset trying to preserve the
variance and mean of the minority class. Works on every type of dataset.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
rwo(dataset, numInstances, classAttr = "Class")
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{dataset}] \code{data.frame} to treat

\item[\code{numInstances}] Integer. Number of new minority examples to generate.

\item[\code{classAttr}] String. Indicates the class attribute from \code{dataset}.
Must exist in it.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Generates \code{numInstances} new minority examples for \code{dataset},
adding to the each column of the j-th example its variance scalated by the
inverse of the number of minority examples and a factor following a N(0,1)
distribution which depends on the example.
\end{Details}
%
\begin{Value}
A \code{data.frame} with the same structure as \code{dataset},
containing the synthetic examples generated
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
data(iris0)
set.seed(12345)

newSamples <- rwo(iris0, numInstances = 100, classAttr = "Class")

\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{neater}{Fitering of oversampled data based on non-cooperative game theory}{neater}
%
\begin{Description}\relax
Filter oversampled examples from a binary class \code{dataset} using game
theory to find out if keeping an example is worthy enough.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
neater(dataset, newSamples, k, iterations, smoothFactor = 1,
  classAttr = "Class")
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{dataset}] The original \code{data.frame}.

\item[\code{newSamples}] A \code{data.frame} containing the samples to be filtered.
Must have the same structure as \code{dataset}.

\item[\code{k}] Integer. Number of nearest neighbours to use in KNN algorithm to
rule out samples.

\item[\code{iterations}] Integer. Number of iterations for the algorithm.

\item[\code{smoothFactor}] A positive real. By default 1.

\item[\code{classAttr}] String. Indicates the class attribute from \code{dataset}
and \code{newSamples}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Uses game theory and Nash equilibriums to calculate the minority examples
probability of trully belongin to the minority class. It discards examples
that at the final stage of the algorithm have more probability of been a
mayority example than a minority one.
\end{Details}
%
\begin{Value}
filtered samples as a \code{data.frame} of same structure as
\code{new.Samples}.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
data(iris0)
set.seed(12345)

newSamples <- smotefamily::SMOTE(iris0[,-5], iris0[,5])$syn_data
# SMOTE overrides Class attr turning it into class
# and dataset must have same class attribute as newSamples
names(newSamples) <- c(names(newSamples)[-5], "Class")

neater(iris0, newSamples, k = 5, iterations = 100,
       smoothFactor = 1, classAttr = "Class")
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{pdfos}{Probability density function estimation based over-sampling}{pdfos}
%
\begin{Description}\relax
Generate synthetic minority examples for a numerical dataset approximating a
Gaussian multivariate distribution which best fits the minority data.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
pdfos(dataset, numInstances, classAttr = "Class")
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{dataset}] \code{data.frame} to treat

\item[\code{numInstances}] Integer. Number of new minority examples to generate.

\item[\code{classAttr}] String. Indicates the class attribute from \code{dataset}.
Must exist in it.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
To generate the synthetic data it approximates a normal distribution of mean
a given example of such class, and the variance of the minority class \eqn{S}{}
multiplied by a parameter which is approximated to minimize Mean Integrated
Squared Error of a Gaussian multivariate kernel function.
\end{Details}
%
\begin{Value}
A \code{data.frame} with the same structure as \code{dataset},
containing the synthetic examples generated
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
data(iris0)
set.seed(12345)

newSamples <- pdfos(iris0, numInstances = 100)

\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{plotComparison}{Plots comparison between the original and the new balanced dataset.}{plotComparison}
%
\begin{Description}\relax
It plots a grid of one to one variables placing the former dataset graphic
next to the balanced one, for each pair of attributes.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
plotComparison(dataset, anotherDataset, attrs, cols = 2,
  classAttr = "Class")
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{dataset}] A \code{data.frame}. The former and imbalanced dataset.

\item[\code{anotherDataset}] A \code{data.frame}. The balanced dataset. \code{dataset}
and \code{anotherDataset} must have the same columns.

\item[\code{attrs}] Vector of String. Attributes to compare. The function generates
each posible combination of attributes.

\item[\code{cols}] Integer. It indicates the number of columns of resulting grid.
Must be an even number. Default value of 2.

\item[\code{classAttr}] String. Indicates the class attribute from \code{dataset}.
Must exsits in it.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
Plot of 2D comparison between the variables.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
data(iris0)
set.seed(12345)

rwoSamples <- rwo(iris0, numInstances = 100)
rwoBalanced <- rbind.data.frame(iris0, rwoSamples)
plotComparison(iris0, rwoBalanced, names(iris0), cols = 2, classAttr = "Class")

\end{ExampleCode}
\end{Examples}
