\section{PAC cognoscibilidad}
Proporcionamos a continuación una definición que materializa el concepto de que un algoritmo pueda aprender de los datos. Las
siglas PAC derivan de Probablemente Aproximadamente Correcto. Conviene tener en cuenta para la siguiente definición la
biyección canónica \ref{biyeccion-canonica}, que nos permite asegurar la medibilidad de las hipótesis incluyéndo sus conjuntos
asociados en la $\sigma$-álgebra.

\begin{definition}[PAC cognoscible]
Una clase de funciones $H \subseteq 2^X$ es PAC cognoscible sii existen $A: \underset{m\in \mathbb{N}}{\bigcup} (X\times Y)^m \rightarrow 2^X$ 
y $m_{H} : ]0,1[^2\rightarrow \mathbb{N}$ verificando que para todo
$0 < \varepsilon, \delta < 1$, para toda distribución $\mathcal{D}=(\Sigma, \prob)$ sobre $X$ tal que $H\subseteq \Sigma$
y para toda función de verdadero etiquetado $f\in \Hclass$, dado $m \ge m_H(\varepsilon, \delta)$ entonces:
\[
  \mprob \bigg[L_{\mathcal{D},f}(A(S)) \le \varepsilon \bigg] \ge 1-\delta
\]
\label{def:pac-original}
\end{definition}

$A$ recibe el nombre de \textit{algoritmo de aprendizaje}, $m_H$ se llama \textit{complejidad muestral}, $(1-\delta)$ es la 
\textit{confianza} de la predicción y $(1-\varepsilon)$ la \textit{exactitud}. Estos dos últimos parámetros explican los 
calificativos aproximadamente ($\equiv$ confianza) y correcto ($\equiv$ exactitud).

En lo que sigue supondremos que las distribuciones de las que se habla, $\dist$, incluyen en la $\sigma$-álgebra las propias
hipótesis, y que tienen las garantías de medibilidad necesarias sobre $\bigg[L_{\mathcal{D},f}(A(S)) \le \varepsilon \bigg]$.

Nótese que la definición de PAC cognoscibilidad implica hipótesis de factibilidad de $H$ respecto a la función de verdadero
etiquetado $f$ y la distribución $\dist$. El último apartado de la proposición \ref{fact:factibilidad} 
nos permite dar una caracterización con $f\in H$ estrictamente:

\begin{charact}[PAC cognoscible]
Una clase de funciones $H \subseteq 2^X$ es PAC cognoscible sii existen una función 
$m_{H} : ]0,1[^2\rightarrow \mathbb{N}$, llamada complejidad muestral, y 
$A: \underset{m\in \mathbb{N}}{\bigcup} (X\times Y)^m \rightarrow 2^X$ verificando que para todo
$0 < \varepsilon, \delta < 1$, para toda distribución $\mathcal{D}$ sobre $X$ y para toda función de 
verdadero etiquetado $f\in H$, dado $m \ge m_H(\varepsilon, \delta)$ entonces:
\[
  \mprob \bigg[L_{\mathcal{D},f}(A(S)) \le \varepsilon \bigg] \ge 1-\delta
\]
\label{def:pac}
\end{charact}

Consideraremos $m_{H}$ única en el sentido de que para cada $(\varepsilon, \delta)$ nos devuelva el menor natural
verificando las hipótesis del enunciado.

\begin{lemma}[Desigualdad fundamental]
 Dados $m\in \mathbb{N}, \varepsilon\in ]0,1[$ se verifica $(1-\varepsilon)^m \le e^{-\varepsilon m}$
 \label{lemma:fund-ineq}
\end{lemma}
\begin{proof}
 Basta hacer la demostración para $m=1$.
 
 Tomando $h(\varepsilon) = e^{-\varepsilon} + \varepsilon - 1$, se tiene que $h$ es derivable y 
 $h'(\varepsilon) = -e^{-\varepsilon} + 1 \ge 0$. Como $h(0) = 0$, se cumple $h \ge 0$.
\end{proof}

\begin{theorem}[Las clases finitas son PAC cognoscibles]
Sea $H \subseteq 2^{X}$ finito. Entonces $H$ es PAC cognoscible con:
\[
  m_H(\varepsilon, \delta) \le \left\lceil \frac{1}{\varepsilon} \log\left(\frac{|H|}{\delta} \right) \right\rceil
\]
\label{th:finitas-pac}
\end{theorem}
  \begin{proof}
   Fijamos $\varepsilon, \delta \in ]0,1[$. Sea una distribución $\mathcal{D}$ sobre $X$, $m\in \mathbb{N}$ y una función de verdadero 
   etiquetado $f\in H$, tomamos la clase de hipótesis ``malas'', que será numerable al ser $|H| < \infty$:
   \[
     H_B = \{h\in H: L_{\mathcal{D},f}(h) > \varepsilon\}
   \]

   Sea $A$ un $ERM_{H}$, entonces:
   \[
     \mprob [L_{\mathcal{D},f}(A(S)) > \varepsilon] \le \mprob 
     [\exists h\in H_B : L_S(h) = 0] \le \sum_{h\in H_B} P [L_S(h) = 0] 
   \]

   La primera desigualdad viene dada por la proposición \ref{fact:factibilidad}. La segunda, por subaditividad, puesto que:
   \[
     \mprob [\exists h\in H_B : L_S(h) = 0] = \mprob \left(\underset{h\in H_B}{\bigcup} [L_S(h) = 0]\right)
   \]

   Además, fijada $h\in H_B$, como $L_{\mathcal{D},f}(h) > \varepsilon$:
   \begin{align*}
   \mprob[L_S(h) = 0] = \mprob[h(x_i) = f(x_i), i =1,\ldots, m\}]\\
   = \prod_{i=1}^m \prob[h = f] = \prod_{i=1}^m (1 - L_{\mathcal{D},f}(h)) \le (1-\varepsilon)^m 
   \underset{\textrm{\ref{lemma:fund-ineq}}}{\le} e^{-\varepsilon m}
   \end{align*}

   Las dos desigualdades probadas, junto a la hipótesis del enunciado, y usando $H_B \subseteq H$ dan lugar a:
   \[
     \mprob[L_{\mathcal{D},f}(h_S) > \varepsilon] \le |H|e^{-\varepsilon m}
   \]
   
   Y basta tomar $m\in \mathbb{N}$ tal que $|H|e^{-\varepsilon m} \le \delta$, sii 
   $m\ge \frac{1}{\varepsilon} \log\left(\frac{|H|}{\delta}\right)$.
  \end{proof}

¿Hay ejemplos de clases infinitas PAC cognoscibles? La respuesta es afirmativa. Veamos un ejemplo.

\begin{example}
  \begin{definition}[Clasificadores de rectángulo]
  La clase de clasificadores de rectángulo en el plano se define por:
  \[
    H^2_{rec} = \{ h_{a,b,c,d}: a\le b, c\le d\}
  \]
  donde $h_{a,b,c,d} = \mathds{1}_{[a,b]\times [c,d]}$
  \label{def:rec2}
  \end{definition}
    \begin{fact}
    $H_{rec}^2$ es PAC cognoscible
    \end{fact}

    \begin{proof}
    Haremos la prueba tratando los clasificadores como conjuntos, en virtud de la biyección canónica \ref{biyeccion-canonica}.
    
    Sea $A$ el algoritmo que devuelve el rectángulo más pequeño que engloba a todos los ejemplos positivos de un conjunto 
    de entrenamiento $S$. Claramente $A$ es un $ERM_H$.

    Sea $R= [a,b]\times [c,d]$ el rectángulo que proporciona el verdadero etiquetado. Fijamos $\varepsilon, \delta \in ]0,1[$.
    
    Supongamos $\mathcal{B} = [L_{\dist,R}(A(S)) > \epsilon] \neq \emptyset$. Sea $S\in \mathcal{B}$ y 
    $A(S) = [a^{\ast}, b^{\ast}] \times [c^{\ast}, d^{\ast}]$. Tomamos los rectángulos:
    \begin{align*} 
    R_1^S = [a,b^{\ast}] \times [c,d], \qquad R_2^S = [a^{\ast},b] \times [c,d] \\ 
    R_3^S =[a,b] \times [c,d^{\ast}],   \qquad R_4^S = [a,b] \times [c^{\ast},d]     
    \end{align*}
    verificándose $R_j^S \subseteq R$ para todo $j$. Además debe existir $L_{\mathcal{D},R}(R_j^S) \ge \frac{\varepsilon}{4}$, 
    para algún $j\in \{1, \ldots 4\}$, ya que en caso contrario, por subaditividad tendríamos $L_{\dist,R}(A(S)) < \epsilon$. 
    Además podemos suponer s.p.g. $(R_i^S)^c = R\setminus R_i^S$ puesto que un clasificador $A(S)$ sólo fallará en el
    espacio restante entre $R$ y $A(S)$.
    
    Seleccionamos $R_i \in \{R_i^S: S\in \mathcal{B}\}$, $i=1, \ldots, 4$. suponiendo s.p.g. que existen los cuatro (caso opuesto 
    bastaría hacer la demostración para los rectángulos que sí existiesen). Fácilmente se comprueba que 
    $S\in \mathcal{B} \Rightarrow A(S) \cap R_i^c = \emptyset$ para todo $i=1, \ldots, 4$, usando de nuevo reducción al 
    absurdo y subaditividad.
    
    La demostración acaba probando que:
    \begin{align*}
    \mprob [\exists i : A(S)\cap R_i^c = \emptyset] &\le \sum_{i=1}^4 \mprob[A(S)\cap R_i^c = \emptyset] \\
                                                    &\le 4 \left(1-\varepsilon \right)^m \le 4e^{-\varepsilon m/4}
    \end{align*}

    Juntando toda esta información: 
    \[
      \mprob[L_{\dist,R}(A(S)) > \epsilon] \le \mprob [\exists i : A(S)\cap R_i^c = \emptyset] \le 4e^{-\varepsilon m/4}
    \]
                                    
    y tomando $m > \frac{4}{\varepsilon} log \left( \frac{4}{\delta} \right)$ llegaríamos al resultado buscado.
    \end{proof}
\end{example}

\section{APAC cognoscibilidad}

Nótese que las condiciones exigidas en PAC cognoscibilidad son muy fuertes: cumplir la hipótesis de factibilidad y que 
la hipótesis devuelta deba estar en $H$.Relajaremos esta definición con el concepto de PAC agnóstico (APAC).
Vamos a relajar tanto la definición de cognoscibilidad como el marco teórico de aprendizaje.

Sea $Z$ un conjunto, $\mathcal{D} = (\Sigma, \mathcal{P})$ distribución sobre $Z$ en lo que sigue.

\begin{definition}[Función de pérdida]
Sea $H$ un conjunto arbitrario, se denomina función de pérdida de $H$ sobre $Z$ a cualquier función de la 
forma:
\[
  l : H \times Z \rightarrow \mathbb{R}_0^{+}
\]
\end{definition}

Modificamos los siguientes puntos de las definiciones básicas en \ref{sec:defs}:

\begin{itemize}
  \item \textbf{Verdadero etiquetado}. No asumimos que exista.
  \item \textbf{Generación de instancias}. Tenemos una distribución $\mathcal{D}$ sobre $Z = (X,Y)$ donde $X$ es el dominio
  y $Y$ el conjunto de etiquetas.
  \item \textbf{Error del clasificador}- Lo redefinimos como:
  \[
    L_{\mathcal{D}}(h) :=  \expect_{z\sim \dist} (l(h,z))
  \]  
  \item \textbf{Error empírico}. Lo redefinimos como:
  \[L_{S} (h) := \frac{1}{m} \sum_{i=1}^m l(h,z_i)\]
\end{itemize}

\begin{definition}[APAC cognoscible]
Una clase $H$ es agnósticamente PAC (APAC) cognoscible respecto a $Z$ y respecto a una función de pérdida 
$l: H \times Z \rightarrow \mathbb{R}^{+}$ si existe una función 
$m_{H} : ]0,1[^2\rightarrow \mathbb{N}$ y un algoritmo 
$A: \underset{m\in \mathbb{N}}{\bigcup} Z^m \rightarrow Y^X$ verificando que si 
$0 < \varepsilon, \delta < 1$, entonces para toda distribución $\mathcal{D}$ sobre $Z$
se cumple que dado $m\ge m_{H}(\varepsilon, \delta)$:
\[\mprob \bigg[L_{\mathcal{D}}(A(S)) \le \underset{h\in H}{inf} L_{\mathcal{D}}(h) + \varepsilon \bigg] \ge 1-\delta\]
\end{definition}

Nótese que en contraste con nuestra definición de PAC cognoscibilidad, el algoritmo $A$ podía devolver una 
hipótesis que no estuviera en $H$. Cuando permitimos que el algoritmo $A$ devuelva una función 
$h \notin H$, de manera que $h \in H'$ y  $H \subset H'$ una clase de funciones donde la función de pérdida 
es extensible de manera natural, el aprendizaje recibe el nombre de \textit{aprendizaje impropio}. Si suponemos
$A(S) \in H$ estaremos usando \textit{aprendizaje propio}.

Análogamente al aprendizaje PAC, en las definiciones supondremos implícitamente que $l$ o $L_{\dist}$ proporcionan las suficientes
garantías de medibilidad.

\subsection{Particularización a distintos paradigmas}

\subsubsection{Clasificación binaria}
\label{sec:clas-binaria}

Tendremos $Y=\{0,1\}$ o $Y=\{-1,1\}$ conjunto de etiquetas, un conjunto $X$ y $Z=X\times Y$, y nuestras clases
de hipótesis $H$ estarán contenidas en $Y^X$, de manera que al aprender distribuciones sobre $Z$ podríamos 
tener etiquetas distintas para una misma instancia $x \in X$.

\begin{definition}[Función de pérdida $0-1$]
Llamamos función de pérdida $0-1$ a una función de pérdida de $H\subseteq Y^X$ sobre $Z$, $l$, verificando:
\[
  l(h, (x,y)) = \left\{\begin{array}{ll}
                        1 & \textrm{si } h(x)\neq y\\
                        0 & \textrm{en otro caso}
                        \end{array}\right.
\]
para cualquier $h\in H$, cualquier $(x,y) \in Z$
\label{def:zero-one-loss}
\end{definition}

Consideraremos la función de pérdida $0-1$ en clasificación binaria de ahora en adelante, a no ser que se especifique
lo opuesto.

Con estos conceptos revisitados, podríamos asegurar que la hipótesis que menor error comete para 
clasificación binaria y la función de pérdidad $0-1$ es el llamado \textit{clasificador de Bayes}:
\[
  f_{\mathcal{D}}(x) = \left\{\begin{array}{ll}
                              1 & P [y = 1 |x] >= 0.5\\
                              0 & \textrm{caso opuesto}
                              \end{array}\right.
\]

Pero evidentemente asumimos que el algoritmo no tiene acceso a la distribución, sino solo a los datos 
preetiquetados de $S$, y que por tanto no puede devolver el clasificador de Bayes directamente.

Particularizamos a continuación la definición de APAC anterior a clasificación binaria
con función de pérdida $0-1$ y aprendizaje impropio (ya que la función de pérdida $0-1$ está definida 
en $2^X$).

\begin{definition}[APAC cognoscible - paradigma binario]
Una clase de funciones $H \subseteq 2^X$ es APAC cognoscible sii existen 
$m_{H} : ]0,1[^2\rightarrow \mathbb{N}$ y un algoritmo $A: \underset{m\in \mathbb{N}}{\bigcup} Z^m \rightarrow 2^X$ verificando que para todo
$0 < \varepsilon, \delta < 1$ y para toda distribución $\mathcal{D}$ sobre $Z$, dado $m \ge m_H(\varepsilon, \delta)$:
\[
  \mprob \bigg[L_{\dist,f}(A(S)) \le \inf_{h\in H} L_D(h) + \varepsilon \bigg] \ge 1-\delta 
\]
\end{definition}

\subsection{Clasificación multiclase}
Dado $Y$, verificando $|Y| > 2$, $Z=X\times Y$ y $H\subseteq Y^X$, al problema lo llameremos de 
clasificación multiclase. 

Una posible función de pérdida para este problema sería también la función de pérdida $0-1$.

\subsection{Regresión}
Tenemos un problema de regresión si tomamos $Z=X \times \mathbb{R}$ y $H\subseteq \mathbb{R}^X$
y $l(h,(x,y)) = (h(x)-y)^2$ función de pérdida cuadrática.

\subsection{k-clustering}
Tenemos un problema de clústering si $Z=\mathbb{R}^d$ y los elementos de $H$ son funciones constantes 
$h = (c_1, \ldots c_k)$ con $c_i \in \mathbb{R}^d$. Una posible función de pérdida podría ser 
$l((c_1, \ldots c_k), z) = \min_{i=1,\ldots, k} \norm{c_i - z}^2$.


\section{Relación entre APAC y PAC cognoscibilidad}
Aunque pueda parecer contraintuitivo, las clases APAC cognoscibles están contenidas en las PAC cognoscibles. De hecho, 
tenemos una relación tal cual se muestra en la figura siguiente:

\imgcaption{./imgs/clases-pac.png}{Relación entre clases PAC}{0.6}

\begin{fact}
 Si $H\subseteq 2^X$ es APAC propiamente cognoscible con funciones de pérdida $0-1$, entonces $H$ es PAC cognoscible.
 \label{fact:rel-pac-apac}
\end{fact}

  \begin{proof}
   Sea $f\in H$ y una distribución $\dist=(\Sigma,P)$ sobre $X$ verificando que $H\subseteq \Sigma$. Podemos ver 
   $\dist$ como una distribución que asigna a cada $x\in X$ la etiqueta $f(x)$. Por definición $\inf_{h\in H} L_{\dist}(h) = 0$. 
   Para terminar, dado $S=(z_1, \ldots, z_m)$ con $z_i = (x_i,y_i)$:
   \[
     L_{\dist}(h) = \expect_{x\sim \dist}\bigg(l \big(h,(x,f(x)) \big)\bigg) = \prob[f \cap h] = L_{\dist,f}(h)
   \]
   donde $[f\cap h] \in \Sigma$ por hipótesis ($f,h\in H \subseteq \Sigma$).
   
   Por otro lado:
   \[
     L_{S}(h) = \frac{1}{m} \sum_{i=1}^m l(h,z_i) = \frac{|\{i\in\{1,\ldots, m\}: h(x_i) \neq f(x_i)(= y_i)\}|}{m}
   \]
   
   Además $l(h,\cdot)$ es $\Sigma$ medible ya que $l(h, \cdot)^{-1}(\{0\}) = [f = h] \in \Sigma$ y 
   $l(h, \cdot)^{-1}(\{1\}) = [f\neq h]$. Por hipótesis existiría un algoritmo $A$ y $m_H(\epsilon, \delta)$
   tal que para todo $m\ge m_H(\epsilon, \delta)$:
   \[
     \mprob \bigg[L_{\mathcal{D}}(A(S)) \le \varepsilon \bigg] \ge 1-\delta
   \]
   
   Luego $H$ sería PAC cognoscible.
  \end{proof}