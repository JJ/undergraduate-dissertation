\section{PAC cognoscibilidad}
Proporcionamos a continuación una definición que materializa el concepto de que un algoritmo pueda aprender de los datos. Las
siglas PAC derivan de Probablemente Aproximadamente Correcto.

\begin{definition} \textbf{PAC cognoscible}

Una clase de funciones $H \subseteq 2^X$ es PAC cognoscible sii existen una función 
$m_{H} : ]0,1[^2\rightarrow \mathbb{N}$, llamada complejidad muestral, y un algoritmo 
$A: \underset{m\in \mathbb{N}}{\bigcup} (X\times Y)^m \rightarrow 2^X$ verificando que para todo
$0 < \varepsilon, \delta < 1$, para toda distribución $\mathcal{D}$ sobre $X$ y para toda función de 
verdadero etiquetado $f\in H$, dados $m \ge m_H(\varepsilon, \delta)$ y $S\sim \mathcal{D}^m$:

\[\mprob \bigg[L_{\mathcal{D},f}(A(S)) \le \varepsilon \bigg] \ge 1-\delta\]

\label{def:pac}
\end{definition}

Llamamos a $(1-\delta)$ \textbf{confianza} de la predicción y a $(1-\varepsilon)$ \textbf{exactitud}. Estos dos parámetros 
explican el nombre aproximadamente ($\equiv$ confianza) correcto ($\equiv$ exactitud).

La definición de PAC cognoscibilidad nos exige que exista un algoritmo y una función a la que proporcionándole unos requisitos
de confianza y exactitud, nos indique un tamaño del conjunto de entrenamiento tal que el algoritmo pueda aproximar cualquier
distribución y etiquetado, siempre que le ofrezcamos como entrada un conjunto de entrenamiento que satisfaga los 
requisitos de tamaño.

Consideraremos $m_{H}$ única en el sentido de que para cada $(\delta, \varepsilon)$ nos devuelva el menor natural
verificando las hipótesis del enunciado.

\begin{theorem} \textbf{Las clases finitas son PAC cognoscibles}

Sea $H \subseteq 2^{X}$ finito. Entonces $H$, bajo hipótesis de factibilidad, es PAC cognoscible con:

\[m_H(\varepsilon, \delta) \le \left\lceil \frac{1}{\varepsilon}log \left(\frac{|H|}{\delta} \right) \right\rceil\]
\end{theorem}

  \begin{proof}
  Fijamos $\varepsilon \in ]0,1[$. Sea una distribución $\mathcal{D}$ sobre $X$, $m\in \mathbb{N}$ y una función de verdadero 
  etiquetado $f\in H$, tomamos la clase de hipótesis ``malas'':

  \[H_B = \{h\in H: L_{\mathcal{D},f}(h) > \varepsilon\}\]

  Sea $A$ un $ERM_{H}$, entonces:

  \[\mprob [L_{\mathcal{D},f}(A(S)) > \varepsilon] \le \mprob 
  [\exists h\in H_B : L_S(h) = 0] \le \sum_{h\in H_B} P [L_S(h) = 0] \]

  La primera desigualdad viene dada por la proposición \ref{fact:ermh}. La segunda, por subaditividad, puesto que:
  
  \[\mprob [\exists h\in H_B : L_S(h) = 0] = \mprob \left(\underset{h\in H_B}{\bigcup} [L_S(h) = 0]\right)\]

  Además, fijada $h\in H_B$, como $L_{\mathcal{D},f}(h) > \varepsilon$:

  \begin{align*}
  \mprob[L_S(h) = 0] = \mprob[h(x_i) = f(x_i), i =1,\ldots m\}] =\\
  = \prod_{i=1}^m \prob[h = f] = \prod_{i=1}^m (1 - L_{\mathcal{D},f}(h)) \le (1-\varepsilon)^m \le e^{-\varepsilon m}
  \end{align*}


  Las dos desigualdades probadas, junto a la hipótesis del enunciado, y usando $H_B \subseteq H$ dan lugar a:

  \[\mprob[L_{\mathcal{D},f}(h_S) > \varepsilon] \le |H|e^{-\varepsilon m}\]
  
  Y basta acabar haciendo encontrando $m\in \mathbb{N}$ tal que $|H|e^{-\varepsilon m} \le \delta$.
  \end{proof}

¿Hay ejemplos de clases infinitas PAC cognoscibles? La respuesta es afirmativa. Veamos un ejemplo.

\begin{example}
  \begin{definition} \textbf{Clasificadores de rectángulo}

  La clase de clasificadores de rectángulo en el plano se define por:

  \[H^2_{rec} = \{ h_{a,b,c,d}: a\le b, c\le d\}\]

  donde $h_{a,b,c,d} = \mathds{1}_{[a,b]\times [c,d]}$
  \end{definition}


  \begin{fact} \textbf{$H_{rec}^2$ es PAC cognoscible}

  Bajo hipótesis de factibilidad, los rectángulos son PAC cognoscibles
  \end{fact}

    \begin{proof}
    Haremos la prueba tratando los clasificadores como conjuntos, en virtud de la biyección canónica \eqref{sec:hip-con}
    
    Sea $A$ el algoritmo que devuelve el rectángulo más pequeño que engloba a todos los ejemplos positivos del conjunto 
    de entrenamiento $S$.

    Por hipótesis de factibilidad, $\exists R^\ast = [a,b]\times [c,d]$ tal que $L_{\mathcal{D},f}(R^\ast) = 0$, y por 
    proposición \ref{fact:ermh} se tiene $\mprob \bigg[L_S(R^\ast) = 0 \bigg] = 1$. 
    Dado por tanto $S \in (X\times Y)^m$ tal que $L_S(R^\ast) = 0$, claramente $x \in A(S)$ para 
    todo $(x,1)\in S$. Por tanto $A(S) \subseteq R^\ast$, y se deduce $x \notin A(S)$ para 
    cualquier $(x,0)\in S$. Por tanto, $A$ es un ERM.

    Fijamos $1 > \varepsilon, \delta > 0$.

    Tomamos los rectángulos:
    
    \begin{align*} 
    R_1 = [a,b^{\ast}] \times [c,d], \qquad R_2= [a^{\ast},b] \times [c,d] \\ 
    R_3=[a,b] \times [c,d^{\ast}],   \qquad R_4=[a,b] \times [c^{\ast},d]     
    \end{align*}
 
    verificándose $R_i \subseteq R^\ast$ y $L_{\mathcal{D},f}(R_i) \le \frac{\varepsilon}{4}$. Llamamos 
    $\bar{R}_i = R\setminus R_i$
    
    Supongamos s.p.g que $L_{\mathcal{D},f}(R_i) > 0$ para todo $i\in\{1,\ldots 4\}$ (caso opuesto 
    bastaría hacer la demostración para los rectángulos para los que no fuese nulo). Llamamos 
    $\bar{\varepsilon} = \max_i L_{\mathcal{D},f}(R_i)$ y $\widetilde{\varepsilon} = \min_i L_{\mathcal{D},f}(R_i)$.

    Fijado $S\in (X\times Y)^m$, $R=A(S)$, supongamos $\forall i : R \cap \bar{R_i} \neq \emptyset$. Entonces:

    \[L_{\mathcal{D},f}(R) = \prob [h_R \neq f] \le \prob 
    \left(\underset{i}{\cup} [h_R \neq f] \cap \bar{R}_i\right) \le \prob \left(\underset{i}{\cup} 
    \bar{R}_i\right) \le 4 \bar{\varepsilon}\]

    La demostración acaba probando que:

    \[\mprob [\exists i : A(S)\cap \bar{R}_i = \emptyset] \le \sum_{i=1}^4 
    \mprob[A(S)\cap \bar{R}_i = \emptyset] \le 4 \left(1-\widetilde{\varepsilon} \right)^m \le 4e^{-\widetilde{\varepsilon}m/4}\]

    y tomando $m > \frac{4}{\widetilde{\varepsilon}} log \left( \frac{4}{\delta} \right)$ llegaríamos al resultado buscado.
    \end{proof}
\end{example}


Nótese que las condiciones exigidas, cumplir la hipótesis de factibilidad y que la hipótesis devuelta deba estar en $H$, 
son muy fuertes. En contraposición, esta definición tiene la virtud de que el tamaño muestral es independiente de la 
distribución. Relajaremos esta definición con el concepto de PAC agnóstico (APAC)


\section{APAC cognoscibilidad}

Vamos a relajar tanto la definición de cognoscibilidad como las condiciones sobre las que aprender 
(factibilidad por ejemplo, pérdidas arbitrarias, etc).

Sea $Z$ un conjunto, $\mathcal{D} = (\Sigma, \mathcal{P})$ distribución sobre $Z$

\begin{definition} \textbf{Función de pérdida}
Sea $H$ un conjunto arbitrario, se denomina función de pérdida de $H$ sobre $Z$ a cualquier función de la 
forma:

\[l : H \times Z \rightarrow \mathbb{R}^{+}\]

que verifique que fijada $h\in H$ arbitrario la función $l(h, \cdot)$ sea $\Sigma$ medible.
\end{definition}

Modificamos los siguientes puntos de las definiciones básicas en \ref{sec:defs}:

\begin{itemize}
  \item \textbf{Verdadero etiquetado}: No asumimos que exista.

  \item \textbf{Generación de instancias}: Tenemos una distribución $\mathcal{D}$ sobre $Z$.
  \item \textbf{Error del clasificador}: Lo redefinimos como:
  \[L_{\mathcal{D}}(h) :=  \mathbb{E}_{z\sim \mathcal{D}}[l(h,z)]\]
  
  \item \textbf{Error empírico}: Lo redefinimos como:
  \[L_{S} (h) := \frac{1}{m} \sum_{i=1}^m l(h,z_i)\]
\end{itemize}

Estamos en condiciones de dar una definción más laxa de cognoscibilidad.

\begin{definition} \textbf{APAC cognoscible}

Una clase $H$ es agnósticamente PAC (APAC) cognoscible respecto a $Z$ y a una función de pérdida 
$l: H \times Z \rightarrow \mathbb{R}^{+}$ si existe una función 
$m_{H} : ]0,1[^2\rightarrow \mathbb{N}$ y un algoritmo 
$A: \underset{m\in \mathbb{N}}{\bigcup} Z^m \rightarrow H$ verificando que si 
$0 < \varepsilon, \delta < 1$, entonces para toda distribución $\mathcal{D}$ sobre $Z$ se verifica
que dado $S\sim \mathcal{D}^m$, con $m\ge m_{H}(\varepsilon, \delta)$ se tiene:

\[P_{S\sim \mathcal{D}^m}[L_{\mathcal{D}}(A(S)) \le \underset{h\in H}{inf} L_{\mathcal{D}}(h) + \varepsilon] \ge 1-\delta\]
\end{definition}

Nótese que en contraste con nuestra definición de PAC cognoscibilidad, el algoritmo $A$ podía devolver una 
hipótesis que no estuviera en $H$. Cuando permitimos que el algoritmo $A$ devuelva una función 
$h \notin H$, de manera que $h \in H'$ y  $H \subset H'$ una clase de funciones donde la función de pérdida 
es extensible de manera natural, el aprendizaje recibe el nombre de \textbf{aprendizaje impropio}. La definición 
anterior se ha hecho para \textbf{aprendizaje propio}.


\subsection{Particularización a distintos paradigmas}

\subsubsection{Clasificación binaria}

Tendremos $Y=\{0,1\}$ o $Y=\{-1,1\}$ conjunto de etiquetas, un conjunto $X$ y $Z=X\times Y$, y nuestras clases
de hipótesis $H$ estarán contenidas en $Y^X$, de manera que al aprender distribuciones sobre $Z$ podríamos 
tener etiquetas distintas para una misma instancia $x \in X$

\begin{definition} \textbf{Función de pérdida $0-1$}

Llamamos función de pérdida $0-1$ a $l$ función de pérdida de $H\subseteq Y^X$ sobre $Z$ verificando:
 
 \[l(h, (x,y)) = \left\{\begin{array}{ll}
                         1 & \textrm{si } h(x)\neq y\\
                         0 & \textrm{en otro caso}
                        \end{array}\right.\]

para cualquier $h\in \mathcal{H}$, cualquier $(x,y) \in Z$
\end{definition}

Consideraremos la función de pérdida $0-1$ en clasificación binaria de ahora en adelante, a no ser que se especifique
lo opuesto.

Nótese también que PAC cognoscibilidad es equivalente a APAC cognoscibilidad si asumimos hipótesis 
de factibilidad.

Con estos conceptos revisitados, podríamos asegurar que la hipótesis que menor error comete para 
clasificación binaria y la función de pérdidad $0-1$ es el llamado \textbf{clasificador de Bayes}:

\[f_{\mathcal{D}}(x) = \left\{\begin{array}{ll}
1 & P [y = 1 |x] >= 0.5\\
0 & \quad si \quad no
\end{array}\right.\]

Pero evidentemente asumimos que el algoritmo no tiene acceso a la distribución, sino solo a los datos 
preetiquetados de $S$, y que por tanto no puede devolver el clasificador de Bayes directamente.

Particularizamos a continuación la definición de APAC anterior a clasificación binaria
con función de pérdida $0-1$ y aprendizaje impropio (ya que la función de pérdida $0-1$ está definida 
en $2^X$).

\begin{definition} \textbf{APAC cognoscible - paradigma binario}

Una clase de funciones $H \subseteq 2^X$ es APAC cognoscible sii existen una función 
$m_{H} : ]0,1[^2\rightarrow \mathbb{N}$ y un algoritmo $A: \underset{m\in \mathbb{N}}{\bigcup} Z^m \rightarrow 2^X$ verificando que para todo
$0 < \varepsilon, \delta < 1$ y para toda distribución $\mathcal{D}$ sobre $Z$, dados 
$m \ge m_H(\varepsilon, \delta)$ y $S\sim \mathcal{D}^m$:

\[\mprob \bigg[L_{\mathcal{D},f}(A(S)) > \inf_{h\in H} L_D(h) + \varepsilon] < \delta\]
\end{definition}

\subsubsection{Clasificación multiclase}

Dado $Y$, verificando $|Y| > 2$, $Z=X\times Y$ y $H\subseteq Y^X$, al problema lo llameremos de 
clasificación multiclase. 

Una posible función de pérdida para este problema sería también la función de pérdida $0-1$:

\subsection{Regresión}

Tenemos un problema de regresión si tomamos $Z=X \times \mathbb{R}$ y $H\subseteq \mathbb{R}^X$
y $l(h,(x,y)) = (h(x)-y)^2$ función de pérdida cuadrática.

\subsection{k-clustering}

Tenemos un problema de clústering si $Z=\mathbb{R}^d$ y los elementos de $H$ son funciones constantes 
$h = (c_1, \ldots c_k)$ con $c_i \in \mathbb{R}^d$. Una posible función de pérdida podría ser 
$l((c_1, \ldots c_k), z) = \min_{i} \norm{c_i}^2$.
