\section{PAC cognoscibilidad}
Proporcionamos a continuación una definición que materializa el concepto de que un algoritmo pueda aprender de los datos. Las
siglas PAC derivan de Probablemente Aproximadamente Correcto.

\begin{definition*} \textbf{PAC cognoscible}

Una clase de funciones $H \subseteq 2^X$ es PAC cognoscible sii existen una función 
$m_{H} : ]0,1[^2\rightarrow \mathbb{N}$, llamada complejidad muestral, y un algoritmo $A$ verificando que para todo
$0 < \epsilon, \delta < 1$, para toda distribución $\mathcal{D}$ sobre $X$ y para toda función de 
verdadero etiquetado $f\in H$, dados $m \ge m_H(\epsilon, \delta)$ y $S\sim \mathcal{D}^m$:

\[P_{S\sim \mathcal{D}^m} \bigg[L_{\mathcal{D},f}(A(S)) \le \epsilon \bigg] \ge 1-\delta\]
\end{definition*}

Llamamos a $(1-\delta)$ \textbf{confianza} de la predicción y a $(1-\epsilon)$ \textbf{exactitud}. Estos dos parámetros 
explican el nombre aproximadamente ($\equiv$ confianza) correcto ($\equiv$ exactitud).

La definición de PAC cognoscibilidad nos exige que exista un algoritmo y una función a la que proporcionándole unos requisitos
de confianza y exactitud, nos indique un tamaño del conjunto de entrenamiento tal que el algoritmo pueda aproximar cualquier
distribución y etiquetado, siempre que le ofrezcamos como entrada un conjunto de entrenamiento que satisfaga los 
requisitos de tamaño.

Consideraremos $m_{H}$ única en el sentido de que para cada $(\delta, \epsilon)$ nos devuelva el menor natural
verificando las hipótesis del enunciado.

Nótese que las condiciones exigidas, cumplir la hipótesis de factibilidad y que la hipótesis devuelta deba estar en $H$, 
son muy fuertes. En contraposición, esta definición tiene la virtud de que el tamaño muestral es independiente de la 
distribución. Relajaremos esta definición más adelante con el concepto de PAC agnóstico.

\begin{theorem*} \textbf{Las clases finitas son PAC cognoscibles}

Sea $H \subseteq 2^{X}$ finito. Entonces $H$, bajo hipótesis de factibilidad, es PAC cognoscible con:

\[m_H(\epsilon, \delta) \le \left\lceil \frac{1}{\epsilon}log \left(\frac{|H|}{\delta} \right) \right\rceil\]
\end{theorem*}

  \begin{proof}
  Fijamos $\epsilon \in ]0,1[$. Sea una distribución $\mathcal{D}$ sobre $X$, $m\in \mathbb{N}$ y una función de verdadero 
  etiquetado $f\in H$, tomamos la clase de hipótesis ``malas'':

  \[H_B = \{h\in H: L_{\mathcal{D},f}(h) > \epsilon\}\]

  Sea $A$ un $ERM_{\mathcal{H}}$, entonces:

  \[\underset{S\sim \mathcal{D}^m}{P} [L_{\mathcal{D},f}(A(S)) > \epsilon] \le \underset{S\sim \mathcal{D}^m}{P} 
  [\exists h\in H_B : L_S(h) = 0] \le \sum_{h\in H_B} P [L_S(h) = 0] \]

  La primera desigualdad viene dada por la proposición \ref{fact:ermh}. La segunda, por subaditividad, puesto que:
  
  \[\underset{S\sim \mathcal{D}^m}{P} [\exists h\in H_B : L_S(h) = 0] = \underset{S\sim \mathcal{D}^m}{P} \left(\underset{h\in H_B}{\bigcup} [L_S(h) = 0]\right)\]

  Además, fijada $h\in H_B$, como $L_{\mathcal{D},f}(h) > \epsilon$:

  \begin{align*}
  \underset{S\sim \mathcal{D}^m}{P}[L_S(h) = 0] = \underset{S\sim \mathcal{D}^m}{P}[h(x_i) = f(x_i), i =1,\ldots m\}] =\\
  = \prod_{i=1}^m \underset{x\sim \mathcal{D}}{P}[h = f] = \prod_{i=1}^m (1 - L_{\mathcal{D},f}(h)) \le (1-\epsilon)^m \le e^{-\epsilon m}
  \end{align*}


  Las dos desigualdades probadas, junto a la hipótesis del enunciado, y usando $H_B \subseteq H$ dan lugar a:

  \[P_{S\sim \mathcal{D}^m}[L_{\mathcal{D},f}(h_S) > \epsilon] \le |H|e^{-\epsilon m}\]
  
  Y basta acabar haciendo encontrando $m\in \mathbb{N}$ tal que $|H|e^{-\epsilon m} \le \delta$.
  \end{proof}

¿Hay ejemplos de clases infinitas PAC cognoscibles? La respuesta es afirmativa. Veamos un ejemplo.

\begin{example}
  \begin{definition*} \textbf{Clasificadores de rectángulo}

  La clase de clasificadores de rectángulo en el plano se define por:

  \[H^2_{rec} = \{ h_{a,b,c,d}: a\le b, c\le d\}\]

  donde $h_{a,b,c,d} = \mathds{1}_{[a,b]\times [c,d]}$
  \end{definition*}


  \begin{fact} \textbf{$H_{rec}^2$ es PAC cognoscible}

  Bajo hipótesis de factibilidad, los rectángulos son PAC cognoscibles
  \end{fact}

    \begin{proof}
    Sea $A$ el algoritmo que devuelve el rectángulo más pequeño que engloba a todos los ejemplos positivos del conjunto 
    de entrenamiento $S$. Notamos a dicho rectángulo $R(S)$, $A(S) = \mathds{1}_{R(S)}$.

    Por hipótesis de factibilidad, $\exists R^{\ast} = [a,b]\times [c,d]$ tal que $\bar{h} = \mathds{1}_{R^{\ast}}$ cumple 
    $L_{\mathcal{D},f}(\bar{h}) = 0$, y por proposición \ref{fact:ermh} se tiene 
    $\underset{S\sim \mathcal{D}^m}{P} \bigg[L_S(\bar{h}) = 0 \bigg] = 1$. 
    Dado por tanto $S \in (X\times Y)^m$ tal que $L_S(\bar{h}) = 0$, claramente $\bar{h}(x,1)= 1 = A(S)(x,1)$ para 
    todo $(x,1)\in S$. Por tanto $R(S) \subseteq R^{\ast}$ c.p.d., y se deduce $\bar{h}(x,0)= 0 = A(S)(x,0)$ para 
    todo $(x,0)\in S$. $A$ es por tanto un ERM.

    Fijamos $1 > \epsilon, \delta > 0$.

    Tomamos $R_1 = [a,b^{\ast}] \times [c,d]$ un rectángulo verificando $L_{\mathcal{D},f}(\mathds{1}_{R_1}) = \epsilon/4$, 
    con $a\le b^{\ast} \le b$.

    $R_2= [a^{\ast},b] \times [c,d], R_3=[a,b] \times [c,d^{\ast}], R_4=[a,b] \times [c^{\ast},d]$ se definen 
    de forma análoga, y notamos $\bar{R_i} = R^{\ast} \setminus R_i$

    Fijado $R=R(S)$, supongamos $\forall i : R \cap \bar{R_i} \neq \emptyset$. Entonces:

    \[L_{\mathcal{D},f}(h_R) = \underset{x\sim \mathcal{D}}{P} [h_R \neq f] \le \underset{x\sim \mathcal{D}}{P} 
    \left(\underset{i}{\cup} [h_R \neq f] \cap \bar{R}_i\right) \le \underset{x\sim \mathcal{D}}{P} \left(\underset{i}{\cup} 
    \bar{R}_i\right) \le 4\frac{\epsilon}{4}\]

    La demostración acaba probando que:

    \[\underset{S\sim \mathcal{D}^m}{P} [\exists i : R(S)\cap \bar{R}_i = \emptyset] \le \sum_{i=1}^4 
    \underset{S\sim \mathcal{D}^m}{P}[R(S)\cap \bar{R}_i = \emptyset] = 4(1-\frac{\epsilon}{4})^m \le 4e^{-\epsilon m/4}\]

    y tomando $m > \frac{4}{\epsilon} log \left( \frac{4}{\delta} \right)$ llegamos al resultado buscado.
    \end{proof}
\end{example}

