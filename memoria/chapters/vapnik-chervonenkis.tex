\section{Conjunto fragmentado}
La abreviaremos dimensión VC

\begin{definition} \textbf{Conjunto fragmentado}
 Sea $H\subseteq 2^X$ y sea $A\subseteq X$. Decimos que $H$ fragmenta a $A$ si para toda función $g:A\rightarrow \{0,1\}$ 
 existe $h\in H$ verificando $h_{|A} = g$.
\end{definition}

Recordando la biyección canónica entre hipótesis y conjuntos \ref{biyeccion-canonica}, podemos aportar la siguiente 
caracterización de fragmentación.

\begin{fact} \textbf{Caracterización de conjunto fragmentado}
 Sea $H\subseteq 2^X$ y sea $A\subseteq X$. Decimos que $H$ fragmenta a $A$ si para toda $B\subseteq A$, se tiene que existe
 $h\in H$ verificando $X_h \cap A = B$.
\end{fact}

  \begin{proof}
  Trivial desde \ref{biyeccion-canonica}
  \end{proof}


\begin{definition}
Sea $H\subseteq 2^X$, y $C \subseteq X$. Llamamos restricción de $H$ a $C$ a:

\[H_{C} := \{h_{|C} : h\in H\}\]
\end{definition}


Cuando demostrábamos el teorema No Free Lunch \ref{th:nfl}, pedíamos que la clase de clasificadores $H$ fuese todo $2^X$. 
Observando la demostración, solo asumiamos la existencia de un conjunto $C\subseteq X$ sobre el que pudiéramos definir 
todas las posibles hipótesis $C \rightarrow \{0,1\}$. Revisitamos por tanto el enunciado de dicho teorema.

\begin{theorem}
\textbf{Teorema de No Free Lunch revisitado}

Sea un dominio $X$, un conjunto de etiquetas $Y=\{0,1\}$, $H \subseteq 2^X$.
Sea $A: \underset{m\in \mathbb{N}}{\bigcup} (X\times Y)^m \rightarrow 2^X$ un algoritmo. Dado $m \le \frac{|X|}{2}$,
supongamos que existe $C\subseteq X$ de tamaño $2m$ fragmentado por $H$. Entonces existe $\dist$ distribución sobre 
$X\times \{0,1\}$ verificando:

\begin{enumerate}[i]
\item Existe una función $f: X \rightarrow \{0,1\}$ con $L_{\dist}(f)=0$
\item $\mprob \left[ L_{\dist}(A(S)) \ge \frac{1}{8} \right] \ge \frac{1}{7}$
\end{enumerate}

\label{th:nfl2}
\end{theorem}


\section{Dimensión VC}

\begin{definition}
\textbf{Dimensión VC}

Sea $H \subseteq 2^X$. Definimos su dimensión Vapnik-Chervonenkis, que abreviaremos dimensión VC, como:

\[VC(H) := \max \{|A| : A\subseteq X, A \textrm{ es fragmentado por } H\}\]
\end{definition}


\begin{example}

Sea $H_{thr} = \{h_a = \mathds{1}_{[-\infty, a]}: a\in \mathbb{R}\}$ clase de hipótesis sobre $\mathbb{R}$. 

Dado un conjunto $C=\{\alpha\}$, podemos tomar $h_{\alpha}$ y $h_{\alpha+1}$, que nos dan todos los posibles etiquetados de
$C$. Sin embargo, dado un conjunto de tamaño 2, $C=\{\alpha, \beta\}$, donde podemos suponer spg. $\alpha < \beta$. 
Entonces no podemos encontrar $h_b \in H$ verificando $h_b(\alpha)=0$ y $h_b(\beta) = 1$, ya que esto implicaría que 
$b > \beta$ y por tanto entraría en contradicción con que $h_b(\alpha) = 0$.

Hemos probado $VC(H) = 1$.

\end{example}

\begin{example}

Sea $H^1_{rec} = \{h_{a,b} = \mathds{1}_{[a,b]}: a,b\in \mathbb{R}\}$ clase de hipótesis sobre $\mathbb{R}$. 

Dado un conjunto $C=\{\alpha\, \beta\}$, con $\alpha < \beta$, las hipótesis $h_{\alpha+\delta_1, \beta + \delta_2}$ con 
$\delta_i \in \{0,1\}$ nos dan todos los posibles etiquetados de $C$. Sin embargo, dado un conjunto de tamaño 3, 
$C=\{\alpha, \beta\, \theta\}$, donde podemos suponer $\alpha < \beta < \theta$. Entonces no podemos encontrar $h_b \in H$ 
verificando $h_b(\alpha)=1$ y $h_b(\theta) = 1$ y $h_b(\beta) = 0$.

Por tanto $VC(H) = 2$.
\end{example}

\begin{example}

Sea $H^2_{rec} = \{h_{a,b,c,d} = \mathds{1}_{[a,b]\times [c,d]}: a,b,c,d\in \mathbb{R}\}$ clase de hipótesis sobre 
$\mathbb{R}^2$.

Podemos fragmentar el conjunto $C = \{(0,1), (0,-1), (1,0), (-1,0)\}$

\img{./imgs/rect-vc.png}{0.7}

Luego $VC(H^2_{rec}) \ge 4$. Veamos que $VC(H^2_{rec}) = 4$.

Sea $C\subseteq \mathbb{R}^2$ con $|C|=5$. Entonces podemos 
tomar $x_1 \in C$ punto más a la izquierda de $C$ (primera coordenada más pequeña), $x_2 \in C$ punto más a la derecha de 
$C$, $x_3 \in C$ punto más arriba en $C$ (segunda coordenada más grande), $x_4 \in C$ punto más abajo en $C$. Se podría 
tener $x_i = x_j$ para algún $i\neq j$. Si tenemos un rectángulo que contiene a dichos puntos, debe necesariamente contenerlos
a todos.
\end{example}



\begin{fact} \textbf{Propiedades de la dimensión VC}

Sean $H,H_1, H_2 \subseteq 2^X$. Se cumple:

\begin{enumerate}[i]
 \item $VC(H) \le log_2(|H|)$.
 \item Si $H_1 \subseteq H_2$ entonces $VC(H_1) \le VC(H_2)$.
\end{enumerate}
\end{fact}

\begin{proof} \\~
 \begin{enumerate}[i]
  \item Equivalentemente, veamos $2^{VC(H)} \le |H|$. Si $VC(H) < \infty$, entonces $H$ fragmenta un conjunto de tamaño
  $VC(H)$, a saber, $A$, y por tanto deben existir tantos elementos en $H$ al menos como subconjuntos de $A$. El número de
  subconjuntos de $A$ viene dado por $2^{|A|} = 2^{VC(H)}$. 
  
  Si $VC(H) = \infty$, entonces $|H| = \infty$, ya que para cada 
  $n\in \mathbb{N}$ podríamos fragmentar con $H$ un conjunto de tamaño $n$, y por tanto $H$ debería tener al menos $2^n$ 
  elementos.
  \item Si $H_1$ fragmenta un conjunto de tamaño $VC(H_1)$, entonces $H_2$ también fragmenta ese mismo conjunto puesto que
  $H_1 \subseteq H_2$. Por tanto $VC(H_1) \le VC(H_2)$.
 \end{enumerate}
\end{proof}


\subsubsection{Dimensión VC y número de parámetros}

Puede demostrarse que la dimensión VC de los clasificadores de rectángulo $H = \{h_{a,b,c,d} := \mathds{1}_{[a\le x\le b, c\le y\le d]}\}$ en $\mathbb{R}^2$, que ya mencionamos en un ejemplo en los temas anteriores es 4. Esto, unido a los ejemplos anteriores con intervalos podría hacernos conjeturar que la dimensión VC depende del número de parámetros con el que definimos los clasificadores. El siguiente ejemplo demuestra que esto es falso.

Dada la clase de clasificadores $H = \{h_{\theta}: \theta \in \mathbb{R}\}$ donde 
$h_{\theta}: X \rightarrow {0,1}$ está definida por $h_\theta (x) = \lceil 0.5 sen(\theta x) \rceil$, y 
$d\in \mathbb{N}$ arbitrario, podemos tomar $d$ puntos codificados por $x_i = 0.x_{1,j} \ldots x_{2^d,j} x^{2^d + 1,j}$ 
donde cada $x_{i}$ es una fila de la matriz $(x_{i,j})$ donde la columna $2^{d+1}$ ésima es 1, y la columna 
$i$ -ésima codifica el número $i-1$ en binario, leído de arriba a abajo. Así dado una asignación de $d$ 
etiquetas, debe codificar un número en binario entre $1$ y $2^d-1$, a saber, la columna $k$ ésima de la 
matriz. Tomamos el clasificador $h = \lceil 0.5 sen(10^k \pi x) \rceil$ que verificará que su asignación de 
etiquetas es justamente la columna $k$ ésima. El sentido de la última columna constantemente $1$ puede 
explicarse en que daremos $x_{i,1} \ldots x_{i,k}$ medias vueltas a la circunferencia unidad y recorreremos 
y una fracción $0.x_{i,k+1} \ldots x_{2^d,j} 1$ no nula de otra media vuelta a la circunferencia. Si 
$x_{i,k} = 1$ entonces $h(x_i) = 1$, y si $x_{i,k}=0$ entonces $h(x_i) = 0$. Luego $VCdim(H) = \infty$.

\subsection{Teorema fundamental de aprendizaje PAC}

\begin{theorem}
\textbf{Teorema fundamental de aprendizaje PAC}

Sea $H$ clase de hipótesis de la forma $h: X \rightarrow \{0,1\}$ y la función de pérdida 0-1. Entonces equivalen:

\begin{enumerate}
\item $H$ tiene la propiedad de convergencia uniforme.
\item $H$ es agnósticamente PAC cognoscible por cuaquier algoritmo ERM.
\item $H$ es agnósticamente PAC cognoscible.
\item $H$ es PAC cognoscible.
\item $H$ es PAC cognoscible por cualquier algoritmo ERM.
\item $VC (H) < \infty$.
\end{enumerate}
\end{theorem}


La implicación que nos falta es $6 \implies 1$. El resto de implicaciones se consiguen a partir de teoremas ya probados en temas anteriores.

Daremos una serie de lemas y definiciones previas antes de probarla.

\begin{definition}
\textbf{Función de crecimiento}

Sea $H$ una clase de hipótesis. Definimos como función de crecimiento de $H$:

\[\begin{array}{ll}
\tau_{H}: & \mathbb{N} \rightarrow \mathbb{N}\\
                    & m          \mapsto     \underset{C \subseteq X: |C|=m}{max}{|H_C|}
\end{array}\]

Esta función está bien definida puesto que fijado $m \in \mathbb{N}$ se tiene siempre que $|H_C| \le 2^C$
\end{definition}

\begin{lemma}
\textbf{Lema de Sauer-Shelah}

Sea $H$ clase de hipótesis con $VC(H) \le d < \infty$. Entonces para todo $m\in \mathbb{N}$ se tiene $\tau_{H} (m) \le \sum_{i=0}^d \binom{m}{i}$. Se deduce que si $m > d+1$ entonces $\tau_{H}(m) \le (em/d)^d$.
\end{lemma}


\begin{proof}
Empezamos probando que una clase de hipótesis $\mathcal{F}$ finita fragmenta al menos $|\mathcal{F}|$ conjuntos.

Lo hacemos por inducción sobre el tamaño de $X_\mathcal{F}$ que es un conjunto finito por ser $\mathcal{F}$ clase finita.

Si su tamaño es 1, parte al conjunto vacío.

Supuesto que se verifica la hipótesis para tamaños menores que $k-1$ y sea $|X_\mathcal{F}| = k$. Escogemos entonces $x\in X$ verificando que $x$ pertenece a algunos conjuntos de $X_\mathcal{F}$ pero no a todos (debe existir, sino tendríamos que $X_\mathcal{F}$ contiene un único conjunto).

Sean $A = \{S \subseteq X_\mathcal{F} : x\in S\}$, $A'=\{S\setminus\{x\} : S \in A\}$, $B= \{S \subseteq X_\mathcal{F} : x\not\in S\}$.

Claramente $|A| = |A'|$ y por hipótesis de inducción $A'$ fragmenta $k \ge |A|$ conjuntos, y $B$ fragmenta $|B|$ conjuntos. También es trivial ver que los conjuntos $S$ y $S\cup \{x\}$ son fragmentados por $A$ donde $S$ es un conjunto fragmentado por $A'$. Hemos probado que $A$ fragmenta $|A| + k$ conjuntos, y si un conjunto es fragmentado por $A$ y $B$ a la vez, entoces no contiene a $x$, luego es fragmentado por $A'$ y por $B$ a la vez, y podremos tener a lo sumo $k$ conjuntos de este tipo.

En definitiva hemos probado que fragmentamos $|A| + k + |B| - k = |A| + |B| = |\mathcal{F}|$ conjuntos.
Probado esto, si $\tau_{H}(m) > \sum_{i=0}^d \binom{m}{i}$ entonces $H$ debe fragmentar un conjunto de tamaño 
$d+1$ al menos, puesto que el número de subconjuntos de un conjunto finito $C$ menor que  $d+1$ es exactamente
$\sum_{i=0}^d \binom{|C|}{i}$. Luego tendríamos $VC(H) > d$
\end{proof}