\section{Conjunto fragmentado}

\begin{definition}
Sea $H\subseteq 2^X$, y $C \subseteq X$. Llamamos restricción de $H$ a $C$ a:

\[H_{C} := \{h_{|C} : h\in H\}\]

En notación conjuntista, existe biyección entre $H_C$ y $C_{H}$ con:

\[C_{H} := \{C_h : h\in H\} \subseteq \mathcal{P}(X)\]
\end{definition}


\begin{definition} \textbf{Conjunto fragmentado}

 Sea $H\subseteq 2^X$ y sea $A\subseteq X$. Decimos que $H$ fragmenta a $A$ si para toda función $g:A\rightarrow \{0,1\}$ 
 existe $h\in H$ verificando $h_{|A} = g$.
\end{definition}

Recordando la biyección canónica entre hipótesis y conjuntos \ref{biyeccion-canonica}, podemos aportar la siguiente 
caracterización de fragmentación.

\begin{fact} \textbf{Caracterizaciones de conjunto fragmentado}

 Sea $H\subseteq 2^X$ y sea $A\subseteq X$ finito. Entonces equivalen:
 
 \begin{enumerate}[i]
  \item $H$ fragmenta a $A$.
  \item Para todo $B\subseteq A$, existe $h\in H$ verificando $X_h \cap A = B$
  \item $A_H = \mathcal{P}(A)$.
  \item $|A_H| = 2^{|A|}$
  \item $|H_A| = 2^{|A|}$
 \end{enumerate}
\end{fact}

  \begin{proof}
  Trivial desde \ref{biyeccion-canonica}
  \end{proof}


Cuando demostrábamos el teorema No Free Lunch \ref{th:nfl}, pedíamos que la clase de clasificadores $H$ fuese todo $2^X$. 
Observando la demostración, solo asumiamos la existencia de un conjunto $C\subseteq X$ sobre el que pudiéramos definir 
todas las posibles hipótesis $C \rightarrow \{0,1\}$. Revisitamos por tanto el enunciado de dicho teorema.

\begin{theorem}
\textbf{Teorema de No Free Lunch revisitado}

Sea un dominio $X$, un conjunto de etiquetas $Y=\{0,1\}$, $H \subseteq 2^X$.
Sea $A: \underset{m\in \mathbb{N}}{\bigcup} (X\times Y)^m \rightarrow 2^X$ un algoritmo. Dado $m \le \frac{|X|}{2}$,
supongamos que existe $C\subseteq X$ de tamaño $2m$ fragmentado por $H$. Entonces existe $\dist$ distribución sobre 
$X\times \{0,1\}$ verificando:

\begin{enumerate}[i]
\item Existe una función $f: X \rightarrow \{0,1\}$ con $L_{\dist}(f)=0$
\item $\mprob \left[ L_{\dist}(A(S)) \ge \frac{1}{8} \right] \ge \frac{1}{7}$
\end{enumerate}

\label{th:nfl2}
\end{theorem}


\section{Dimensión VC}

\begin{definition}
\textbf{Dimensión VC}

Sea $H \subseteq 2^X$. Definimos su dimensión Vapnik-Chervonenkis, que abreviaremos dimensión VC, como:

\[VC(H) := \max \{|A| : A\subseteq X, A \textrm{ es fragmentado por } H\}\]
\end{definition}


\begin{example}

Sea $H_{thr} = \{h_a = \mathds{1}_{[-\infty, a]}: a\in \mathbb{R}\}$ clase de hipótesis sobre $\mathbb{R}$. 

Dado un conjunto $C=\{\alpha\}$, podemos tomar $h_{\alpha}$ y $h_{\alpha+1}$, que nos dan todos los posibles etiquetados de
$C$. Sin embargo, dado un conjunto de tamaño 2, $C=\{\alpha, \beta\}$, donde podemos suponer spg. $\alpha < \beta$. 
Entonces no podemos encontrar $h_b \in H$ verificando $h_b(\alpha)=0$ y $h_b(\beta) = 1$, ya que esto implicaría que 
$b > \beta$ y por tanto entraría en contradicción con que $h_b(\alpha) = 0$.

Hemos probado $VC(H_{thr}) = 1$.

\end{example}

\begin{example}

Sea $H^1_{rec} = \{h_{a,b} = \mathds{1}_{[a,b]}: a,b\in \mathbb{R}\}$ clase de hipótesis sobre $\mathbb{R}$. 

Dado un conjunto $C=\{\alpha\, \beta\}$, con $\alpha < \beta$, las hipótesis $h_{\alpha+\delta_1, \beta + \delta_2}$ con 
$\delta_i \in \{0,1\}$ nos dan todos los posibles etiquetados de $C$. Sin embargo, dado un conjunto de tamaño 3, 
$C=\{\alpha, \beta\, \theta\}$, donde podemos suponer $\alpha < \beta < \theta$. Entonces no podemos encontrar $h_b \in H$ 
verificando $h_b(\alpha)=1$ y $h_b(\theta) = 1$ y $h_b(\beta) = 0$.

Por tanto $VC(H^1_{rec}) = 2$.
\end{example}

\begin{example}

Sea $H^2_{rec} = \{h_{a,b,c,d} = \mathds{1}_{[a,b]\times [c,d]}: a,b,c,d\in \mathbb{R}\}$ clase de hipótesis sobre 
$\mathbb{R}^2$.

Podemos fragmentar el conjunto $C = \{(0,1), (0,-1), (1,0), (-1,0)\}$

\img{./imgs/rect-vc.png}{0.7}

Luego $VC(H^2_{rec}) \ge 4$. Veamos que $VC(H^2_{rec}) = 4$.

Sea $C\subseteq \mathbb{R}^2$ con $|C|=5$. Entonces podemos 
tomar $x_1 \in C$ punto más a la izquierda de $C$ (primera coordenada más pequeña), $x_2 \in C$ punto más a la derecha de 
$C$, $x_3 \in C$ punto más arriba en $C$ (segunda coordenada más grande), $x_4 \in C$ punto más abajo en $C$. Se podría 
tener $x_i = x_j$ para algún $i\neq j$. Si tenemos un rectángulo que contiene a dichos puntos, debe necesariamente contenerlos
a todos.
\end{example}



\begin{fact} \textbf{Propiedades de la dimensión VC}

Sean $H,H_1, H_2 \subseteq 2^X$. Se cumple:

\begin{enumerate}[i]
 \item $VC(H) \le log_2(|H|)$.
 \item Si $H_1 \subseteq H_2$ entonces $VC(H_1) \le VC(H_2)$.
 \item $VC(H)$ no depende del número de parámetros de los clasificadores de $H$.
\end{enumerate}
\label{fact:props-vc}
\end{fact}

\begin{proof}~
 \begin{enumerate}[i]
  \item Equivalentemente, veamos $2^{VC(H)} \le |H|$. 
  
  Si $VC(H) < \infty$, entonces $H$ fragmenta un conjunto de tamaño
  $VC(H)$, a saber, $A$, y por tanto deben existir tantos elementos en $H$ al menos como subconjuntos de $A$. El número de
  subconjuntos de $A$ viene dado por $2^{|A|} = 2^{VC(H)}$. 
  
  Si $VC(H) = \infty$, entonces $|H| = \infty$, ya que para cada 
  $n\in \mathbb{N}$ podríamos fragmentar con $H$ un conjunto de tamaño $n$, y por tanto $H$ debería tener al menos $2^n$ 
  elementos.
  
  \item Si $H_1$ fragmenta un conjunto de tamaño $VC(H_1)$, entonces $H_2$ también fragmenta ese mismo conjunto puesto que
  $H_1 \subseteq H_2$. Por tanto $VC(H_1) \le VC(H_2)$.
 
  \item En los ejemplos anteriores hemos visto como $VC(H)$ coincidía con el número de parámetros de los que dependían los
  clasificadores de $H$: $VC(H_{thr}) = 1$, $VC(H^1_{rec}) = 2$, $VC(H^2_{rec}) = 4$. Veamos un ejemplo donde no se cumple
  esto.
  
  Sea $H_{sin} = \{h_{\theta}: \theta \in \mathbb{R}\}$ donde $h_{\theta}: \mathbb{R} \rightarrow \{0,1\}$ está definida por 
  $h_\theta (x) = \lceil -0.5sen(\theta x) \rceil$. Cada $h \in H_{sin}$ depende de un único parámetro $\theta$.
  
  Sea $d\in \mathbb{N}$. Tomamos $d$ puntos codificados por $x_i = 0.x_{i,1} x_{i,2} \ldots x_{i, 2^d} x_{i, 2^d+1}$ 
  donde cada $x_{i}, i=1, \ldots d$ viene determinado por una fila de la matriz $\bigg(x_{i,j}\bigg)$
  donde la columna $j$-ésima codifica el número $j-1$ en binario leído de arriba a abajo, para $j\neq 2^d+1$, y la columna 
  $2^d+1$-ésima es constantemente $1$.
  
% 
%   \[\left(\begin{array}{ccccc}
%     \underbrace{\begin{array}{c} 0 \\ 0 \\ \vdots \\0 \end{array}}_{\phantom{0}} &
%     \underbrace{\begin{array}{c} 0 \\ 0 \\ \vdots \\1 \end{array}}_{1} &
%     \underbrace{\begin{array}{c} \ldots \\ \ldots \\ \ddots \\ \ldots \end{array}}_{1} &
%     \underbrace{\begin{array}{c} 1 \\ 1 \\ \vdots \\1 \end{array}}_{2^d-1} &
%     \underbrace{\begin{array}{c} 1 \\ 1 \\ \vdots \\1 \end{array}}
%   \end{array}\right)\]

    
  \[d \left\{\left( 
    \begin{array}{lcccccccccccc}
    0 &&& 0 &&& \ldots &&& 1 &&& 1\\
    0 &&& 0 &&& \ldots &&& 1 &&& 1\\
    \vdots &&& \vdots &&& \ddots &&& \vdots &&& \vdots\\
    \undermat{0}{0} &&& \undermat{1}{1} &&& \ldots &&& \undermat{2^d-1}{1} &&& 1
    \end{array}
  \right)\right.\]
  
  \bigskip
  
  
  Así, dada una asignación de $d$ etiquetas, debe codificar un número en binario entre $0$ y $2^d-1$, a saber, 
  la columna $k$ ésima de la matriz. Tomamos el clasificador $h(x) = \lceil -0.5 sen(10^k \pi x) \rceil$ que verificará
  que su asignación de etiquetas es justamente la columna $k$ ésima. 
  
  El sentido de la última columna constantemente $1$
  puede explicarse en que daremos $x_{i,1} \ldots x_{i,k}$ medias vueltas a la circunferencia unidad y recorreremos
  una fracción $0.x_{i,k+1} \ldots x_{i, 2^d} 1$ no nula de otra media vuelta a la circunferencia. Si 
  $x_{i,k} = 1$ entonces $h(x_i) = 1$, y si $x_{i,k}=0$ entonces $h(x_i) = 0$. 
  
  Como $d\in \mathbb{N}$ era arbitrario, $VC(H_{sin}) = \infty$, y por tanto hemos encontrado una clase de hipótesis 
  uniparámetrica con dimensión $VC(H_{sin})\neq 1$.
 \end{enumerate}
\end{proof}


\begin{corollary}
 Sea $H \subseteq 2^X$. Si $VC(H) = \infty$ entonces $H$ no es PAC cognoscible.
 \label{cor:vc-finito}
\end{corollary}

  \begin{proof}
  Si $X < \infty$, entonces $|H| \le 2^{|X|} < \infty$, y $VC(H) \le log_2(|H|) < \infty$. Luego $|X| = \infty$
  
  Por reducción al absurdo. Sea $A$ el algoritmo de aprendizaje. Para todo $m\in \mathbb{N}$ par arbitrario, como 
  $m\le \frac{|X|}{2}$, existe $C$ con $|C| = m$ fragmentado por $H$, el teorema \ref{th:nfl2} nos diría que existe una 
  distribución $\dist$ verificando:
 
  \[\mprob \left[ L_{\dist}(A(S)) \ge \frac{1}{8} \right] \ge \frac{1}{7}\]
 
  Luego no puede existir $m_H \left(\frac{1}{8}, \delta\right)$ con $\delta > \frac{6}{7}$.
  \end{proof}
  
\subsection{Teorema fundamental del aprendizaje automático}

\begin{definition}
\textbf{Función de crecimiento}

Sea $H$ una clase de hipótesis. Definimos como función de crecimiento de $H$:

\[\Pi_{H}: \mathbb{N} \longrightarrow \mathbb{N}, \qquad \Pi_{H}(m) = \max_{C \subseteq X, \,\, |C|=m} |H_C|\]
\end{definition}

Esta función está bien definida puesto que fijado $m \in \mathbb{N}$ y $C\subseteq X$ con $|C| = m$, se tiene siempre 
que $|H_C| \le 2^C$, identificando elementos de $H_C$ con subconjuntos de $C$.

\begin{lemma} \textbf{Lema de Sauer-Shelah-Perles}

Sea $H \subseteq 2^X$ con $VC(H) \le d < \infty$. Entonces para $m\in \mathbb{N}$ verificando $d\le m$:

\[\Pi_{H} (m) \le \sum_{i=0}^d \binom{m}{i} \le m^d + 1\]
\label{lemma:sauer}
\end{lemma}


\begin{proof}
Supongamos s.p.g. $VC(H) = d$.

Empezamos probando que una clase $\mathcal{F}$, con $|\mathcal{F}| < \infty$, fragmenta al menos $|\mathcal{F}|$ conjuntos. 
Lo hacemos por inducción sobre el tamaño de $X_\mathcal{F}$ que es un conjunto finito por ser $\mathcal{F}$ clase finita.

\begin{subenv}
Si su tamaño es 1, parte al conjunto vacío.

Supuesto que se verifica la hipótesis para tamaños menores que $k-1$ y sea $|X_\mathcal{F}| = k$. Escogemos entonces 
$x\in X$ verificando que $x$ pertenece a algunos conjuntos de $X_\mathcal{F}$ pero no a todos (debe existir, sino 
tendríamos que $X_\mathcal{F}$ contiene un único conjunto).

Definimos: $\begin{array}{l} 
	A = \{S \subseteq X_\mathcal{F} : x\in S\} \\
        A'=\{S\setminus\{x\} : S \in A\} \\ 
        B= \{S \subseteq X_\mathcal{F} : x\not\in S\}
     \end{array}$ donde $X_{\mathcal{F}} = A + B$.

Claramente $|A| = |A'|$ y por hipótesis de inducción $A'$ fragmenta $k \ge |A|$ conjuntos, y $B$ fragmenta $|B|$ conjuntos. 
También es trivial ver que los conjuntos $S$ y $S\cup \{x\}$ son fragmentados por $A$ donde $S$ es un conjunto fragmentado 
por $A'$. 

Hemos probado que $A$ fragmenta $|A'| + k = |A|+k$ conjuntos, y si un conjunto es fragmentado por $A$ y $B$ a la vez, 
entoces no contiene a $x$, luego es fragmentado por $A'$ y por $B$ a la vez, y podremos tener a lo sumo $k$ conjuntos de
este tipo.
\end{subenv}

Por tanto $F$ fragmenta al menos $|A| + k + |B| - k = |A| + |B| = |\mathcal{F}|$ conjuntos.

Fijamos ahora un $C\subseteq X$ con $|C| = m$. Entonces $|H_C| < \infty$. Luego: 

\[|H_C| \le |\{B\subseteq C : H_C \textrm{ fragmenta } B\}| = |\{B\subseteq C : H \textrm{ fragmenta } B\}|\] 

Por tanto: $|\{B\subseteq C : H \textrm{ fragmenta } B\}| \le \sum_{i=0}^d \binom{m}{i}$ puesto que $VC(H)=d$ y el número de 
subconjuntos de $C$ de cardinal menor o igual a $d$ viene dado por $\sum_{i=0}^d \binom{m}{i}$.

Veamos ahora $\sum_{i=0}^d \binom{m}{i} \le m^d + 1$. La parte izquierda de la desigualdad nos da todos los posibles 
subconjuntos desde $C$. Excepto el conjunto vacío, todos los demás subconjuntos pueden obtenerse extrayendo elementos con
repetición desde $C$, y hay $m^d$ posibles extracciones de este tipo.
\end{proof}

\begin{corollary}
Sea $H \subseteq 2^X$. Si $\exists d\in \mathbb{N}$ tal que $\Pi_{H} (m) > \sum_{i=0}^d \binom{m}{i} \le m^d + 1$,
entonces $VC(H) \ge d+1$.
\end{corollary}

\begin{proof}
Por reducción al absurdo desde el lema \ref{lemma:sauer}, si $\Pi_{H}(m) > \sum_{i=0}^d \binom{m}{i}$ entonces $VC(H) > d$.
\end{proof}

\begin{theorem}
 
\end{theorem}



\begin{theorem}
\textbf{Teorema fundamental de aprendizaje PAC}

Sea $H\subseteq 2^X$ y consideramos funciones de pérdida $0-1$. Entonces equivalen:

\begin{enumerate}[i]
\item \label{th:fundi} $H$ es de Glivenko-Cantelli.
\item \label{th:fundii} $H$ es APAC cognoscible por cuaquier algoritmo $ERM_H$.
\item \label{th:fundiii} $H$ es APAC cognoscible.
\item \label{th:fundiv} $H$ es PAC cognoscible.
\item \label{th:fundv} $H$ es PAC cognoscible por cualquier algoritmo $ERM_H$
\item \label{th:fundvi} $VC (H) < \infty$.
\end{enumerate}
\end{theorem}

\begin{proof}
 El teorema \ref{th:gc-apac} demuestra $\eqref{th:fundi} \Rightarrow \eqref{th:fundii} \Rightarrow \eqref{th:fundiii}$.
 
 Bajo las condiciones de función de pérdida $0-1$, $\eqref{th:fundiii} \Rightarrow \eqref{th:fundiv}$ y 
 $\eqref{th:fundii} \Rightarrow \eqref{th:fundv}$. 
 
 Del corolario \ref{cor:vc-finito} deducimos, por contradicción, $\eqref{th:fundiv} \Rightarrow \eqref{th:fundvi}$
 y $\eqref{th:fundv} \Rightarrow \eqref{th:fundvi}$.
 
 Falta ver $\eqref{th:fundvi} \Rightarrow \eqref{th:fundi}$. Sea $Z = X \times \{0,1\}$.
 
 Llamamos \begin{align*} 
	   Q = \{S \in Z^m : \exists h\in H \textrm{ con } |L_S(h) - L_{\dist}(h)| \ge \varepsilon\}\\
           R = \{(S_1, S_2) \in Z^{2m}: \exists h\in H \textrm{ con } |L_{S_1}(h) - L_{S_2}(h)| \ge \frac{\varepsilon}{2}\}\\
          \end{align*}
 Entonces para $m\ge \frac{1}{\varepsilon^2}$ se verifica $\mprob(Q) \le 2 \mmprob(R)$
 
 \begin{subenv}
 Aplicando desigualdad de Hoeffding \ref{ineq:hoeffding}, sabemos que fijado $h\in H$ tenemos 

 \begin{equation}
  \mdosprob \left[|L_{\dist}(h) - L_{S_2}(h)| \le \frac{\varepsilon}{2} \right] \ge 
   (1 - 2e^{-2m\varepsilon^2}) \underset{m\ge 1/\varepsilon^2}{\ge} (1 - 2e^{-2}) \ge \frac{1}{2}
  \tag{\mbox{$\ast$}}\label{eqn:s2bound}
 \end{equation}

 Dado $h\in H$ verificando $|L_{\dist}(h) - L_{S_1}(h)| \ge, \varepsilon$ $|L_{\dist}(h) - L_{S_2}(h)| \le \frac{\varepsilon}{2}$
 entonces por desigualdad triangular tenemos $|L_{S_1}(h) - L_{S_2}(h)| \ge \frac{\varepsilon}{2}$. Así:
 
 \begin{align}
  \mmprob(R) &\ge \mmprob \bigg[ \exists h \in H: |L_{\dist}(h) - L_{S_1}(h)| 
		 \ge \varepsilon, |L_{\dist}(h) - L_{S_1}(h)| \le \frac{\varepsilon}{2} \bigg] \ge \nonumber\\
             &\underset{\eqref{eqn:s2bound}}{\ge} \frac{\munoprob(Q)}{2}
 \end{align} 
 \end{subenv}
 
 Consideraremos en lo que sigue $S = (S_1, S_2) \sim \dist{2m}$.
 
 Consideramos ahora un conjunto de permutaciones $\Gamma$ sobre $\{1, \ldots, 2m\}$ verificando que para todo $i\in \{1,\ldots, m\}$
 entonces $\tau(i) = i+m, \tau(i+m) = i$ o bien $\tau(i) = i, \tau(i+m) = i$. Es decir son permutaciones que intercambian
 (o no) posiciones de $S_1$ y $S_2$. Asignaremos a $\Gamma$ una distribución uniforme $U^{\Gamma}$ y llamaremos 
 $P_{\tau} := \underset{\tau \sim U^{\Gamma}}{P}$
 
 Entonces se verificará:
 
 \begin{equation}
  \dosmprob(R) = \dosmexpect \bigg[P_{\tau} [\tau(S) \in R] \bigg] \le \max_{S\in Z^{2m}} P_{\tau}[\tau(S) \in R]  
  \label{eqn:Rbound}
 \end{equation}

 Veámoslo. Usando que $S$ es una muestra i.d.d. escogida desde $\dist^{2m}$:
 
 \[\dosmprob(R) = \dosmprob[S \in R] = \mprob[\tau(S) \in R], \quad \forall \tau \in \Gamma\]
 
 \begin{align*}
 \dosmprob(R) &= \dosmexpect [\mathds{1}_R(S)] = \dosmexpect [\mathds{1}_R(\tau(S))]
   = \frac{1}{|\Gamma|} \sum_{\tau \in \Gamma} \dosmexpect [\mathds{1}_R(\tau(S))] = \\
   &\underset{\textrm{(linealidad de $\expect$)}}{=} \dosmexpect \bigg[ \frac{1}{|\Gamma|} 
   \sum_{\tau \in \Gamma} [\mathds{1}_R(\tau(S))]\bigg] = \dosmexpect \bigg[P_{\tau} [\tau(S) \in R] \bigg]
 \end{align*}

 Ahora la desigualdad $\dosmexpect \bigg[P_{\tau} [\tau(S) \in R] \bigg] \le \max_{S\in Z^{2m}} P_{\tau}[\tau(S) \in R]$
 es trivial sin más que asegurar que existe el máximo en $S\in Z^{2m}$ de $P_{\tau}[\tau(S) \in R]$ puesto que $P_{\tau}$
 toma finitos valores sobre su $\sigma$ álgebra correspondiente.
  
 
 
 
\end{proof}

