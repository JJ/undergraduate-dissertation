\section{Conjunto fragmentado}
La abreviaremos dimensión VC

\begin{definition} \textbf{Conjunto fragmentado}

 Sea $H\subseteq 2^X$ y sea $A\subseteq X$. Decimos que $H$ fragmenta a $A$ si para toda función $g:A\rightarrow \{0,1\}$ 
 existe $h\in H$ verificando $h_{|A} = g$.
\end{definition}

Recordando la biyección canónica entre hipótesis y conjuntos \ref{biyeccion-canonica}, podemos aportar la siguiente 
caracterización de fragmentación.

\begin{fact} \textbf{Caracterización de conjunto fragmentado}

 Sea $H\subseteq 2^X$ y sea $A\subseteq X$. Decimos que $H$ fragmenta a $A$ si para toda $B\subseteq A$, se tiene que existe
 $h\in H$ verificando $X_h \cap A = B$.
\end{fact}

  \begin{proof}
  Trivial desde \ref{biyeccion-canonica}
  \end{proof}


\begin{definition}
Sea $H\subseteq 2^X$, y $C \subseteq X$. Llamamos restricción de $H$ a $C$ a:

\[H_{C} := \{h_{|C} : h\in H\}\]

En notación conjuntista, existe biyección entre $H_C$ y $C_{H}$ con:

\[C_{H} := \{C_h : h\in H\} \subseteq \mathcal{P}(X)\]
\end{definition}

Cuando demostrábamos el teorema No Free Lunch \ref{th:nfl}, pedíamos que la clase de clasificadores $H$ fuese todo $2^X$. 
Observando la demostración, solo asumiamos la existencia de un conjunto $C\subseteq X$ sobre el que pudiéramos definir 
todas las posibles hipótesis $C \rightarrow \{0,1\}$. Revisitamos por tanto el enunciado de dicho teorema.

\begin{theorem}
\textbf{Teorema de No Free Lunch revisitado}

Sea un dominio $X$, un conjunto de etiquetas $Y=\{0,1\}$, $H \subseteq 2^X$.
Sea $A: \underset{m\in \mathbb{N}}{\bigcup} (X\times Y)^m \rightarrow 2^X$ un algoritmo. Dado $m \le \frac{|X|}{2}$,
supongamos que existe $C\subseteq X$ de tamaño $2m$ fragmentado por $H$. Entonces existe $\dist$ distribución sobre 
$X\times \{0,1\}$ verificando:

\begin{enumerate}[i]
\item Existe una función $f: X \rightarrow \{0,1\}$ con $L_{\dist}(f)=0$
\item $\mprob \left[ L_{\dist}(A(S)) \ge \frac{1}{8} \right] \ge \frac{1}{7}$
\end{enumerate}

\label{th:nfl2}
\end{theorem}


\section{Dimensión VC}

\begin{definition}
\textbf{Dimensión VC}

Sea $H \subseteq 2^X$. Definimos su dimensión Vapnik-Chervonenkis, que abreviaremos dimensión VC, como:

\[VC(H) := \max \{|A| : A\subseteq X, A \textrm{ es fragmentado por } H\}\]
\end{definition}


\begin{example}

Sea $H_{thr} = \{h_a = \mathds{1}_{[-\infty, a]}: a\in \mathbb{R}\}$ clase de hipótesis sobre $\mathbb{R}$. 

Dado un conjunto $C=\{\alpha\}$, podemos tomar $h_{\alpha}$ y $h_{\alpha+1}$, que nos dan todos los posibles etiquetados de
$C$. Sin embargo, dado un conjunto de tamaño 2, $C=\{\alpha, \beta\}$, donde podemos suponer spg. $\alpha < \beta$. 
Entonces no podemos encontrar $h_b \in H$ verificando $h_b(\alpha)=0$ y $h_b(\beta) = 1$, ya que esto implicaría que 
$b > \beta$ y por tanto entraría en contradicción con que $h_b(\alpha) = 0$.

Hemos probado $VC(H_{thr}) = 1$.

\end{example}

\begin{example}

Sea $H^1_{rec} = \{h_{a,b} = \mathds{1}_{[a,b]}: a,b\in \mathbb{R}\}$ clase de hipótesis sobre $\mathbb{R}$. 

Dado un conjunto $C=\{\alpha\, \beta\}$, con $\alpha < \beta$, las hipótesis $h_{\alpha+\delta_1, \beta + \delta_2}$ con 
$\delta_i \in \{0,1\}$ nos dan todos los posibles etiquetados de $C$. Sin embargo, dado un conjunto de tamaño 3, 
$C=\{\alpha, \beta\, \theta\}$, donde podemos suponer $\alpha < \beta < \theta$. Entonces no podemos encontrar $h_b \in H$ 
verificando $h_b(\alpha)=1$ y $h_b(\theta) = 1$ y $h_b(\beta) = 0$.

Por tanto $VC(H^1_{rec}) = 2$.
\end{example}

\begin{example}

Sea $H^2_{rec} = \{h_{a,b,c,d} = \mathds{1}_{[a,b]\times [c,d]}: a,b,c,d\in \mathbb{R}\}$ clase de hipótesis sobre 
$\mathbb{R}^2$.

Podemos fragmentar el conjunto $C = \{(0,1), (0,-1), (1,0), (-1,0)\}$

\img{./imgs/rect-vc.png}{0.7}

Luego $VC(H^2_{rec}) \ge 4$. Veamos que $VC(H^2_{rec}) = 4$.

Sea $C\subseteq \mathbb{R}^2$ con $|C|=5$. Entonces podemos 
tomar $x_1 \in C$ punto más a la izquierda de $C$ (primera coordenada más pequeña), $x_2 \in C$ punto más a la derecha de 
$C$, $x_3 \in C$ punto más arriba en $C$ (segunda coordenada más grande), $x_4 \in C$ punto más abajo en $C$. Se podría 
tener $x_i = x_j$ para algún $i\neq j$. Si tenemos un rectángulo que contiene a dichos puntos, debe necesariamente contenerlos
a todos.
\end{example}



\begin{fact} \textbf{Propiedades de la dimensión VC}

Sean $H,H_1, H_2 \subseteq 2^X$. Se cumple:

\begin{enumerate}[i]
 \item $VC(H) \le log_2(|H|)$.
 \item Si $H_1 \subseteq H_2$ entonces $VC(H_1) \le VC(H_2)$.
 \item $VC(H_1 \cup H_2) \le VC(H_1) + VC(H_2)$.
 \item $VC(H)$ no depende del número de parámetros de los clasificadores de $H$.
\end{enumerate}
\end{fact}

\begin{proof}~
 \begin{enumerate}[i]
  \item Equivalentemente, veamos $2^{VC(H)} \le |H|$. 
  
  Si $VC(H) < \infty$, entonces $H$ fragmenta un conjunto de tamaño
  $VC(H)$, a saber, $A$, y por tanto deben existir tantos elementos en $H$ al menos como subconjuntos de $A$. El número de
  subconjuntos de $A$ viene dado por $2^{|A|} = 2^{VC(H)}$. 
  
  Si $VC(H) = \infty$, entonces $|H| = \infty$, ya que para cada 
  $n\in \mathbb{N}$ podríamos fragmentar con $H$ un conjunto de tamaño $n$, y por tanto $H$ debería tener al menos $2^n$ 
  elementos.
  
  \item Si $H_1$ fragmenta un conjunto de tamaño $VC(H_1)$, entonces $H_2$ también fragmenta ese mismo conjunto puesto que
  $H_1 \subseteq H_2$. Por tanto $VC(H_1) \le VC(H_2)$.
 
  \item 
 
  \item En los ejemplos anteriores hemos visto como $VC(H)$ coincidía con el número de parámetros de los que dependían los
  clasificadores de $H$: $VC(H_{thr}) = 1$, $VC(H^1_{rec}) = 2$, $VC(H^2_{rec}) = 4$. Veamos un ejemplo donde no se cumple
  esto.
  
  Sea $H_{sin} = \{h_{\theta}: \theta \in \mathbb{R}\}$ donde $h_{\theta}: \mathbb{R} \rightarrow \{0,1\}$ está definida por 
  $h_\theta (x) = \lceil -0.5sen(\theta x) \rceil$. Cada $h \in H_{sin}$ depende de un único parámetro $\theta$.
  
  Sea $d\in \mathbb{N}$. Tomamos $d$ puntos codificados por $x_i = 0.x_{i,1} x_{i,2} \ldots x_{i, 2^d} x_{i, 2^d+1}$ 
  donde cada $x_{i}, i=1, \ldots d$ viene determinado por una fila de la matriz $\bigg(x_{i,j}\bigg)$
  donde la columna $j$-ésima codifica el número $j-1$ en binario leído de arriba a abajo, para $j\neq 2^d+1$, y la columna 
  $2^d+1$-ésima es constantemente $1$.
  
% 
%   \[\left(\begin{array}{ccccc}
%     \underbrace{\begin{array}{c} 0 \\ 0 \\ \vdots \\0 \end{array}}_{\phantom{0}} &
%     \underbrace{\begin{array}{c} 0 \\ 0 \\ \vdots \\1 \end{array}}_{1} &
%     \underbrace{\begin{array}{c} \ldots \\ \ldots \\ \ddots \\ \ldots \end{array}}_{1} &
%     \underbrace{\begin{array}{c} 1 \\ 1 \\ \vdots \\1 \end{array}}_{2^d-1} &
%     \underbrace{\begin{array}{c} 1 \\ 1 \\ \vdots \\1 \end{array}}
%   \end{array}\right)\]

    
  \[d \left\{\left( 
    \begin{array}{lcccccccccccc}
    0 &&& 0 &&& \ldots &&& 1 &&& 1\\
    0 &&& 0 &&& \ldots &&& 1 &&& 1\\
    \vdots &&& \vdots &&& \ddots &&& \vdots &&& \vdots\\
    \undermat{0}{0} &&& \undermat{1}{1} &&& \ldots &&& \undermat{2^d-1}{1} &&& 1
    \end{array}
  \right)\right.\]
  
  \bigskip
  
  
  Así, dada una asignación de $d$ etiquetas, debe codificar un número en binario entre $0$ y $2^d-1$, a saber, 
  la columna $k$ ésima de la matriz. Tomamos el clasificador $h(x) = \lceil -0.5 sen(10^k \pi x) \rceil$ que verificará
  que su asignación de etiquetas es justamente la columna $k$ ésima. 
  
  El sentido de la última columna constantemente $1$
  puede explicarse en que daremos $x_{i,1} \ldots x_{i,k}$ medias vueltas a la circunferencia unidad y recorreremos
  una fracción $0.x_{i,k+1} \ldots x_{i, 2^d} 1$ no nula de otra media vuelta a la circunferencia. Si 
  $x_{i,k} = 1$ entonces $h(x_i) = 1$, y si $x_{i,k}=0$ entonces $h(x_i) = 0$. 
  
  Como $d\in \mathbb{N}$ era arbitrario, $VC(H_{sin}) = \infty$, y por tanto hemos encontrado una clase de hipótesis 
  uniparámetrica con dimensión $VC(H_{sin})\neq 1$.
 \end{enumerate}
\end{proof}


\subsection{Teorema fundamental del aprendizaje automático}

\begin{definition}
\textbf{Función de crecimiento}

Sea $H$ una clase de hipótesis. Definimos como función de crecimiento de $H$:

\[\tau_{H}: \mathbb{N} \longrightarrow \mathbb{N}, \qquad \tau_{H}(m) = \max_{C \subseteq X, \,\, |C|=m} |H_C|\]
\end{definition}

Esta función está bien definida puesto que fijado $m \in \mathbb{N}$ y $C\subseteq X$ con $|C| = m$, se tiene siempre 
que $|H_C| \le 2^C$, identificando elementos de $H_C$ con subconjuntos de $C$.

\begin{lemma} \textbf{Lema de Sauer-Shelah-Perles}

Sea $H \subseteq 2^X$ con $VC(H) \le d < \infty$. Entonces para $m\in \mathbb{N}$ verificando $d\le m$:

\[\tau_{H} (m) \le \sum_{i=0}^d \binom{m}{i} \le m^d + 1\]
\label{lemma:sauer}
\end{lemma}


\begin{proof}
Supongamos s.p.g. $VC(H) = d$.

Empezamos probando que una clase de hipótesis $\mathcal{F}$ finita fragmenta al menos $|\mathcal{F}|$ conjuntos. Lo hacemos 
por inducción sobre el tamaño de $X_\mathcal{F}$ que es un conjunto finito por ser $\mathcal{F}$ clase finita.

\begin{subenv}
Si su tamaño es 1, parte al conjunto vacío.

Supuesto que se verifica la hipótesis para tamaños menores que $k-1$ y sea $|X_\mathcal{F}| = k$. Escogemos entonces 
$x\in X$ verificando que $x$ pertenece a algunos conjuntos de $X_\mathcal{F}$ pero no a todos (debe existir, sino 
tendríamos que $X_\mathcal{F}$ contiene un único conjunto).

Definimos: $\begin{array}{l} 
	A = \{S \subseteq X_\mathcal{F} : x\in S\} \\
        A'=\{S\setminus\{x\} : S \in A\} \\ 
        B= \{S \subseteq X_\mathcal{F} : x\not\in S\}
     \end{array}$ donde $X_{\mathcal{F}} = A\cup B$.

Claramente $|A| = |A'|$ y por hipótesis de inducción $A'$ fragmenta $k \ge |A|$ conjuntos, y $B$ fragmenta $|B|$ conjuntos. 
También es trivial ver que los conjuntos $S$ y $S\cup \{x\}$ son fragmentados por $A$ donde $S$ es un conjunto fragmentado 
por $A'$. 

Hemos probado que $A$ fragmenta $|A'| + k = |A|+k$ conjuntos, y si un conjunto es fragmentado por $A$ y $B$ a la vez, 
entoces no contiene a $x$, luego es fragmentado por $A'$ y por $B$ a la vez, y podremos tener a lo sumo $k$ conjuntos de
este tipo.
\end{subenv}

En definitiva hemos probado que fragmentamos $|A| + k + |B| - k = |A| + |B| = |\mathcal{F}|$ conjuntos. Por tanto, como 
$VC(H_C) \le VC(H) = d$ y $H_C$ no puede fragmentar ningún subconjunto $C$ de tamaño mayor que $d$ y así: 

\[\tau_{H} (m) \le \sum_{i=0}^d \binom{m}{i}\]

Veamos ahora $\sum_{i=0}^d \binom{m}{i} \le m^d + 1$. La parte izquierda de la desigualdad nos da todos los posibles 
subconjuntos de un conjunto de tamaño $m$, a saber, $C$. Excepto el conjunto vacío, todos los demás subconjuntos pueden
obtenerse extrayendo elementos con repetición desde $C$, y hay $m^d$ posibles extracciones de este tipo.
\end{proof}

\begin{corollary}
Sea $H \subseteq 2^X$. Si $\exists d\in \mathbb{N}$ tal que $\tau_{H} (m) > \sum_{i=0}^d \binom{m}{i} \le m^d + 1$,
entonces $VC(H) \ge d+1$.
\end{corollary}

\begin{proof}
Por reducción al absurdo desde el lema \ref{lemma:sauer}, si $\tau_{H}(m) > \sum_{i=0}^d \binom{m}{i}$ entonces $H$ debe
fragmentar un conjunto de tamaño $d+1$ al menos.
\end{proof}

\begin{theorem}
\textbf{Teorema fundamental de aprendizaje PAC}

Sea $H$ clase de hipótesis de la forma $h: X \rightarrow \{0,1\}$ y la función de pérdida 0-1. Entonces equivalen:

\begin{enumerate}
\item $H$ tiene la propiedad de convergencia uniforme.
\item $H$ es agnósticamente PAC cognoscible por cuaquier algoritmo ERM.
\item $H$ es agnósticamente PAC cognoscible.
\item $H$ es PAC cognoscible.
\item $H$ es PAC cognoscible por cualquier algoritmo ERM.
\item $VC (H) < \infty$.
\end{enumerate}
\end{theorem}


La implicación que nos falta es $6 \implies 1$. El resto de implicaciones se consiguen a partir de teoremas ya probados en temas anteriores.

