Este capítulo introduce el concepto de fragmentación y de dimensión Vapnik-Chervonenkis (VC), un término muy importante en 
combinatoria y en geometría computacional. A partir de dichas nociones obtendremos el lema de Sauer-Shelah, la herramienta 
que nos falta para poder demostrar el teorema fundamental del aprendizaje PAC, que relaciona estadística, a través de 
clases de Glivenko-Cantelli, teoría PAC y geometría computacional, a través de la dimensión VC.

\section{Fragmentación}
\begin{definition}
Sea $H\subseteq 2^X$, y $C \subseteq X$. Llamamos restricción de $H$ a $C$:
\[
  H_{C} := \{h_{|C} : h\in H\}
\]

En notación conjuntista, existe biyección entre $H_C$ y $C_{H}$ donde:
\[
  C_{H} := \{C_h : h\in H\} \subseteq \mathcal{P}(X)
\]
\end{definition}

\begin{definition}[Conjunto fragmentado]
 Sea $H\subseteq 2^X$ y sea $A\subseteq X$. Decimos que $H$ fragmenta a $A$ si para toda función $g:A\rightarrow \{0,1\}$ 
 existe $h\in H$ verificando $h_{|A} = g$.
 \label{def:shattering}
\end{definition}

Recordando la biyección canónica entre hipótesis y conjuntos \ref{biyeccion-canonica}, podemos aportar la siguiente 
caracterización de fragmentación.

\begin{fact}[Caracterizaciones de conjunto fragmentado]
 Sea $H\subseteq 2^X$ y sea $A\subseteq X$ finito. Entonces equivalen:
 \begin{enumerate}[i]
  \item $H$ fragmenta a $A$.
  \item Para todo $B\subseteq A$, existe $h\in H$ verificando $X_h \cap A = B$
  \item $A_H = \mathcal{P}(A)$.
  \item $|A_H| = 2^{|A|}$
  \item $|H_A| = 2^{|A|}$
 \end{enumerate}
\end{fact}

  \begin{proof}
   Trivial desde \ref{biyeccion-canonica} y \ref{def:shattering}.
  \end{proof}

Cuando demostrábamos el teorema No Free Lunch \ref{th:nfl}, pedíamos que la clase de clasificadores $H$ fuese todo $2^X$. 
Observando la demostración, solo asumiamos la existencia de un conjunto $C\subseteq X$ sobre el que pudiéramos definir 
todas las posibles hipótesis $C \rightarrow \{0,1\}$. Revisitamos por tanto el enunciado de dicho teorema.

\begin{theorem}[Teorema de No Free Lunch revisitado]
Sea un dominio $X$, un conjunto de etiquetas $Y=\{0,1\}$, $H \subseteq 2^X$, 
$A: \underset{m\in \mathbb{N}}{\bigcup} (X\times Y)^m \rightarrow 2^X$ un algoritmo. Dado $m \le \frac{|X|}{2}$,
supongamos que existe $C\subseteq X$ de tamaño $2m$ fragmentado por $H$. Entonces existe $\dist$ distribución sobre 
$X\times \{0,1\}$ verificando:
\begin{enumerate}[i]
\item Existe una función $f: X \rightarrow \{0,1\}$ con $L_{\dist}(f)=0$
\item $\mprob \left[ L_{\dist}(A(S)) > \frac{1}{8} \right] \ge \frac{1}{7}$
\end{enumerate}

\label{th:nfl2}
\end{theorem}


\section{Dimensión VC}
\begin{definition}[Dimensión VC]
Sea $H \subseteq 2^X$. Definimos su dimensión Vapnik-Chervonenkis, que abreviaremos dimensión VC, como:
\[
  VC(H) := \max \{|A| : A\subseteq X, A \textrm{ es fragmentado por } H\}
\]
\end{definition}


\begin{example}
Sea $H_{thr} = \{h_a = \mathds{1}_{]-\infty, a]}: a\in \mathbb{R}\}$ clase de hipótesis sobre $\mathbb{R}$. 

Dado un conjunto $C=\{\alpha\}$, podemos tomar $h_{\alpha}$ y $h_{\alpha-1}$, que nos dan todos los posibles etiquetados de
$C$. Sin embargo, dado un conjunto de tamaño 2, $C=\{\alpha, \beta\}$, donde podemos suponer s.p.g. $\alpha < \beta$. 
Entonces no podemos encontrar $h_b \in H$ verificando $h_b(\alpha)=0$ y $h_b(\beta) = 1$, ya que esto implicaría que 
$b \ge \beta$ y por tanto entraría en contradicción con que $h_b(\alpha) = 0$.

Hemos probado $VC(H_{thr}) = 1$.
\end{example}

\begin{example}
Sea $H^1_{rec} = \{h_{a,b} = \mathds{1}_{[a,b]}: a,b\in \mathbb{R}\}$ clase de hipótesis sobre $\mathbb{R}$. 

Dado un conjunto $C=\{\alpha, \beta\}$, con $\alpha < \beta$, las hipótesis $h_{\alpha+\delta_1, \beta - \delta_2}$ con 
$\delta_i \in \left\{0, \frac{\beta - \alpha}{2} \right\}$ nos dan todos los posibles etiquetados de $C$. Sin embargo, 
dado un conjunto de tamaño 3, $C=\{\alpha, \beta, \theta\}$, donde podemos suponer $\alpha < \beta < \theta$, no
podemos encontrar $h_b \in H$ verificando $h_b(\alpha)=1$, $h_b(\theta) = 1$ y $h_b(\beta) = 0$.

Por tanto $VC(H^1_{rec}) = 2$.
\end{example}

\begin{example}
Sea $H^2_{rec}$ la clase de hipótesis definidas en \ref{def:rec2}.

Podemos fragmentar el conjunto $C = \{(0,1), (0,-1), (1,0), (-1,0)\}$

\imgcaption{./imgs/rect-vc.png}{Rectángulos que framentan $C$}{0.65}

Luego $VC(H^2_{rec}) \ge 4$. Veamos que $VC(H^2_{rec}) = 4$.

Sea $C\subseteq \mathbb{R}^2$ con $|C|=5$. Entonces podemos 
tomar $z_1 \in C$ punto de menor abscisa, $z_2 \in C$ punto de mayor abscisa, $z_3 \in C$ punto de mayor 
ordenada, $z_4 \in C$ punto de menor ordenada. Se podría tener s.p.g. $z_i = z_j$ para algún $i\neq j$. Si tenemos 
un rectángulo que contiene a dichos puntos, debe necesariamente contenerlos a todos, luego no existe uno que contenga esos
cuatro puntos y no el restante.
\end{example}


\begin{fact}[Propiedades de la dimensión VC]
Sean $H,H_1, H_2 \subseteq 2^X$. Se cumple:
\begin{enumerate}[i]
 \item $VC(H) \le \log_2(|H|)$.
 \item Si $H_1 \subseteq H_2$ entonces $VC(H_1) \le VC(H_2)$.
 \item $VC(H)$ no depende del número de parámetros de los clasificadores de $H$.
\end{enumerate}

\label{fact:props-vc}
\end{fact}

\begin{proof}~
 \begin{enumerate}[i]
  \item Equivalentemente, veamos $2^{VC(H)} \le |H|$. 
  
  Si $VC(H) < \infty$, entonces $H$ fragmenta un conjunto de tamaño
  $VC(H)$, a saber, $A$, y por tanto deben existir tantos elementos en $H$ al menos como subconjuntos de $A$. El número de
  subconjuntos de $A$ viene dado por $2^{|A|} = 2^{VC(H)}$. 
  
  Si $VC(H) = \infty$, entonces $|H| = \infty$, ya que para cada 
  $n\in \mathbb{N}$ podríamos fragmentar con $H$ un conjunto de tamaño $n$, y por tanto $H$ debería tener al menos $2^n$ 
  elementos.
  
  \item Si $H_1$ fragmenta un conjunto de tamaño $VC(H_1)$, entonces $H_2$ también fragmenta ese mismo conjunto puesto que
  $H_1 \subseteq H_2$. Por tanto $VC(H_1) \le VC(H_2)$.
 
  \item En los ejemplos anteriores hemos visto como $VC(H)$ coincidía con el número de parámetros de los que dependían los
  clasificadores de $H$: $VC(H_{thr}) = 1$, $VC(H^1_{rec}) = 2$, $VC(H^2_{rec}) = 4$. Veamos un ejemplo donde no se cumple
  esto.
  
  Sea $H_{sin} = \{h_{\theta}: \theta \in \mathbb{R}\}$ donde $h_{\theta}: \mathbb{R} \rightarrow \{0,1\}$ está definida por 
  $h_\theta (x) = \lceil -0.5sen(\theta x) \rceil$. Cada $h \in H_{sin}$ depende de un único parámetro $\theta$.
  
  Sea $d\in \mathbb{N}$. Tomamos $d$ puntos dados por $x_i = 0.x_{i,1} x_{i,2} \ldots x_{i, 2^d} x_{i, 2^d+1}$ 
  donde cada $x_{i}, i=1, \ldots, d$ viene determinado por una fila de la matriz $\bigg(x_{i,j}\bigg)$
  donde la columna $j$-ésima codifica el número $j-1$ en binario leído de arriba a abajo, para $j\neq 2^d+1$, y la columna 
  $(2^d+1)$-ésima es constantemente $1$.
  
% 
%   \[\left(\begin{array}{ccccc}
%     \underbrace{\begin{array}{c} 0 \\ 0 \\ \vdots \\0 \end{array}}_{\phantom{0}} &
%     \underbrace{\begin{array}{c} 0 \\ 0 \\ \vdots \\1 \end{array}}_{1} &
%     \underbrace{\begin{array}{c} \ldots \\ \ldots \\ \ddots \\ \ldots \end{array}}_{1} &
%     \underbrace{\begin{array}{c} 1 \\ 1 \\ \vdots \\1 \end{array}}_{2^d-1} &
%     \underbrace{\begin{array}{c} 1 \\ 1 \\ \vdots \\1 \end{array}}
%   \end{array}\right)\]

    
  \[d \left\{\left( 
    \begin{array}{lcccccccccccc}
    0 &&& 0 &&& \ldots &&& 1 &&& 1\\
    0 &&& 0 &&& \ldots &&& 1 &&& 1\\
    \vdots &&& \vdots &&& \ddots &&& \vdots &&& \vdots\\
    \undermat{0}{0} &&& \undermat{1}{1} &&& \ldots &&& \undermat{2^d-1}{1} &&& 1
    \end{array}
  \right)\right.\]
  
  \bigskip
  
  Así, dada una asignación de $d$ etiquetas, debe codificar un número en binario entre $0$ y $2^d-1$, a saber, 
  la columna $k$ ésima de la matriz. Tomamos el clasificador $h(x) = \lceil -0.5 sen(10^k \pi x) \rceil$ que verificará
  que su asignación de etiquetas es justamente la columna $k$ ésima. 
  
  La última columna constantemente $1$ puede explicarse en que daremos $x_{i,1} \ldots x_{i,k}$ medias vueltas a la 
  circunferencia unidad y recorreremos una fracción $0.x_{i,k+1} \ldots x_{i, 2^d} 1$ no nula de otra media vuelta a la 
  circunferencia. Si $x_{i,k} = 1$ entonces $h(x_i) = 1$, y si $x_{i,k}=0$ entonces $h(x_i) = 0$. 
  
  Como $d\in \mathbb{N}$ era arbitrario, $VC(H_{sin}) = \infty$, y por tanto hemos encontrado una clase de hipótesis 
  uniparámetrica con dimensión $VC(H_{sin})\neq 1$.
 \end{enumerate}
\end{proof}


\begin{corollary}
 Sea $H \subseteq 2^X$. Si $VC(H) = \infty$, entonces $H$ no es PAC cognoscible.
 \label{cor:vc-finito}
\end{corollary}

  \begin{proof}
  Si $X < \infty$, entonces $|H| \le 2^{|X|} < \infty$, y $VC(H) \le \log_2(|H|) < \infty$. Luego $|X| = \infty$.
  
  Por reducción al absurdo. Sea $A$ el algoritmo de aprendizaje. Para todo $m\in \mathbb{N}$ par arbitrario, como 
  $m\le \frac{|X|}{2}$, existe $C$ con $|C| = m$ fragmentado por $H$, el teorema \ref{th:nfl2} nos diría que existe una 
  distribución $\dist$ verificando:
  \[
    \mprob \left[ L_{\dist}(A(S)) > \frac{1}{8} \right] \ge \frac{1}{7}
  \]
 
  Luego no puede existir $m_H \left(\frac{1}{8}, \delta\right)$ con $\delta < \frac{1}{7}$.
  \end{proof}

Nótese la similitud de este corolario con \ref{cor:P(X)notpac}.

\subsection{Función de crecimiento}
\begin{definition}[Función de crecimiento]
Sea $H$ una clase de hipótesis. Definimos como función de crecimiento de $H$:
\[
  \Pi_{H}: \mathbb{N} \longrightarrow \mathbb{N}, \qquad \Pi_{H}(m) = \max_{C \subseteq X, \,\, |C|=m} |H_C|
\]
\end{definition}

Esta función está bien definida puesto que fijado $m \in \mathbb{N}$ y $C\subseteq X$ con $|C| = m$, se tiene siempre 
que $|H_C| \le 2^C$, identificando elementos de $H_C$ con subconjuntos de $C$.

\begin{lemma}[Lema de Sauer-Shelah-Perles]
Sea $H \subseteq 2^X$ con $VC(H) \le d < \infty$. Entonces para $m\in \mathbb{N}$ verificando $d\le m$:
\[
  \Pi_{H} (m) \le \sum_{i=0}^d \binom{m}{i} \le m^d + 1
\]
\label{lemma:sauer}
\end{lemma}


\begin{proof}
Supongamos s.p.g. $VC(H) = d$.

Empezamos probando que una clase $\mathcal{F}$, con $|\mathcal{F}| < \infty$, fragmenta al menos $|\mathcal{F}|$ conjuntos. 
Lo hacemos por inducción sobre el tamaño de $X_\mathcal{F}$ que es un conjunto finito por ser $\mathcal{F}$ clase finita.

\begin{subenv}
Si su tamaño es 1, fragmenta al conjunto vacío.

Supuesto que se verifica la hipótesis para tamaños menores que $k-1$ y sea $|X_\mathcal{F}| = k$. Escogemos entonces 
$x\in X$ verificando que $x$ pertenece a algunos conjuntos de $X_\mathcal{F}$ pero no a todos (debe existir, sino 
tendríamos que $X_\mathcal{F}$ contiene un único conjunto).

Definimos: $\begin{array}{ll} 
            A &= \{S \in X_\mathcal{F} : x\in S\} \\
            A'&=\{S\setminus\{x\} : S \in A\} \\ 
            B &= \{S \in X_\mathcal{F} : x\not\in S\}
           \end{array}$ donde $X_{\mathcal{F}} = A + B$.

Claramente $|A| = |A'|$ y por hipótesis de inducción $A'$ fragmenta $p \ge |A|$ conjuntos, y $B$ fragmenta $|B|$ conjuntos. 
Por otro lado, si un conjunto es fragmentado por $A$ y $B$ a la vez, entoces no contiene a $x$, luego es fragmentado por
$A'$ y por $B$ a la vez, y podremos tener a lo sumo $p$ conjuntos de este tipo. De hecho, dado $S$ conjunto fragmentado por
$A$ y $B$, se tendría $x\notin S$, claramente, y $S\cup\{x\}$ sería fragmentado por $A + B$, pero no por $A$ o $B$ por
separado.

Así, $\mathcal{F}$ fragmenta al menos $|A| + p + |B| - p = |A| + |B| = |X_\mathcal{F}|$ conjuntos.
\end{subenv}

Fijamos ahora un $C\subseteq X$ con $|C| = m$. Entonces $|H_C| < \infty$. Luego:
\[
  |H_C| \le |\{B\subseteq C : H_C \textrm{ fragmenta } B\}| = |\{B\subseteq C : H \textrm{ fragmenta } B\}|
\]

Por tanto $|\{B\subseteq C : H \textrm{ fragmenta } B\}| \le \sum_{i=0}^d \binom{m}{i}$ al ser $VC(H)=d$ y el número de 
subconjuntos de $C$ de cardinal menor o igual a $d$ viene dado por $\sum_{i=0}^d \binom{m}{i}$.

Veamos ahora $\sum_{i=0}^d \binom{m}{i} \le m^d + 1$. La parte izquierda de la desigualdad nos da todos los posibles 
subconjuntos desde $C$. Excepto el conjunto vacío, todos los demás subconjuntos pueden obtenerse extrayendo elementos con
repetición desde $C$, y hay $m^d$ posibles extracciones de este tipo.
\end{proof}

\begin{corollary}
Sea $H \subseteq 2^X$. Si existe $d\in \mathbb{N}$ tal que $\Pi_{H} (m) > \sum_{i=0}^d \binom{m}{i}$,
entonces $VC(H)$ es al menos $d+1$.
\end{corollary}

\begin{proof}
Por reducción al absurdo desde el lema \ref{lemma:sauer}, si $\Pi_{H}(m) > \sum_{i=0}^d \binom{m}{i}$ entonces 
no puede ser $VC(H) \le d$.
\end{proof}

\section{Teorema fundamental del aprendizaje PAC}

\begin{lemma}
 Sea $a > 0$. Entonces $x \ge 2a \log(a) \Rightarrow x \ge a \log(x)$ para cualquier $x > 0$.
 \label{lemma:ineqlog}
\end{lemma}

\begin{proof}
 Tomamos $f(x) = x - a\log(x)$. Se tiene $f'(x) = 1 - \frac{a}{x}$ y $f'(x) = 0 \Leftrightarrow x = a$. Como 
 $\lim_{x\rightarrow +\infty}f(x) = +\infty$, $f$ debe tener un mínimo en $a$, por haber un único punto crítico.
 
 Si $a \le e$, entonces se tiene $f(a) = a-a \log(a) \ge a-a \ge 0$ y la desigualdad se cumple.
 
 Si $a > e$, entonces $2a \log(a) \ge a$ y:
 
 \begin{align*}
  f(2a \log(a)) &= 2a \log(a) - a \log(2a \log(a)) \\
	        &= 2a \log(a) - a \log(a) - a \log(2\log(a)) \\
                &= a \log(a) - a \log(2 \log(a)) = a \log \left(\frac{a}{2 \log(a)}\right)
 \end{align*}
 
 Tomando $g(a) = a - 2\log(a)$, se tiene $g'(a) = 1 - \frac{2}{a} = 0$ sii $a = 2$, y $g(2) = 2 - \log(4) \ge 2 - \log(e^2) = 0$.
 Luego $g(a) \ge 0$ y $\frac{a}{2 \log(a)} \ge 1$, con lo que $f(2a \log(a)) \ge 0$, y se deduce la igualdad al ser $f$
 creciente para $x\ge a$.
\end{proof}


\begin{lemma}
 Sea $a \ge \frac{1}{2}$, $b > 0$. Entonces $x\ge 4a \log(2a) + 2b \Rightarrow x \ge a \log(x) + b$.
 \label{lemma:boundlog}
\end{lemma}

\begin{proof}
 Como $a \ge \frac{1}{2}$, entonces $x \ge 4a \log(2a) \ge 0$ y $x \ge 2b$.
 
 Por otro lado, como $b > 0$, entonces $2b > 0$ y $x \ge 4a \log(2a)$. Usando el lema \ref{lemma:ineqlog} 
 deducimos que $x \ge 2a \log(x)$.
 
 Sumando desigualdades: $2x \ge 2a \log(x) + 2b \Leftrightarrow x \ge a\log(x) + b$.
\end{proof}


\begin{theorem}[Teorema fundamental de aprendizaje PAC]
Sea $H\subseteq 2^X$. Consideramos funciones de pérdida $0-1$. Entonces equivalen:
\begin{enumerate}[i]
\item \label{th:fundi} $H$ es de Glivenko-Cantelli.
\item \label{th:fundii} $H$ es APAC cognoscible por cuaquier algoritmo $ERM_H$.
\item \label{th:fundiii} $H$ es APAC cognoscible.
\item \label{th:fundiv} $H$ es PAC cognoscible.
\item \label{th:fundv} $H$ es PAC cognoscible por cualquier algoritmo $ERM_H$.
\item \label{th:fundvi} $VC (H) < \infty$.
\end{enumerate}
\label{th:fundamental}
\end{theorem}

\begin{proof}
 El teorema \ref{th:gc-apac} demuestra $\eqref{th:fundi} \Rightarrow \eqref{th:fundii} \Rightarrow \eqref{th:fundiii}$.
 
 Bajo las condiciones de función de pérdida $0-1$, $\eqref{th:fundiii} \Rightarrow \eqref{th:fundiv}$ y 
 por proposición \ref{fact:rel-pac-apac} $\eqref{th:fundii} \Rightarrow \eqref{th:fundv}$.
 
 Del corolario \ref{cor:vc-finito} deducimos, por contradicción, $\eqref{th:fundiv} \Rightarrow \eqref{th:fundvi}$
 y $\eqref{th:fundv} \Rightarrow \eqref{th:fundvi}$. Falta ver $\eqref{th:fundvi} \Rightarrow \eqref{th:fundi}$.
 
 Sea $Z = X \times \{0,1\}$. Fijamos $\epsilon, \delta \in ]0,1[$ y una distribución $\mathcal{D}$ sobre $Z$ que cumpla
 suficientes condiciones de medibilidad sobre la función de pérdida. Llamamos:
 \begin{align*}
  Q &= \{S \in Z^m : \exists h\in H \textrm{ con } |L_S(h) - L_{\dist}(h)| \ge \varepsilon\}\\
  R &= \{(S_1, S_2) \in Z^m \times Z^m: \exists h\in H \textrm{ con } |L_{S_1}(h) - L_{S_2}(h)| \ge \frac{\varepsilon}{2}\}\\
 \end{align*}
 Entonces para $m\ge \frac{4}{\varepsilon^2}$ se verifica $\mprob(Q) \le 2 \cdot \mmprob(R)$. Veámoslo.
 
 Aplicando desigualdad de Hoeffding \ref{ineq:hoeffding}, sabemos que fijado $h\in H$ tenemos:
 \begin{equation}
  \mdosprob \left[|L_{\dist}(h) - L_{S_2}(h)| \le \frac{\varepsilon}{2} \right] \ge 
   (1 - 2e^{-m \varepsilon^2/2}) \underset{m\ge 4/\varepsilon^2}{\ge} (1 - 2e^{-2}) \ge \frac{1}{2}
  \label{eqn:s2bound}\tag{\textrm{$\dagger$}}
 \end{equation}

 Dado $h\in H$ verificando $|L_{\dist}(h) - L_{S_1}(h)| \ge, \varepsilon$ $|L_{\dist}(h) - L_{S_2}(h)| \le \frac{\varepsilon}{2}$
 entonces por desigualdad triangular tenemos $|L_{S_1}(h) - L_{S_2}(h)| \ge \frac{\varepsilon}{2}$. Así:
 \begin{align}
  \mmprob(R) &\ge \mmprob \bigg[ \exists h \in H: |L_{\dist}(h) - L_{S_1}(h)| 
              \ge \varepsilon, |L_{\dist}(h) - L_{S_2}(h)| \le \frac{\varepsilon}{2} \bigg] \nonumber\\
             &\underset{\eqref{eqn:s2bound}}{\ge} \frac{\munoprob(Q)}{2} = \frac{\mprob(Q)}{2}
 \label{eqn:Qbound}\tag{\textrm{$\ast$}}
 \end{align} 

 Sea en lo que sigue $S = (S_1, S_2) \sim \dist^{2m}$.
 
 Consideramos ahora un conjunto de permutaciones $\Gamma$ sobre $\{1, \ldots, 2m\}$ verificando que para todo $i\in \{1,\ldots, m\}$
 entonces o bien $\tau(i) = i+m, \tau(i+m) = i$ o bien $\tau(i) = i, \tau(i+m) = i$. Es decir son permutaciones que intercambian
 (o no) posiciones de $S_1$ y $S_2$. Asignaremos a $\Gamma$ una distribución uniforme $U^{\Gamma}$ y llamaremos 
 $P_{\tau} := \underset{\tau \sim U^{\Gamma}}{P}$. Se verificará: 
 \begin{equation}
  \dosmprob(R) = \dosmexpect \bigg(P_{\tau} [\tau(S) \in R] \bigg) \le \max_{S\in Z^{2m}} P_{\tau}[\tau(S) \in R]  
  \label{eqn:Rbound}\tag{\textrm{$\ast$$\ast$}}
 \end{equation}

  Veámoslo. Usando que $S$ es una muestra i.i.d. escogida desde $\dist^{2m}$:
  \[
    \dosmprob(R) = \dosmprob[S \in R] = \mprob[\tau(S) \in R], \quad \forall \tau \in \Gamma
  \]
  
  Por otro lado, fijada $\tau \in \Gamma$:
  \begin{align*}
  \dosmprob(R) &= \dosmexpect (\mathds{1}_R(S)) = \dosmexpect (\mathds{1}_R(\tau(S)))
    = \frac{1}{|\Gamma|} \sum_{\tau \in \Gamma} \dosmexpect (\mathds{1}_R(\tau(S))) = \\
    &\underset{\textrm{(linealidad de $\expect$)}}{=} \dosmexpect \bigg( \frac{1}{|\Gamma|} 
    \sum_{\tau \in \Gamma} [\mathds{1}_R(\tau(S))]\bigg) = \dosmexpect \bigg(P_{\tau} [\tau(S) \in R] \bigg)
  \end{align*}

  Ahora la desigualdad $\dosmexpect \bigg(P_{\tau} [\tau(S) \in R] \bigg) \le \max_{S\in Z^{2m}} P_{\tau}[\tau(S) \in R]$
  es trivial sin más que asegurar que existe el máximo en $S\in Z^{2m}$ de $P_{\tau}[\tau(S) \in R]$ puesto que $P_{\tau}$
  toma finitos valores sobre su $\sigma$ álgebra correspondiente.
 
 Por último veamos que:
 \begin{equation}
  \max_{S\in Z^{2m}} P_{\tau}[\tau(S) \in R] \le 2\Pi_H(2m) e^{-\varepsilon^2 m/8}
  \label{eqn:permbound}\tag{\textrm{$\ast$$\ast$$\ast$}}
 \end{equation}

  Sea $S = \{(x_1, y_1), \ldots (x_{2m}, y_{2m})\}$ un conjunto de entrenamiento que maximice $P_{\tau}[\tau(S) \in R]$. 
  Llamando $C=\{x_1, \ldots x_{2m}\}$, entonces $H_C = \{h_1, \ldots, h_t\}$, con $t\le \Pi_H(2m)$, por definición de $\Pi_H$.

  Dado $\tau \in \Gamma$, entonces $\tau(S) \in R$ sii para algún $h\in H_C$ se tiene (con $l$ función de pérdida $0-1$):
  \[
    \frac{1}{m} \bigg|\sum_{i=1}^m l\big(h,(x_{\tau(i)}, y_{\tau(i)}) \big) - 
    \sum_{i=m+1}^{2m} l\big(h,(x_{\tau(i)}, y_{\tau(i)})\big) \bigg| \ge \frac{\varepsilon}{2}
  \]
      
  Fijado $h_j \in H_C$ llamamos $u_k^j = l \big(h_j, (x_k,y_k) \big)$ para $k=1, \ldots, 2m$, $j=1, \ldots, t$.

  Se tiene que $u_{\tau(i)}^j - u_{\tau(m+i)}^j$ vale o $u_i^j - u_{m+1}^j$ o $u_{m+1}^j - u_i^j$ con igual probabilidad,
  por haber asignado sobre $\Gamma$ una distribución uniforme. Por tanto considerando una distribución uniforme $U^{\pm}$ 
  sobre $\{1,-1\}^m$.
  \[
    P_{\tau} \left(\frac{1}{m} \bigg|\sum_{i=1}^m (u_{\tau(i)}^j - u_{\tau(i+m)}^j) \bigg| \ge \frac{\varepsilon}{2} \right) = 
    \underset{(\beta_1, \ldots, \beta_m) \sim U^{\pm}}{P} \left(\frac{1}{m} \bigg|\sum_{i=1}^m (u_i^j - u_{i+m}^j)\beta_i \bigg| \ge 
    \frac{\varepsilon}{2} \right)
  \]
    
  Dado $\beta = (\beta_1, \ldots \beta_m) \in \{1,-1\}^m$, entonces si $\sum_{i=1}^m (u_i^j - u_{i+m}^j)\beta_i = \alpha$,
  tomando $-\beta$, se tiene que $\sum_{i=1}^m (u_i^j - u_{i+m}^j)(-\beta_i) = -\alpha$. Por tanto podemos 
  agrupar las permutaciones de dos en dos, con valores opuestos, luego:
  \[
    \underset{\beta \sim U^{\pm}}{\expect} \left[\frac{1}{m}\sum_{i=1}^m (u_i^j - u_{i+m}^j)\beta_i \right] = 0
  \]

  Estamos en condiciones de aplicar desigualdad \ref{ineq:hoeffding} de Hoeffding, con $W_i = (u_i^j -u_{i+m}^j)\cdot \beta_i$,
  donde $-1 \le W_i \le 1$. Deducimos:
  \[
    \underset{\beta \sim U^{\pm}}{P} \left(\frac{1}{m} \bigg|\sum_{i=1}^m (u_{i}^j - u_{i+m}^j)\beta_i \bigg| \ge 
    \frac{\varepsilon}{2} \right) \le 2 e^{-2m (\epsilon/4)^2} = 2 e^{-\epsilon^2 m /8}
  \]

  Como habíamos fijado un $h_j\in H_C$, 
  \begin{align*}
  P_{\tau} [\tau(S) \in R] &= P_{\tau} \left[\exists h_j \in H_C: \frac{1}{m} : 
  \bigg|\sum_{i=1}^m (u_{\tau(i)}^j - u_{\tau(i+m)}^j) \bigg| \ge \frac{\varepsilon}{2} \right] \\
  &\le 2 |H_C| e^{-\varepsilon^2 m/8} \le 2\Pi_H(2m) e^{-\varepsilon^2 m/8}
  \end{align*}

  En definitiva, uniendo \eqref{eqn:Qbound}, \eqref{eqn:Rbound} y \eqref{eqn:permbound} hemos probado que:
  \[
    \mprob \bigg[|L_S(h) - L_{\dist}(h)| \ge \varepsilon \bigg] \le 4 \Pi_H(2m) e^{- \varepsilon^2 m/8}
  \]
  para $m \ge \frac{4}{\varepsilon^2}$, que era condición suficiente para que se verificara \eqref{eqn:Qbound}.
  
  Ya estamos en condiciones de probar \eqref{th:fundvi} $\Rightarrow$ \eqref{th:fundi}. Sea $VC(H) = d < \infty$. 
  Por lema \ref{lemma:sauer} de Sauer se tiene $\Pi_H(2m) \le (2m)^d + 1 \le 2(2m)^d$. Podemos suponer s.p.g. que $d > 0$.
  \[
    \mprob \bigg[|L_S(h) - L_{\dist}(h)| \ge \varepsilon \bigg] 
    \le 4 \Pi_H(2m) e^{-\varepsilon^2 m/8} \le 8 (2m)^d e^{-\varepsilon^2 m/8}
  \]
 
  Necesitamos $8 (2m)^d e^{-\varepsilon^2 m/8} \le \delta$. Tomando logaritmos en ambos miembros deducimos:
  \begin{align*}
    \log(8) + d\log(2m) \le \log(\delta) + \frac{2\varepsilon^2 m}{16} \Leftrightarrow \\
    \Leftrightarrow \underbrace{\frac{16}{\varepsilon^2} \bigg(log(8) - log(\delta) \bigg)}_b + 
    \underbrace{\frac{16 d}{\epsilon^2}}_a \log(2m) \le \underbrace{2m}_x
  \end{align*}
  
  $a \ge \frac{1}{2}$ por ser $d \ge 1$, y podemos aplicar el lema \ref{lemma:boundlog}, que nos asegura que si 
  $2m \ge 4a \log(2a) + 2b$, se verifica la anterior desigualdad. Es decir, nos vale:
  \[
    m_{H}^{CU}(\epsilon, \delta) \le \left\lceil\frac{32 d}{\epsilon^2} \log\left(\frac{32 d}{\epsilon^2}\right) + 
    \frac{16}{\epsilon^2} \log\left(\frac{8}{\delta}\right) \right\rceil
  \]
 
\end{proof}

