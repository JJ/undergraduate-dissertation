\section{Dimensión Vapnik-Chervonenkis}
\subsection{Introducción}
La abreviaremos dimensión VC

\begin{definition}
\textbf{Restricción de $H$ a $C$}

Sea $H$ clase de hipótesis de $X$ a $\{0,1\}$, y $C=\{c_1, \ldots c_m\} \subseteq X$. 
Llamamos restricción de $H$ a $C$ al conjunto de funciones:

\[H_{C} = \{h_{|C} : h\in H\} \cong \{(h(c_1), \ldots h(c_m)): h\in H\}\]
\end{definition}


\begin{definition}
\textbf{Restricción de $X$ a $H$}

Sea $H$ clase de hipótesis de $X$ a $\{0,1\}$, y $C=\{c_1, \ldots c_m\} \subseteq X$. Llamamos restricción de $X$ por $H$ a:

\[X_{H} = \{S \subseteq X: \exists h\in H, h(S)=\{1\} \}\]
\end{definition}


\begin{definition}
\textbf{Conjunto fragmentado por otro}

Un conjunto $\mathcal{F}$ diremos que fragmenta a otro conjunto finito $C$ si se verifica que para todo subconjunto de $D \subseteq C$ existe $S\in \mathcal{F}$ con $S \cap C = D$
\end{definition}


\begin{definition}
\textbf{Conjunto fragmentado por una clase de hipótesis}

Una clase de hipótesis $H$ fragmenta un conjunto finito $C \subseteq X$ sii la restricción de $H$ a $C$ nos da todas las posibles funciones de $C$ a $\{0,1\}$. Esto es, si $|H_{C}| = 2^{|C|}$

\begin{lemma}
\textbf{Caracterización del concepto de fragmentación de $C$ por $H$}

Una clase de hipótesis $H$ fragmenta un conjunto finito $C \subseteq X$ sii $X_{H}$ fragmenta $C$
\end{lemma}

\begin{proof}
La demostración de la caracterización es trivial desde la biyección:

\[\{h_{|C} : h\in H\} \cong \{(h(c_1), \ldots h(c_m)): h\in H\}\]

con $C = \{c_1, \ldots c_m\}$
\end{proof}

Este lema nos permite trabajar indistintamente con la fragmentación de un conjunto por una clase de funciones o por la restricción del espacio por dicha clase de hipótesis.

Cuando demostrábamos el teorema de No Free Lunch, no teníamos ninguna restricción sobre la distribución que construíamos ni sobre la hipótesis que daba lugar a esa distribución, la $f$ que cumplía que tenía error nulo. Siempre que el conjunto $C$ que tomamos sea fragmentado por $H$, podremos asegurar que la $f$ que genera la distribución pertenece a la clase de funciones $H$. Formalmente:

\begin{theorem}
\textbf{Teorema de No Free Lunch revisitado}

Sea $H$ una clase de hipótesis de $H$ a $\{0,1\}$, $m < |X|/2$ el tamaño del conjunto de entrenamiento. Supongamos que existe $C\subseteq X$ de tamaño $2m$ fragmentado $H$. Sea $A$ cualquier algoritmo de aprendizaje, entonces existe una distribución $\mathcal{D}$ sobre $X \times \{0,1\}$ verificando:

\begin{enumerate}
\item Existe una función $f: X \rightarrow \{0,1\}$, $f\in H$ con $L_{\mathcal{D}}(f)=0$
\item $P_{S\sim \mathcal{D}^m} [L_{\mathcal{D}} (A(S)) \ge 1/8] \ge 1/7$
\end{enumerate}

\label{nofreelunch-v2}
\end{theorem}

Intuitivamente, si existe un conjunto $C$ fragmentado por $H$ y nuestro conjunto de entrenamiento contiene la mitad de instancias de $C$ (recordemos que la distribución que construíamos en la demostración de No Free Lunch asignaba toda la masa de probabilidad al conjunto $C$), entonces no tenemos información suficiente para etiquetar correctamente el resto de instancias (hay demasiadas posibles hipótesis que etiquetan el conjunto de entrenamiento de igual forma pero difieren en el resto de instancias).

\#+begin$_{\text{definition}}$
\textbf{Dimensión VC}

Definimos la dimensión VC de una clase de hipótesis $H$ como el tamaño máximo de los conjuntos $C \subseteq X$ verificando que son fragmentados por $H$. Si no existe máximo, decimos que $H$ tiene dimensión VC infinita. La notamos $VC(H)$
\end{definition}


Del teorema No Free Lunch revisitado deducimos:

\begin{theorem}
\textbf{Ser PAC cognoscible implica tener dimensión VC finita}

Sea $H$ clase de hipótesis con $VC(H) = \infty$. Entonces $H$ no es PAC cognoscible
\end{theorem}

\subsection{Ejemplos}

\subsubsection{Intervalos $]-\infty, a[$}

Sea $H = \{h_a = \mathds{1}_[x<a]: a\in \mathbb{R}\}$ clase de hipótesis sobre $\mathbb{R}$. 

Dado un conjunto $C=\{\alpha\}$, podemos tomar $h_{\alpha}$ y $h_{\alpha+1}$, que nos dan todos los posibles etiquetados de $C$.
Sin embargo, dado un conjunto de tamaño 2, $C=\{\alpha, \beta\}$, donde podemos suponer spg. $\alpha < \beta$. Entonces no podemos encontrar $h_b \in H$ verificando $h_b(\alpha)=0$ y $h_b(\beta) = 1$, ya que esto implicaría que $b > \beta$ y por tanto entraría en contradicción con que $h_b(\alpha) = 0$

Hemos probado $VCdim(H) = 1$.

\subsubsection{Intervalos cerrados y acotados}

Sea $H = \{h_{a,b} = \mathds{1}_[a<x<b]: a,b\in \mathbb{R}\}$ clase de hipótesis sobre $\mathbb{R}$. 

Dado un conjunto $C=\{\alpha\, \beta\}$, con $\alpha < \beta$, las hipótesis $h_{\alpha+\delta_1, \beta + \delta_2}$ con $\delta_i \in \{0,1\}$ nos dan todos los posibles etiquetados de $C$.
Sin embargo, dado un conjunto de tamaño 3, $C=\{\alpha, \beta\, \theta\}$, donde podemos suponer $\alpha < \beta < \theta$. Entonces no podemos encontrar $h_b \in H$ verificando $h_b(\alpha)=1$ y $h_b(\theta) = 1$ y $h_b(\beta) = 0$

Hemos probado $VCdim(H) = 2$.

\subsubsection{Clases finitas}

Sea $H$ una clase finita. Entonces para un conjunto $C \subseteq H$ se tiene $|H_C| \le |H|$ y por tanto el 
conjunto no puede ser fragmentado por $H$ si $|H| < 2^{|C|}$, lo que implica $VCdim(H) \le log_2(|H|)$

\subsubsection{Dimensión VC y número de parámetros}

Puede demostrarse que la dimensión VC de los clasificadores de rectángulo $H = \{h_{a,b,c,d} := \mathds{1}_{[a\le x\le b, c\le y\le d]}\}$ en $\mathbb{R}^2$, que ya mencionamos en un ejemplo en los temas anteriores es 4. Esto, unido a los ejemplos anteriores con intervalos podría hacernos conjeturar que la dimensión VC depende del número de parámetros con el que definimos los clasificadores. El siguiente ejemplo demuestra que esto es falso.

Dada la clase de clasificadores $H = \{h_{\theta}: \theta \in \mathbb{R}\}$ donde 
$h_{\theta}: X \rightarrow {0,1}$ está definida por $h_\theta (x) = \lceil 0.5 sen(\theta x) \rceil$, y 
$d\in \mathbb{N}$ arbitrario, podemos tomar $d$ puntos codificados por $x_i = 0.x_{1,j} \ldots x_{2^d,j} x^{2^d + 1,j}$ 
donde cada $x_{i}$ es una fila de la matriz $(x_{i,j})$ donde la columna $2^{d+1}$ ésima es 1, y la columna 
$i$ -ésima codifica el número $i-1$ en binario, leído de arriba a abajo. Así dado una asignación de $d$ 
etiquetas, debe codificar un número en binario entre $1$ y $2^d-1$, a saber, la columna $k$ ésima de la 
matriz. Tomamos el clasificador $h = \lceil 0.5 sen(10^k \pi x) \rceil$ que verificará que su asignación de 
etiquetas es justamente la columna $k$ ésima. El sentido de la última columna constantemente $1$ puede 
explicarse en que daremos $x_{i,1} \ldots x_{i,k}$ medias vueltas a la circunferencia unidad y recorreremos 
y una fracción $0.x_{i,k+1} \ldots x_{2^d,j} 1$ no nula de otra media vuelta a la circunferencia. Si 
$x_{i,k} = 1$ entonces $h(x_i) = 1$, y si $x_{i,k}=0$ entonces $h(x_i) = 0$. Luego $VCdim(H) = \infty$.

\subsection{Teorema fundamental de aprendizaje PAC}

\begin{theorem}
\textbf{Teorema fundamental de aprendizaje PAC}

Sea $H$ clase de hipótesis de la forma $h: X \rightarrow \{0,1\}$ y la función de pérdida 0-1. Entonces equivalen:

\begin{enumerate}
\item $H$ tiene la propiedad de convergencia uniforme.
\item $H$ es agnósticamente PAC cognoscible por cuaquier algoritmo ERM.
\item $H$ es agnósticamente PAC cognoscible.
\item $H$ es PAC cognoscible.
\item $H$ es PAC cognoscible por cualquier algoritmo ERM.
\item $VC (H) < \infty$.
\end{enumerate}
\end{theorem}


La implicación que nos falta es $6 \implies 1$. El resto de implicaciones se consiguen a partir de teoremas ya probados en temas anteriores.

Daremos una serie de lemas y definiciones previas antes de probarla.

\begin{definition}
\textbf{Función de crecimiento}

Sea $H$ una clase de hipótesis. Definimos como función de crecimiento de $H$:

\[\begin{array}{ll}
\tau_{H}: & \mathbb{N} \rightarrow \mathbb{N}\\
                    & m          \mapsto     \underset{C \subseteq X: |C|=m}{max}{|H_C|}
\end{array}\]

Esta función está bien definida puesto que fijado $m \in \mathbb{N}$ se tiene siempre que $|H_C| \le 2^C$
\end{definition}

\begin{lemma}
\textbf{Lema de Sauer-Shelah}

Sea $H$ clase de hipótesis con $VC(H) \le d < \infty$. Entonces para todo $m\in \mathbb{N}$ se tiene $\tau_{H} (m) \le \sum_{i=0}^d \binom{m}{i}$. Se deduce que si $m > d+1$ entonces $\tau_{H}(m) \le (em/d)^d$.
\end{lemma}


\begin{proof}
Empezamos probando que una clase de hipótesis $\mathcal{F}$ finita fragmenta al menos $|\mathcal{F}|$ conjuntos.

Lo hacemos por inducción sobre el tamaño de $X_\mathcal{F}$ que es un conjunto finito por ser $\mathcal{F}$ clase finita.

Si su tamaño es 1, parte al conjunto vacío.

Supuesto que se verifica la hipótesis para tamaños menores que $k-1$ y sea $|X_\mathcal{F}| = k$. Escogemos entonces $x\in X$ verificando que $x$ pertenece a algunos conjuntos de $X_\mathcal{F}$ pero no a todos (debe existir, sino tendríamos que $X_\mathcal{F}$ contiene un único conjunto).

Sean $A = \{S \subseteq X_\mathcal{F} : x\in S\}$, $A'=\{S\setminus\{x\} : S \in A\}$, $B= \{S \subseteq X_\mathcal{F} : x\not\in S\}$.

Claramente $|A| = |A'|$ y por hipótesis de inducción $A'$ fragmenta $k \ge |A|$ conjuntos, y $B$ fragmenta $|B|$ conjuntos. También es trivial ver que los conjuntos $S$ y $S\cup \{x\}$ son fragmentados por $A$ donde $S$ es un conjunto fragmentado por $A'$. Hemos probado que $A$ fragmenta $|A| + k$ conjuntos, y si un conjunto es fragmentado por $A$ y $B$ a la vez, entoces no contiene a $x$, luego es fragmentado por $A'$ y por $B$ a la vez, y podremos tener a lo sumo $k$ conjuntos de este tipo.

En definitiva hemos probado que fragmentamos $|A| + k + |B| - k = |A| + |B| = |\mathcal{F}|$ conjuntos.
Probado esto, si $\tau_{H}(m) > \sum_{i=0}^d \binom{m}{i}$ entonces $H$ debe fragmentar un conjunto de tamaño 
$d+1$ al menos, puesto que el número de subconjuntos de un conjunto finito $C$ menor que  $d+1$ es exactamente
$\sum_{i=0}^d \binom{|C|}{i}$. Luego tendríamos $VC(H) > d$
\end{proof}