\section{Algoritmos RACOG y wRACOG}
Sea un subconjunto $\spos$ de ejemplos de clase positiva de un conjunto de entrenamiento, donde cada $(x,y)\in \spos$ 
verifica que $x = (w_1, \ldots, w_d)$ es $d-$dimensional. Los algoritmos RACOG y wRACOG parten de la base de que lo que vamos
a aproximar es una distribución discreta, y podemos obtener instancias siguiendo una distribución 
conjunta $P(W_1, \ldots, W_d)$.

El problema es que calcular la anterior distribución es muy costoso, dado que tendríamos un número de posibles valores para 
cada instancia de:
\[
  |\{\textrm{Posibles valores para }W_1\}| \cdots |\{\textrm{Posibles valores para} W_d\}|
\]

Si consiguiéramos aproximar $P(W_1, \ldots, W_d)$ como producto de distribuciones marginales, esto es, 
$P(W_1, \ldots W_d) \approx \prod_{i=1}^d P(W_i \mid W_{n(i)})$ donde cada $n(i) \in \{1, \ldots, d\}$,
entonces calcular dicha distribución sería menos costoso. Para expresar $P(W_1, \ldots W_d)$ como producto de marginales,
es decir, para decidir qué valor le damos a cada $n(i)$, se usa el algoritmo \ref{alg:chowliu} de Chow-Liu,
que minimiza la distancia de Kullback-Leibler entre dos distribuciones. Esta distancia proporciona una medida de la 
aproximación de una distribución de probabilidad $Q$ a la verdadera distribución $P$, usando teoría de la información. 
Está definida como la esperanza de la diferencia logarítmica entre ambas distribuciones:
\[
  D_{KL}(P \parallel Q) = \sum_{i} P(i) \left(\log P(i) - \log Q(i)\right)
\]

Debemos recordar que la definición de información mutua para dos variables aleatoria $W_i, W_j$, discretas, 
se define como:
\[
  I(W_i, W_j) = \sum_{w_1\in W_1} \sum_{w_2\in W_2} p(w_1, w_2) \log\left(\frac{p(w_1,w_2)}{p(w_1) p(w_2)}\right)
\]

\begin{algorithm}[H]
\begin{algorithmic}[1]
 \REQUIRE $S = \{x_i=(w_1^{(i)}, \ldots w_d^{(i)})\}_{i=1}^m$, instancias
 \FOR{Cada pareja $i, j$}
   \STATE{Calcular la información mutua $I(w_i, w_j)$}
 \ENDFOR
 \STATE{Construir $G$ el árbol generador de peso máximo mediante el algoritmo de Kruskal}
 \RETURN{$G$}
\end{algorithmic}
\caption{Algoritmo de Chow-Liu de construcción de un árbol maximizando la suma de información
mutua de los arcos}
\label{alg:chowliu}
\end{algorithm}

Una vez construido el árbol, es fácil, mediante el siguiente algoritmo obtener un grafo dirigido, en que a
cada nodo (variable en la distribución), tiene un único padre. Lo hacemos recorriendo el árbol desde la raíz
y actualizando el conjunto de nodos que pueden ir siendo padres de otros nodos, $H$.

\begin{algorithm}[H]
\begin{algorithmic}[1]
 \REQUIRE $G = (E,V)$ grafo no dirigido
 \STATE{Inicializar $visitados = \emptyset$}
 \STATE{Inicializar $G'=(E',V)$, con $E'=\{(p,q)\}$ con $(p,q) \in E$ arbitrario. $p$ será la raíz del árbol}
 \STATE{Hacer $E = E\setminus\{(p,q)\}$ y $H=\{p,q\}$ conjunto de padres}
 \WHILE{Mientras $E\neq \emptyset$ y $H\neq \emptyset$}
   \STATE{Toma $h\in H$, y actualiza $H = H\setminus \{h\}$}
   \STATE{Calcula $J = \{(u,v)\in E: u=h \vee v=h\}$}
   \STATE{Calcula $J' = \{(u,v) : [(u,v)\in J, u=h] \vee [(v,u)\in J, u=h]\}$}
   \STATE{$E = E\setminus J$}
   \STATE{$E'= E'\cup J'$}
   \STATE{$H = H \cup \{v : (u,v) \in J'\}$}
 \ENDWHILE
 \NEWLINE
 \RETURN{$G'$}
\end{algorithmic}
\caption{Construcción del grafo a partir del árbol de Chow-Liu}
\label{alg:make-directed}
\end{algorithm}

Por tanto queda un algoritmo para aproximar la distribución de los ejemplos positivos tal que así:

\begin{algorithm}[H]
\begin{algorithmic}[1]
  \REQUIRE $S = \{x_i=(w_1^{(i)}, \ldots, w_d^{(i)})\}_{i=1}^m$, instancias
  \STATE{Calcular $G' = (E',V')$ árbol de dependencia según Chow Liu}
  \STATE{Construir $G$ un grafo no dirigido con algoritmo \ref{alg:make-directed} desde $G'$,
  donde $E$ son los arcos, $r$ la raíz. Definimos $P(W_r|n(r)) := P(W_r)$}
  \FOR{$(u,v) \in E$}
    \STATE{Hacer $n(v) = u$}
    \STATE{Calcular $P(W_v \mid W_u)$}
    \STATE{Calcular $P(W_u), P(W_v)$}
  \ENDFOR
  \RETURN{$\{P(W_v \mid W_u), P(W_u), P(W_v)\}_{(u,v)\in E}$}  
\end{algorithmic}
\caption{Algoritmo AproximarDistribución}
\label{alg:aproxdist}
\end{algorithm}

Nótese que tenemos la distribución almacenada como marginales, y que por tanto necesitamos aún una forma de extraer
muestras desde ella. Se usará para la construcción de ejemplos mediante dicha distribución un método de Monte 
Carlo llamado GibbsSampler. Se parte de cada ejemplo, y se construye un nuevo ejemplo basado en el anterior y 
la distribución conjunta aproximada. El método de Monte Carlo con dicho algoritmo consiste en ir construyendo 
$m$ cadenas de Markov, una para cada instancia, de manera que iteramos una vez cada vez que llamamos al método.


\begin{algorithm}[H]
\begin{algorithmic}[1]
  \REQUIRE $S = \{x_i=(w_1^{(i)}, \ldots, w_d^{(i)})\}_{i=1}^m$, instancias
  \REQUIRE $\{P(W_v \mid W_u), P(W_u)\}_{(u,v)\in E}$
  \FOR{$i=1, \ldots, m$}
    \FOR{$k=1,\ldots, d$}
      \STATE{$\bar{w}_k^{(i)} \sim P(W_k \mid \bar{w}_1^{(i)}, \ldots, \bar{w}_{k-1}^{(i)}, w_{k+1}^{(i)} \ldots, w_{d}^{(i)})$}
    \ENDFOR
  \ENDFOR
  \RETURN{$S = \{\bar{x}_i=(\bar{w}_1^{(i)}, \ldots \bar{w}_d^{(i)})\}_{i=1}^m$, conjunto de instancias\\ generado desde $S$ y $P$}
\end{algorithmic}
\caption{Algoritmo GibbsSampler}
\label{alg:gibbs}
\end{algorithm}

\imgcaption{imgs/monte-carlo.png}{Cadena de Markov obtenida por GibbsSampler}{0.5}


Nótese además que fijado $k\in \{1, \ldots, d\}$, tenemos: 
\[
  P(W_k \mid \bar{w}_1^{(i)}, \ldots, \bar{w}_{k-1}^{(i)}, w_{k+1}^{(i)} \ldots, w_{d}^{(i)}) = 
  \frac{P(W_k, \bar{w}_1^{(i)}, \ldots, \bar{w}_{k-1}^{(i)}, w_{k+1}^{(i)} \ldots, w_{d}^{(i)})}
      {P(\bar{w}_1^{(i)}, \ldots, \bar{w}_{k-1}^{(i)}, w_{k+1}^{(i)} \ldots, w_{d}^{(i)})}
\]

Como el denominador es constante porque $\bar{w}_1^{(i)}, \ldots, \bar{w}_{k-1}^{(i)}, w_{k+1}^{(i)} \ldots, w_{d}^{(i)}$
son valores fijos para cualquier valor que pueda tomar $W_k$, no nos hace falta calcularlo. Además, como el
numerador lo calculamos como $\prod_{i=1}^d P(W_i \mid W_{n(i)})$, también encontramos una serie de términos 
multiplicando que no debemos calcular, todos aquellos que cumplan $i\neq k \wedge n(i) \neq k$.

Una última observación: el cálculo de $P(W_k \mid W_{n(k)} = w_{n(k)})$ es directo. Pero el cálculo de 
$P(W_k = w_k \mid W_{n(k)})$ donde $n(i) = d$ debe hacerse usando el teorema de Bayes:
\[
  P(W_i = w_i \mid W_{n(i)} = a) = \frac{P(W_{n(i)} = a \mid W_i = w_i)}{P(W_{n(i)} = a)} \cdot P(W_i = w_i)
\] 

\subsection{RACOG}
RACOG (\textit{Rapidly Converging Gibbs}) consiste en ir construyendo una cadena de Markov para cada una de 
las $m$ instancias minoritarias, de manera que descartamos las $\beta$ primeras instancias producidas para 
cada ejemplo del conjunto positivo, y a partir de entonces cada $\alpha$ iteraciones se va escogiendo una. 
Esto permite, según \citep{das2015}, perder dependencia de los valores iniciales en la cadena 
(parámetro $\beta$ o \textit{burnin}), así como reducir la dependencia en la generación de sucesivas instancias 
(parámetro $\alpha$ o \textit{lag}).

\begin{algorithm}[H]
\begin{algorithmic}[1]
  \REQUIRE $S = \{x_1, \ldots x_m\}$, ejemplos positivos
  \REQUIRE $\beta$, burnin
  \REQUIRE $\alpha$, lag
  \REQUIRE $T$, número de instancias sintéticas a generar
  \STATE{$P = \textrm{AproximarDistribución}(S)$}
  \STATE{$S'= \emptyset$}
  \STATE{$M = \left\lceil\frac{T}{m}\right\rceil \cdot \alpha + \beta$}
  \NEWLINE
  \FOR{$t=1,\ldots, M$}
    \STATE{$S = \textrm{GibbsSampler}(S, P)$}
    \NEWLINE
    \IF{$t > \beta$ \AND $t\mod(\alpha) = 0$}
      \STATE{$S' = S' \cup S$}
    \ENDIF
  \ENDFOR
  \NEWLINE
  \STATE{$S' =$ Escoger $T$ instancias aleatorias de entre $S'$}
  \RETURN{$S'$, ejemplos positivos sintéticos}    
\end{algorithmic}
\caption{Algoritmo de \textit{oversampling} RACOG}
\label{alg:racog}
\end{algorithm}

Traducido a código:

\lstinputlisting[language=R, caption = Cuerpo del algoritmo RACOG]{codelst/racog.R}

\subsection{wRACOG}
El algoritmo RACOG presenta un problema: depende de los parámetros de \textit{burnin}, \textit{lag} y
es el usuario quien decide el número de instancias que desea. El algoritmo wRACOG (\textit{wrapper-based RACOG}),
por el contrario, encapsula las iteraciones de GibbsSampler de manera que se proporcionan un conjunto de 
entrenamiento y otro de validación. A cada iteración, se efectúa una iteración del GibbsSampler, se añaden 
las instancias mal clasificadas por el modelo obtenido por un \textit{wrapper} de entre las muestras sintéticas
al conjunto de sintéticas, y se genera un nuevo \textit{train} y un nuevo modelo. Se deja de iterar 
cuando se verifica un criterio sobre el conjunto de validación.

\begin{algorithm}[H]
\begin{algorithmic}[1]
  \REQUIRE $S_{train} = \{z_1=(x_1, y_1), \ldots, z_m=(x_m, y_m)\}$, conjunto de \textit{train}
  \REQUIRE $S_{val}$, conjunto de validación
  \REQUIRE $wrapper$, clasificador
  \REQUIRE $T$, número de iteraciones a considerar
  \REQUIRE $\alpha$, parámetro de tolerancia
  \STATE{$S = \spos_{train}$}
  \STATE{$P = \textrm{AproximarDistribución}(S)$}
  \STATE{Obetener $modelo$ con $wrapper$ y $S_{train}$}
  \STATE{Inicializar nuevas muestras $S'= \emptyset$}
  \STATE{Inicializar $\tau = (\underset{1)}{+\infty}, \ldots, \underset{T)}{+\infty})$}
  \NEWLINE
  \WHILE{Desviación estándar de $\tau \ge \alpha$}
    \STATE{$S = \textrm{GibbsSampler}(S, P)$}
    \STATE{$S_{misc} =$ instancias mal clasisicadas de $S$ clasificadas por $modelo$}
    \STATE{Actualizar nuevas instancias, $S' = S' \cup S_{misc}$}
    \STATE{Actualizar \textit{train}, $S_{train} = S_{train} \cup S_{misc}$}
    \STATE{Obetener $modelo$ con $wrapper$ y $S_{train}$}
    \STATE{Hacer $s = $ sensibilidad de la predicción de $modelo$ sobre $S_{val}$}
    \STATE{Hacer $\tau = (\tau_2, \ldots, \tau_T, s)$}
  \ENDWHILE
  \NEWLINE
  \RETURN{$S'$, ejemplos positivos sintéticos}    
\end{algorithmic}
\caption{Algoritmo de \textit{oversampling} wRACOG}
\label{alg:wracog}
\end{algorithm}

\subsection{Críticas a los algoritmos}
Existe una crítica clara a los algoritmos: al aproximar distribuciones discretas, no se puede esperar que tengan buen 
comportamiento sobre atributos continuos.

Asimismo, como requieren acceso a la distribución precalculada, deberíamos tener alguna forma de tener acceso $\algcomp{1}$ a
los valores para dicha distribución. El problema es que \R, el lenguaje donde se ha implementado, no proporciona estructura
de tablas \textit{hash}, a pesar de existir un paquete \footnote{\url{https://cran.rstudio.com/web/packages/hash/}} que 
las simula, pero que no proporciona acceso a datos $\algcomp{1}$. Esto supone que la implementación no puede hacerse eficiente
a no ser que programemos los algoritmos \ref{alg:aproxdist}, \ref{alg:racog} y \ref{alg:gibbs} en otro lenguaje que se llame
desde \R. El problema con esta última aproximación hubiera sido que \textit{wrapper} en el algoritmo \ref{alg:wracog} es
una función que se le pasa al método y no podríamos mezclar dos lenguajes en el bucle principal.


%\lstinputlisting[language=R, caption = Cuerpo del algoritmo wRACOG]{codelst/wracog.R}

