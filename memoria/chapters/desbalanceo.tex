\section{Problema de desbalanceo}

Se considerará en toda la sección de informática clasificación binaria, donde tenemos un dominio $X$, y un conjunto de etiquetas 
$Y=\{-1,1\}$. Llamamos $Z = X\times Y$. También consideraremos que los datos siguen una distribución dada por una función de 
probabilidad prefijada pero no conocida por el algoritmo, a saber, $\dist=(\Sigma, P)$. 

Por simplicidad, podemos considerar siempre $\Sigma = \mathcal{P}(X)$, por lo que identificaremos en lo que sigue $\mathcal{D} \equiv P$.
El problema de clasificación binaria consiste en buscar, dado $S= \left\{(x_1, y_1), \ldots (x_m, y_m)\right\}$ un 
conjunto de entrenamiento, un clasificador de alguna clase $H$, $h$, verificando que $L_{P}(h)$ sea lo más pequeño posible.
Nótese que estamos relajando las hipótesis de \ref{sec:clas-binaria} al considerar la distribución de probabilidad como prefijada.

\begin{definition} \textbf{Instancias positivas, negativas}

Llamamos instancias positivas a $Z^{+} = \{(x,y)\in Z: y=1\}$.

LLamamos instancias negativas a $Z^{-} = \{(x,y)\in Z: y=-1\}$.

Una notación equivalente se aplica a $S$. Además llamamos $\ppos = P(Z_{+})$ y $\pneg = P(Z_{-})$.
\end{definition}


\begin{definition} \textbf{Desbalanceo entre clases}

 Decimos que un problema de clasificación binaria es desbalanceado (entre clases) si se verifica que tomando 
 $\nneg = |S^{-}|$ y $\npos = |S^{+}|$, entonces $\npos < \nneg$.
 
 Llamamos ratio de desbalanceo a $r(S)=\frac{\nneg}{\npos}$.
\end{definition}

Por convención se consideran problemas de desbalanceo donde $r(S) \gg 1$, tal y como se afirma en \cite{he2009}. Asimismo,
en estos problemas nos interesa especialmente mejorar el error de los clasificadores sobre la clase minoritaria o positiva.

En muchas ocasiones, al realizar análisis de datos no se tiene en cuenta el desbalanceo existente entre clases, lo
cual hace que los algoritmos no reflejen adecuadamente la distribución de las clases. Para ciertos conjuntos de datos,
donde el número de instancias positivas con respecto a las negativas es muy pequeño, un clasificador podría simplemente
etiquetar todas las instancias como negativas, y obtener un error global pequeño, pero no acertaría en ninguna instancia
positiva. Si los datasets empleados representaran enfermedades, por ejemplo, como en el caso de \cite{keelderma}, estaríamos
nos interesaría encontrar mecanismos para disminuir el error sobre la clase positiva (la de las instancias correspondientes
a la enfermedad).

El desbalanceo puede ser:

\begin{enumerate}[i]
  \item \textbf{Intrínseco}, cuando $\ppos < \pneg$, y por tanto la cantidad de datos de cada clase que extraemos viene 
  verdaderamente determinada por la distribución real de los datos.

  \item \textbf{Extrínseco}, si $\ppos \ge \pneg$ pero sin embargo para nuestro conjunto de entrenamiento $S$, 
  tenemos que $\npos < \nneg$. Esto quiere decir que el desbalanceo no viene determinado por la verdadera distribución de
  los datos, sino por factores como por ejemplo la complejidad para extraer la misma cantidad de datos de la misma clase.
\end{enumerate}

También podemos decir existe desbalanceo \textbf{intra clases} cuando además la clase minoritaria está repartida en varias 
regiones no conexas, a saber $\sneg \supset A_1, \ldots A_m$ donde $|A_1| \le \ldots \le |A_m|$, con algún menor o igual estricto.
Esto dificulta aún más la correcta clasificación de dichas instancias, porque aparecen zonas con instancias positivas 
pobremente representadas. A estas zonas de baja representación las llamamos instancias raras. Nótese que hay que distinguir
el concepto de instancia rara de otro concepto que surge en ciencia de datos: el concepto de ruido, instancias aisladas completamente.
   
La aparición de instancias raras no sólo se debe al desbalanceo de clases, sino que se puede deber a un amplio rango de factores 
que afectan a la complejidad de los datos: \textit{overlapping} ($\sneg_x \cap \spos_x \neq \emptyset$), falta de 
representatividad en los datos, \textit{small disjuncts}, etc. En general, los clasificadores intentan aprender a partir de una clase 
creando reglas disjuntas que agrupen a instancias. Como consecuencia de la infrarrepresentación de instancias, podemos tener
reglas que cubran una pequeña porción de las instancias de la clase positiva. Estas instancias conforman lo que llamamos 
un \textit{small-disjunct}. Los \textit{small-disjuncts} no sólo afectan a la clase minoritaria, sino que pueden darse también 
dentro de la mayoritaria, aunque la mayor densidad de datos de esta clase hace que el efecto no sea tan agravado o 
sea una situación menos frecuente. 

\imgcaption{./imgs/desbalanceo.png}{Ejemplo de dataset con desbalanceo}{0.85}

\section{Técnicas para aprendizaje con desbalanceo}
\subsection{Oversampling/Undersampling}
El \textit{oversampling} consiste en generar instancias sintéticas etiquetadas como positivas, $E$, normalmente 
respetando la distribución de $\spos$, intentando generar conexos a partir de instancias raras, etc. Se lleva a cabo la 
tarea de clasificación usando el conjunto de entrenamiento $S' = S\cup E$. 

El \textit{undersampling} consiste en escoger un subconjuto $E \subseteq \sneg$, de manera que $S' = S\setminus E$ 
y tenemos un problema de clasificación con el conjunto de entrenamiento $S'$. Normalmente $E$ se genera de manera que se
intentan eliminar instancias que aportan información redundante dentro del conjunto de instancias negativas.

Suponemos también que tanto el \textit{oversampling} como el \textit{undersampling} se realiza de manera que $r(S') < r(S)$.

\subsubsection{Limpieza de instancias en \textit{oversampling}}

\subsection{Aprendizaje \textit{cost sensitive}}
