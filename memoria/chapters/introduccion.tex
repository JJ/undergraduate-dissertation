El aprendizaje automático o ciencia de datos ha sido un área que en los últimos años, con el desarrollo de máquinas cada vez
más potentes, y ante la enorme acumulación de datos en prácticamente todos los ámbitos de la sociedad (redes sociales, venta de
productos, información de pacientes, \ldots) ha sufrido un importante auge.

\section{Motivación}
El objetivo del aprendizaje automático es convertir datos en conocimiento a través de un razonamiento inductivo, de manera que
proporcionándole datos a una máquina seamos capaces de extraer un conocimiento (una generalización de los datos que nos permita
inferir información). Surge la pregunta de por qué es necesario el aprendizaje automático o \textit{machine learning}, si la
estadística también se encarga de obtener conocimiento a partir de unos datos.

\subsection{¿Por qué necesitamos \textit{machine learning}?}
\begin{enumerate}[i]
\item Para resolver \textbf{tareas que requieren automatización}. Entran dentro de esta categoría tanto aquellas tareas para
las que no existe una axiomatización o un conocimiento exacto, como pueden ser el reconocimiento de dígitos o de voz, como 
aquellas tareas que requieren del análisis de un gran número de datos, y quedan fuera de la capacidad humana para realizar
un análisis estadístico manual. En el primer caso necesitamos apoyarnos en conocimiento auxiliar (por ejemplo, 
un conjunto de dígitos o de muestras de voz preetiquetadas con los que poder comparar muestras sin etiquetar/clasificar); 
en el segundo, se hace necsario el uso de una máquina para poder extraer conocimiento de todos los datos.

\item \textbf{Tareas que requieren adaptatividad}. Si cambian los datos de entrada, necesitamos que los algoritmos se readapten
a ellos, y no tengamos un conocimiento rígido, sino que pueda variar/mejorar en función de la entrada.
\end{enumerate}

\subsection{Áreas relacionadas con el aprendizaje}
Entre las áreas relacionadas con el aprendizaje automático, cabe mencionar:

\begin{enumerate}[i]
\item \textbf{Inteligencia Artificial}. Aprender de los datos, cuando tenemos gran cantidad es una tarea que 
claramente encaja con el uso de los ordenadores para ejecutar algoritmos que a un humano le sería imposible ejecutar a mano, o para
extraer relaciones sutiles entre los datos que al mismo se le escaparían con facilidad.

\item \textbf{Inferencia}. Entre las diferencias que podemos mencionar con la estadística convencional, destaca 
la necesidad de programar las tareas, dado el volumen de datos con el que normalmente se trabaja, mientras que en muchos análisis 
estadísticos basta lápiz y papel. También destaca la \textbf{independencia respecto a distribución} con la que se trabaja (no se
asume una distribución determinada sobre los datos). La principal diferencia del aprendizaje automático respecto a la inferencia 
es que la inferencia se encarga de comprobar la validez de las hipótesis que propone el estadista, mientras que el algoritmo de
\textit{machine learning} genera hipótesis para unos datos determinados, con unas ciertas condiciones de aproximación y error.

\item \textbf{Teoría de la medida}. Usaremos nociones de probabilidad y teoría de la medida para construir nuestra noción de 
aprendizaje: con cierta probabilidad, el error cometido deberá ser pequeño. Asumiremos conocimiento previo de variables 
aleatorias, y de resultados fundamentales del grado en matemáticas como el teorema de convergencia monótona.

\item \textbf{Algorítmica y optimización de algoritmos}. Debemos analizar el tiempo asintótico de los algoritmos que deben ser ejecutados
sobre una máquina. Se busca que sean lo más eficientes posibles. Se ha intentado que la implementaciones realizadas 
tengan el menor orden de eficiencia posible, y dada la lentitud de \R en los bucles, se han extraído algunas partes clave de dichos 
algoritmos para ejecutarlas sobre \texttt{C++}.
\end{enumerate}

Nuestro estudio de informática estará centrado en el desbalanceo de clases (dos clases, una clase de los datos de entrenamiento más 
abundante que la otra), por lo que construiremos los cimientos de la teoría PAC sobre el paradigma binario. El máximo exponente
de esta teoría fue Leslie Valiant, en los años 80, y le grangeo el más prestigioso premio a nivel de informática teórica
existente hoy día: el Premio Turing, equivalente a la medalla Fields en matemáticas. Antes, en los años 60, Vladimir Vapnik y 
Alexey Chervonenkis dieron una definición de aprendizaje combinatoria y en los años 30, Valery Ivanovich Glivenko y Francesco
Paolo Cantelli habían proporcionado una noción de aprendizaje estadístico uniforme. Valiant relacionó todos esos paradigmas
entre sí.

Se ha intentado adaptar la teoría PAC de teoría computacional lo máximo posible a un \textit{framework} matemático, proporcionando
una introducción a los problemas de medibilidad de los conjuntos cuando ha sido posible. El presente trabajo constituye una
revisión del trabajo existente, así como un intento de mejora, adaptación y simplificación de las demostraciones y definiciones.

Se conduce el trabajo desde la dura formalización de aprendizaje PAC pasando por la contextualización del problema de 
desbalanceo de clases, hasta llegar al estudio de \textit{small disjuncts}, un problema puramente empírico. El desarrollo
informático se centra en proveer de algoritmos recientes en el campo del \textit{oversampling}, técnica usada para paliar
el desbalanceo de clases, cuya implementación no ha sido liberada por sus autores.

Fundamentalmente nuestra memoria está dividida en:
\begin{itemize}
\item \textbf{Parte de matemáticas}:
 \begin{itemize}
  \item Nociones básicas de probabilidad, así como desigualdades básicas.
  \item Introdución los primeros conceptos de aprendizaje PAC, y relajación posterior que se concreta en la teoría APAC
  \item Noción de aprendizaje uniforme de Glivenko-Cantelli.
  \item Teorema de No Free Lunch.
  \item Dimensión Vapnik Chervonenkis y construcción del teorema fundamental del aprendizaje PAC.
 \end{itemize}
\item \textbf{Parte de informática}:
 \begin{itemize}
  \item Presentación del problema de desbalanceado y de las técnicas existentes.
  \item Descripción de los algoritmos implementados.
  \item Descripción de las tecnologías empleadas y el paquete desarrollado.
  \item Experimentación con \textit{small disjuncts}.
 \end{itemize}
\item \textbf{Conclusiones}: donde se presentan las conclusiones alcanzadas con la realización del proyecto y las vías de 
 trabajo futuro.
\item \textbf{Apéndice}: que provee de un manual para el manejo del paquete \texttt{imbalance}, del lenguaje de estadística
computacional \texttt{R}.
\end{itemize}


\section{Bibliografía empleada}
El material principal en el que se ha basado la sección de matemáticas ha sido \citep{shalev}, 
que proporciona una detallada formalización de los fundamentos matemáticos del Aprendizaje Automático. Puntualizamos además
una serie de referencias extras usadas en la elaboración de esta parte:

\begin{itemize} 
 \item La primera parte del capítulo de introducción a teoría de la probabilidad ha sido confeccionado por el autor de 
 este trabajo a partir de \citep{caratheodory}, que ofrece una demostración guiada a través de ejercicios de dicho teorema. 
 También se ha usado como apoyo para dicho capítulo el libro \citep{loeve}. 
 \item Para la parte final del primer capítulo de introducción a probabilidad se ha usado \citep{shalev}, pero
 mayoritariamente el contenido de Wikipedia \citep{wiki:markov, wiki:hoeff_lemma, wiki:hoeffding}.
 \item Para el teorema \ref{th:fundamental} sobre el teorema fundamental PAC se ha usado principalmente el material de clase
 proporcionado por \citep{slfetaya} para sus clases en \textit{Weizmann Institue of Science}.
\end{itemize}
 
 Para la sección de informática se ha empleado:
 
\begin{itemize}
 \item Para el capítulo introductorio al desbalanceo se ha empleado \citep{he2009} 
 \item Cada uno de los algoritmos implementados corresponde al material descrito en un paper de revista científica: 
 MWMOTE \citep{barua14}; RACOG y wRACOG \citep{das2015}; RWO \citep{zhang2014}; PDFOS \citep{gao2014} y 
 NEATER \citep{almogahed2014}. Para la introducción a PDFOS se ha usado además material adaptado desde el libro \citep{silverman}.
\end{itemize}

 Para el desarrollo del software se ha seguido fundamentalmente \citep{rhadleypkg}, que proporciona una guía detallada de
 cómo construir correctamente un paquete del lenguaje R. También se han consultado capítulos aislados de \citep{rgillespie},
 que ofrece información muy útil sobre cómo hacer softwre eficiente en R.
 
\section{Conocimiento libre}
 El contenido de este trabajo ha sido publicado bajo una licencia Creative Commons BY-NC-SA en un repositorio de Github 
 \footnote{\url{https://github.com/ncordon/tfg}}. El código del fichero en \LaTeX, así como todas las imágenes, a excepción
 de las extraídas de otras fuentes y explíticamente marcadas a tal efecto, quedan liberados bajo una licencia GPLv3.
 
 El software, disponible en otro repositorio de Github \footnote{\url{https://github.com/ncordon/imbalance}}, se libera bajo
 una licencia GPLv2 o posterior.