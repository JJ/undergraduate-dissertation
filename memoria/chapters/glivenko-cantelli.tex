\section{Clases de Glivenko-Cantelli}

Empezamos definiendo lo que es un conjunto de entrenamiento $\varepsilon$ representativo.

\begin{definition*}
 Un conjunto de entrenamiento $S$ es $\varepsilon$-representativo de una clase de hipótesis $H$ respecto a una
 distribución $\mathcal{D}$ y una función de pérdida $l$, si $\forall h\in H$ se tiene:
 
 \[|L_S(h) - L_{\dist}(h)| \le \varepsilon\]
\end{definition*}

Intuitivamente, cualquier $ERM_H$ sobre $S$ sería un buen algoritmo de aprendizaje 
si $S$ es $\varepsilon$ representativo para cualquier $\varepsilon \in ]0,1[$ y cualquier distribución. 
Lo formalizamos con la siguiente proposición:

\begin{fact}
 Si $S$ es $\varepsilon$-representativo de $H$ con respecto a una distribución $\mathcal{D}$ y una función de 
 pérdida $l$, entonces se tiene:
 
 \[L_{\dist}(ERM(S)) \le \inf_{h\in H} L_{\dist}(h) + 2\varepsilon\]
 
 \label{fact:epsilon-rep}
\end{fact}

\begin{proof}
 Sea $A$ un $ERM_H$
 
 $L_{\dist}(A(S)) \le L_S(A(S)) + \varepsilon$ por ser $S$ $\varepsilon$-representativa. Pero por definición de 
 $ERM_H$, $\inf_{h\in H} L_S(A(S)) + \varepsilon \le \inf_{h\in H}L_S(h) + \varepsilon$ y aplicando $\varepsilon$-representatividad de nuevo:
 
 \[\inf_{h\in H} L_S(h) + \varepsilon \le \inf_{h\in H} L_{\mathcal{D}}(h) + 2\varepsilon\]
 
 En resumen: $L_{\dist}(A(S)) \le \inf_{h\in H} L_{\mathcal{D}}(h) + 2\varepsilon$, para $A \in ERM_H$ arbitrario.
\end{proof}


\begin{definition*} \textbf{Clase de Glivenko-Cantelli}

Decimos que una clase de hipótesis $H$ es de Glivenko-Cantelli, respecto a un dominio $Z$, y a 
una función de pérdida $l$, si existe una función ${m_{H}^{CU}: ]0,1[^2 \rightarrow \mathbb{N}}$ 
verificando que para todo $0 < \delta, \varepsilon < 1$ y para toda distribución $\dist$ sobre $Z$, siendo 
$S\sim \dist^m$ con $m \ge m_{H}^{CU}(\varepsilon, \delta)$, 
entonces:

\[\mprob [\forall h\in H, |L_S(h) - L_{\dist}(h)| \le \varepsilon] \ge 1-\delta\]
\end{definition*}

Análogamente a lo que ocurría en la definición \ref{def:pac}, podemos considerar $m_H^{CU}$ única, en el sentido de
que es la mínima función que satisface las hipótesis.

\section{Glivenko-Cantelli y APAC cognoscibilidad}

\begin{theorem*}
Sea $H$ una clase de hipótesis de Glivenko-Cantelli, respecto a $\mathcal{D}$ y función de pérdida $l$. 
Entonces es APAC cognoscible con cualquier algoritmo $ERM_H$ y complejidad muestral
$m(\varepsilon, \delta) \le m_{H}^{UC} \left(\frac{\varepsilon}{2}, \delta \right)$ 
\end{theorem*}

  \begin{proof}
  Sea $A$ un $ERM_H$ arbitrario, y $\dist$ una distribución arbitraria sobre $Z$.
  Fijamos $m = m_{H}^{UC} \left(\frac{\varepsilon}{2}, \delta \right)$.

  Sea $S = (z_1, \ldots z_m)$ un conjunto de entrenamiento, verificando que: 

  \begin{equation}
    \forall h\in H, |L_{S}(h)-L_{\dist}(h)| \le \frac{\varepsilon}{2}
    \label{eq:gc-ineq}
  \end{equation}

  Entonces $S$ es $\varepsilon$ representativa para $\dist$ y $l$, y por proposición \ref{fact:epsilon-rep}:

  \begin{equation}
   L_{\dist}(A(S)) \le \inf_{h\in H} L_{\dist}(h) + \varepsilon
   \label{eq:gc-result}
  \end{equation}

  Pero como $\eqref{eq:gc-ineq}$ ocurre con probabilidad (sobre $S$) mayor o igual a $1-\delta$, entonces $\eqref{eq:gc-result}$
  ocurre con probabiliad mayor o igual a $1-\delta$
  
  \end{proof}
  
  
\begin{fact}
Sea $H$ una clase de hipótesis finita, $Z$ un dominio y sea $l : H \times Z \rightarrow [a,b]$ una función de pérdida. Entonces $H$ verifica la propiedad de convegencia uniforme con: 

\[m_{H}^{CU}(\varepsilon, \delta) \le \left\lfloor \frac{log(2|H|/\delta)(b-a)^2}{2\varepsilon^2} \right\rfloor + 1\]
\end{fact}
  \begin{proof}
  Sea $H$ una clase de hipótesis finita.

  Fijamos $0 < \delta, \varepsilon < 1$. Necesitamos encontrar $m\in \mathbb{N}$ verificando:

  \[P_{S\sim \mathcal{D}^m} [\exists h\in H |L_S(h) - L_{\mathcal{D}}(h)| > \varepsilon] < \delta\]

  Partimos de la siguiente desigualdad, que usaremos más adelante, obtenida por subaditividad:

  \[P [\exists h\in H |L_S(h) - L_{\mathcal{D}}(h)| > \varepsilon] \le \sum_{h \in H} P [|L_S(h) - L_{\mathcal{D}}(h)| > \varepsilon]\]
  Fijamos $h \in H$.

  Dado un conjunto de entrenamiento $S_z = (z_1, \ldots z_m)$, recordamos que $L_{\mathcal{D}} (h) = \mathbb{E}_{z\sim \mathcal{D}} [l(h,z)]$ y que $L_{S_z}(h) = \frac{1}{m} \sum_{i=1}^m l(h,z_i)$

  Donde $z_i \sim \mathcal{D}$ y por tanto $\mathbb{E}_{S \sim \mathcal{D}^m} [L_S(h)] = \mathbb{E}_{z \sim \mathcal{D}} [l(h,z)] = L_{\mathcal{D}} (h)$. Además, llamando $X_i = l(h,Z_i)$, por ser $S=(Z_1, \ldots Z_m)$ m.a.s que genera los conjuntos de entrenamiento, se tiene que las $X_i$ son independientes e idénticamente distribuidas, con $P[a \le X_i \le b] = 1$. Estamos en condiciones de aplicar la desigualdad de Hoeffding.

  Por tanto:

  \[P \left[\left| \frac{1}{m} \sum_{i=1}^m X_i - L_{\mathcal{D}} (h) \right| > \varepsilon\right] = P [|L_S(h) - L_{\mathcal{D}}(h)| > \varepsilon] \le 2e^{-2m \left( \frac{\varepsilon}{b-a} \right)^2}\]

  Y por tanto:

  \[P [\exists h\in H |L_S(h) - L_{\mathcal{D}}(h)| > \varepsilon] \le |H| 2e^{-2m \left( \frac{\varepsilon}{b-a} \right)^2}\]

  Despejando $m$ para que $|H| 2e^{-2m \left( \frac{\varepsilon}{b-a} \right)^2} < \delta$ llegamos al resultado buscado.
  \end{proof}

Recordemos hasta ahora el resultado que habíamos obtenido era su carácter PAC cognoscible, donde 
agnósticamente PAC cognoscible y cognoscible con funciones de pérdida 0-1 era un término equivalente. El 
teorema que enunciamos a continuación, deducible a partir del teorema sobre el caracter agnóstico - PAC 
cognoscible de clases de funciones con propiedad de convergencia uniforme, en particular las finitas, 
generaliza el resultado para cualquier funciones de pérdida acotada.

\begin{theorem*}

Sea $H$ una clase de hipótesis finita, $Z$ un dominio y sea $l : H \times Z \rightarrow [a,b]$ una función de pérdida. Entonces $H$ es PAC cognoscible con complejidad muestral:

\[m_{H}( \varepsilon, \delta ) \le \left\lceil \frac{2 log(2|H|/\delta)(b-a)^2}{\varepsilon^2} \right\rceil\]
\end{theorem*}
  \begin{proof}
  Es trivial desde el anterior teorema y el hecho de que convergencia uniforme implica ser agnósticamente PAC cognoscible
  \end{proof}
